{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "SZT0o6TeV1uR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Passive suicidality in a repressive U.S. political context: Aim II"
      ],
      "metadata": {
        "id": "Y7Uzw9hHU4qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Relinks $\\mathcal{V}$-corpus posts and comments, encodes textual covariates of post &rarr; comment trasmissions of fatalistic semantics using sparse explicit TF-IDF and dense implicit vecotr space representations. Includes post-estimation inspection of key token covariates._"
      ],
      "metadata": {
        "id": "20Z9-zTrU91A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> aim_ii_model_02.ipynb<br>\n",
        "> Simone J. Skeen (03-05-2025)"
      ],
      "metadata": {
        "id": "and6TuNOVBGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Prepare](#scrollTo=I77md_rMVZUf)\n",
        "2. [Transform](#scrollTo=SZT0o6TeV1uR)\n",
        "3. [Fit/estimate](#scrollTo=lJahbPAe5I0h)"
      ],
      "metadata": {
        "id": "l3UoDNzOVOEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Prepare\n",
        "Installs, imports, and downloads requisite models and packages.\n",
        "***"
      ],
      "metadata": {
        "id": "I77md_rMVZUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "%pip install causalnlp"
      ],
      "metadata": {
        "id": "nJgVDL-NKobc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjYIXCH2Ujqk"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from causalnlp import CausalInferenceModel\n",
        "from causalnlp.core.causalbert import CausalBertModel\n",
        "from causalnlp.key_driver_analysis import KeyDriverAnalysis\n",
        "from causalnlp.autocoder import Autocoder\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.text import Text\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "pd.set_option(\n",
        "    'display.max_columns',\n",
        "    None,\n",
        "    )\n",
        "pd.set_option(\n",
        "    'display.max_rows',\n",
        "    None,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\n",
        "    '/content/drive',\n",
        "    force_remount = True,\n",
        "    )"
      ],
      "metadata": {
        "id": "mIegTW08Vnw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Transform\n",
        "Merges $\\mathcal{D}$<sub>inf labeled</sub>, $\\mathcal{d}$<sub>comments</sub> &rarr; $\\mathcal{D}$<sub>causal</sub>. Prepares for LIWC-22, causal-text, CausalNLP.\n",
        "***"
      ],
      "metadata": {
        "id": "SZT0o6TeV1uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# D_causal merge\n",
        "\n",
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/inputs/data\n",
        "\n",
        "d_inf = pd.read_csv(\n",
        "    'd_inf_labeled.csv',\n",
        "    index_col = [0],\n",
        "    )\n",
        "\n",
        "d_inf.info()\n",
        "d_inf.head(3)\n",
        "d_inf.tail(3)\n",
        "\n",
        "d_c = pd.read_csv(\n",
        "    'd_comments.csv',\n",
        "    #index_col = [0],\n",
        "    )\n",
        "\n",
        "d_c.info()\n",
        "d_c.head(3)\n",
        "d_c.tail(3)"
      ],
      "metadata": {
        "id": "3r4P1nVwWINq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _Initial nested merge_"
      ],
      "metadata": {
        "id": "fyd3z-6-Xoai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect dtypes\n",
        "\n",
        "print(\n",
        "    d_inf['id'].dtype,\n",
        "    d_c['id'].dtype,\n",
        "    )\n",
        "\n",
        "# convert to str\n",
        "\n",
        "d_inf['id'] = d_inf['id'].astype(str)\n",
        "d_c['id'] = d_c['id'].astype(str)\n",
        "\n",
        "# strip whitespace\n",
        "\n",
        "d_inf['id'] = d_inf['id'].str.strip()\n",
        "d_c['id'] = d_c['id'].str.strip()\n",
        "\n",
        "d = pd.merge(\n",
        "    d_inf,\n",
        "    d_c,\n",
        "    on = 'id',\n",
        "    how = 'left',\n",
        "    )\n",
        "\n",
        "d.info()\n",
        "d.head(30)"
      ],
      "metadata": {
        "id": "7mjjitwYWPea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pseudo-word token for repeat rows\n",
        "\n",
        "d['dupl'] = d.duplicated(\n",
        "    subset = 'text',\n",
        "    keep = 'first',\n",
        "    )\n",
        "\n",
        "d.loc[d['dupl'], 'text'] = '<|RPT|>'\n",
        "d = d.drop(columns = ['dupl'])\n",
        "\n",
        "d['dupl'] = d.duplicated(\n",
        "    subset = 'p_titl',\n",
        "    keep = 'first',\n",
        "    )\n",
        "\n",
        "d.loc[d['dupl'], 'p_titl'] = '<|RPT|>'\n",
        "d = d.drop(columns = ['dupl'])\n",
        "\n",
        "# create nested post-comment 'block' var\n",
        "\n",
        "# new block\" 'text' != '<|RPT|>'\n",
        "\n",
        "d['new_block'] = d['text'] != '<|RPT|>'\n",
        "\n",
        "# sum blocks for block indexing\n",
        "\n",
        "d['block'] = (\n",
        "    d['new_block'].cumsum() + 1).where(\n",
        "        d['new_block'],\n",
        "        0,\n",
        "        ).astype(int)\n",
        "\n",
        "# forward fill\n",
        "\n",
        "d['block'] = d['block'].replace(\n",
        "    0,\n",
        "    method = 'ffill',\n",
        "    ).astype(int)\n",
        "\n",
        "        ### SJS 12/30: preserve new_block var for sense-check (for now)\n",
        "\n",
        "# drop 'new_block'\n",
        "\n",
        "#post_comments = post_comments.drop(columns=['new_block'])\n",
        "\n",
        "# dummy for post author replying within comment threads\n",
        "\n",
        "d['p_au_reply'] = np.where(\n",
        "    d['p_au'] == d['c_au'], 1, 0)\n",
        "\n",
        "d.ino()\n",
        "d.head(30)"
      ],
      "metadata": {
        "id": "suppDthJWPip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decouple post author and commenter comments\n",
        "\n",
        "# dupe c_text col\n",
        "\n",
        "d['p_au_c_text'] = d['c_text']\n",
        "\n",
        "# dupe c_text text\n",
        "\n",
        "d.loc[d['p_au_reply'] == 1, 'p_au_c_text'] = d.loc[d['p_au_reply'] == 1, 'c_text']\n",
        "\n",
        "# disaggregate about p_author_reply = 1/0\n",
        "\n",
        "d.loc[d['p_au_reply'] == 1, 'c_text'] = ' '\n",
        "d.loc[d['p_au_reply'] != 1, 'p_au_c_text'] = ' '\n",
        "\n",
        "# force to str\n",
        "\n",
        "d['c_text'] = d['c_text'].astype(str)\n",
        "d['p_au_c_text'] = d['p_au_c_text'].astype(str)\n",
        "\n",
        "#d['p_uniq'].value_counts()\n",
        "d.info()\n",
        "d.head(30)\n",
        "\n",
        "# save\n",
        "\n",
        "#d.to_csv(\n",
        "#    'd_causal.csv',\n",
        "#    index = True,\n",
        "#    )"
      ],
      "metadata": {
        "id": "wOSHTZIVWPmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _Join_ `'c_text'` _(commenter comments in comment threads) and_ `'p_au_c_text'` _(post author comments in comment threads) by block_"
      ],
      "metadata": {
        "id": "W5OTfjq7XM3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join 'c_text' (commenter comments in comment threads) and 'p_au_c_text' (post author comments in comment threads) by block\n",
        "\n",
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/inputs/data\n",
        "\n",
        "d = pd.read_csv(\n",
        "    'd_causal.csv',\n",
        "    index_col = [0],\n",
        "    )\n",
        "\n",
        "# force to str\n",
        "\n",
        "d['c_text'] = d['c_text'].astype(str)\n",
        "d['p_au_c_text'] = d['p_au_c_text'].astype(str)\n",
        "\n",
        "# concat\n",
        "\n",
        "d['block_c_text'] = d.groupby('block')['c_text'].transform(lambda i: ' '.join(i))\n",
        "d['block_p_au_c_text'] = d.groupby('block')['p_au_c_text'].transform(lambda i: ' '.join(i))\n",
        "\n",
        "# drop 'new_block' = False\n",
        "\n",
        "d = d[d['new_block']]\n",
        "\n",
        "# append post title to concatenated post author comments\n",
        "\n",
        "#d['text_covar_a'] = d.apply(lambda row: row['block_p_au_c_text'] + row['p_titl'] if row['p_titl'] != '<|RPT|>' else row['block_p_au_c_text'], axis = 1)\n",
        "\n",
        "d.info()\n",
        "d.head(30)\n",
        "\n",
        "# save\n",
        "\n",
        "d.to_csv(\n",
        "    'd_causal.csv',\n",
        "    index = True,\n",
        "    )"
      ],
      "metadata": {
        "id": "Xj0ddTy9WPqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect joined subset\n",
        "\n",
        "d = d[[\n",
        "    'block_c_text',\n",
        "    'block_p_au_c_text',\n",
        "    ]].copy()\n",
        "\n",
        "d.head(3)"
      ],
      "metadata": {
        "id": "rZRZeEC_WPuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _Clean, tokenize_"
      ],
      "metadata": {
        "id": "ZjM_GJ-mWuaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess\n",
        "\n",
        "texts = [\n",
        "    'block_c_text',\n",
        "    'block_p_au_c_text',\n",
        "    ]\n",
        "\n",
        "# remove URLs, special characters, convert to lc\n",
        "\n",
        "for t in texts:\n",
        "    d['block_p_au_c_text'] = d['block_p_au_c_text'].str.replace(\n",
        "        'http\\S+|www.\\S+',\n",
        "        ' ',\n",
        "        case = False,\n",
        "        )\n",
        "    d['block_p_au_c_text'] = d['block_p_au_c_text'].str.replace(\n",
        "        '[^A-Za-z0-9]+',\n",
        "        ' ',\n",
        "        )\n",
        "    d['block_p_au_c_text'] = d['block_p_au_c_text'].map(lambda i: i if type(i)!=str else i.lower())\n",
        "\n",
        "d.to_csv(\n",
        "    'd_causal_clean.csv',\n",
        "    index = True,\n",
        "    )"
      ],
      "metadata": {
        "id": "yhwZ4w7OWPxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _Encode_ `'T'` _BAR policy mention_"
      ],
      "metadata": {
        "id": "F2E7Wf4Jra1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GAHC criminalization regex\n",
        "\n",
        "raw_patterns = [\n",
        "    r'\\bArkansas\\b.*?\\bHouse Bill 1570\\b|\\bHB\\s?1570\\b|\\bto create the arkansas save adolescents from experimentation \\(safe\\) act\\b',\n",
        "    r'\\bArizona\\b.*?\\bSenate Bill 1138\\b|\\bSB\\s?1138\\b|\\bprohibition of irreversible gender reassignment surgery for minors\\b',\n",
        "    r'\\bFlorida\\b.*?\\bCS/SB 254\\b|\\bSenate Bill 254\\b|\\ban act relating to treatments for sex reassignment\\b',\n",
        "    r'\\bGeorgia\\b.*?\\bSenate Bill 140\\b|\\bSB\\s?140\\b',\n",
        "    r'\\bIowa\\b.*?\\bSenate File 538\\b|\\bSF\\s?538\\b|\\ba bill for an act relating to prohibited activities regarding gender transition procedures relative to minors\\b',\n",
        "    r'\\bIdaho\\b.*?\\bHouse Bill 71\\b|\\bHB\\s?71\\b|\\brelating to the child protection act\\b',\n",
        "    r'\\bIndiana\\b.*?\\bSenate Enrolled Act 480\\b|\\bSA\\s?538\\b',\n",
        "    r'\\bKentucky\\b.*?\\bSenate Bill 150\\b|\\bSB\\s?150\\b|\\ban relating to children\\b',\n",
        "    r'\\bLouisiana\\b.*?\\bHouse Bill 648\\b|\\bHB\\s?648\\b|\\bthe stop harming our kids act\\b',\n",
        "    r'\\bMissouri\\b.*?\\bSenate Bill 49\\b|\\bSB\\s?49\\b|\\bMissouri save adolescents from experimentation \\(safe\\) act\\b',\n",
        "    r'\\bMississippi\\b.*?\\bHouse Bill 1125\\b|\\bHB\\s?1125\\b|\\bthe regulate experimental adolescent procedures \\(reap\\) act\\b',\n",
        "    r'\\bMontana\\b.*?\\bSenate Bill 0099\\b|\\bSB\\s?0?099\\b|\\ban act providing for a youth health protection act\\b',\n",
        "    r'\\bNorth Carolina\\b.*?\\bHouse Bill 808\\b|\\bHB\\s?808\\b|\\ban act to prohibit gender transition procedures for minors\\b',\n",
        "    r'\\bNorth Dakota\\b.*?\\bHouse Bill 1254\\b|\\bHB\\s?1254\\b|\\bthe prohibition of certain practices against a minor; to provide a penalty; and to declare an emergency\\b',\n",
        "    r'\\bNebraska\\b.*?\\bLegislative Bill 574\\b|\\bLB\\s?574\\b|\\badopt the let them grow act\\b',\n",
        "    r'\\bOhio\\b.*?\\bHouse Bill 68\\b|\\bHB\\s?68\\b|\\bsaving ohio adolescents from experimentation \\(safe\\) act\\b',\n",
        "    r'\\bOklahoma\\b.*?\\bSenate Bill 613\\b|\\bSB\\s?613\\b|\\ban act relating to health care\\b',\n",
        "    r'\\bSouth Carolina\\b.*?\\bHouse Bill 4624\\b|\\bHB\\s?4624\\b|\\bto prohibit the provision of gender transition procedures to a person under eighteen years of age\\b',\n",
        "    r'\\bSouth Dakota\\b.*?\\bHouse Bill 1080\\b|\\bHB\\s?1080\\b|\\ban act to prohibit certain medical and surgical interventions on minor patients\\b',\n",
        "    r'\\bTennessee\\b.*?\\bSenate Bill 1\\b|\\bSB\\s?1\\b|\\ban act to amend tennessee code annotated\\b',\n",
        "    r'\\bTexas\\b.*?\\bSenate Bill 14\\b|\\bSB\\s?14\\b|\\brelating to prohibitions on the provision to certain children of procedures and treatments for gender transitioning, gender reassignment, or gender dysphoria and on the use of public money or public assistance to provide those procedures and treatments\\b',\n",
        "    r'\\bUtah\\b.*?\\bSenate Bill 16\\b|\\bSB\\s?16\\b|\\btransgender medical treatments and procedures amendments\\b',\n",
        "    r'\\bWest Virginia\\b.*?\\bHouse Bill 2007\\b|\\bHB\\s?2007\\b|\\bWest Virginia Medical Practice Act\\b',\n",
        "    r'\\bWyoming\\b.*?\\bSenate File 0099\\b|\\bSF\\s?0?0999\\b|\\bgender transitioning and reassignment procedures for children prohibited\\b',\n",
        "    ]\n",
        "\n",
        "# pre-compile regex patterns with case-insensitive flag\n",
        "\n",
        "compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in raw_patterns]\n",
        "\n",
        "# check match Fx\n",
        "\n",
        "def check_match(text):\n",
        "    if any(pattern.search(str(text)) for pattern in compiled_patterns):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "# apply check match\n",
        "\n",
        "d['T'] = d['text'].apply(check_match)\n",
        "\n",
        "# tally matches\n",
        "\n",
        "bar_n = d['T'].sum()\n",
        "print(bar_n)\n",
        "\n",
        "# save\n",
        "\n",
        "d.to_csv(\n",
        "    'd_causal_liwc.csv',\n",
        "    index = True,\n",
        "    )"
      ],
      "metadata": {
        "id": "nn2DlpMGqOD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _Encode_ `'C'` _subreddit (categorical) covariate_"
      ],
      "metadata": {
        "id": "5P7I0MPvyB52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/inputs/data\n",
        "\n",
        "    ### SJS 2/27: LIWC-22 renames 'block_c_text' to 'Text' - content is the same, crosswalk confirmed\n",
        "\n",
        "d = pd.read_csv(\n",
        "    'd_causal_liwc.csv',\n",
        "    index_col = [0],\n",
        "    )"
      ],
      "metadata": {
        "id": "_ykgAkFhuZql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# fit_tramsform subreddit covar\n",
        "\n",
        "d['C'] = le.fit_transform(d['p_sbrt'])\n",
        "\n",
        "# verify mapping\n",
        "\n",
        "category_mapping = dict(zip(\n",
        "    le.classes_,\n",
        "    le.transform(le.classes_)\n",
        "    ))\n",
        "\n",
        "print(\n",
        "    \"Encoding:\",\n",
        "    category_mapping,\n",
        "    )\n",
        "\n",
        "# inspect\n",
        "\n",
        "d[['id', 'C', 'p_sbrt']].head(10)\n",
        "\n",
        "# save\n",
        "\n",
        "#d.to_csv(\n",
        "#    'd_causal_liwc.csv',\n",
        "#    index = True,\n",
        "#    )"
      ],
      "metadata": {
        "id": "Rbfu2fcEyBUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################### LIWC-22 encoding #######################################"
      ],
      "metadata": {
        "id": "WNqoNwwNxhEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _Post-LIWC-22: encode 'fatalism'_ `'Y'` _outcomes_"
      ],
      "metadata": {
        "id": "wR1jqLykXxir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/inputs/data\n",
        "\n",
        "    ### SJS 2/27: LIWC-22 renames 'block_c_text' to 'Text' - content is the same, crosswalk confirmed\n",
        "\n",
        "d = pd.read_csv(\n",
        "    'd_causal_liwc.csv',\n",
        "    index_col = [0],\n",
        "    )\n",
        "\n",
        "d.info()\n",
        "#d.head(3)\n",
        "d[['id', 'Text']].head(3)"
      ],
      "metadata": {
        "id": "M-yJ23A8WP06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute mdn\n",
        "\n",
        "allnone_mdn = d['allnone'].median()\n",
        "emo_neg_mdn = d['emo_neg'].median()\n",
        "\n",
        "# encode >mdn\n",
        "\n",
        "d['allnone_high'] = (d['allnone'] > allnone_mdn).astype(int)\n",
        "d['emo_neg_high'] = (d['emo_neg'] > emo_neg_mdn).astype(int)\n",
        "\n",
        "# display mdn\n",
        "\n",
        "print(\"\\n'allnone' Mdn:\" allnone_mdn)\n",
        "print(\"\\n'emo_neg' Mdn:\" emo_neg_mdn)\n",
        "\n",
        "# inspect\n",
        "\n",
        "d[['allnone', 'allnone_high', 'emo_neg', 'emo_neg_high']].head(3)"
      ],
      "metadata": {
        "id": "ZhxjAK2ZBGUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"\\n'allnone' Mdn:\", allnone_mdn)\n",
        "#print(\"\\n'emo_neg' Mdn:\", emo_neg_mdn)"
      ],
      "metadata": {
        "id": "Fs3IMsvLEPUX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Y' = 'allnone_high' * 'emo_neg_high'\n",
        "\n",
        "d['Y'] = d['allnone_high'] * d['emo_neg_high']\n",
        "\n",
        "# inspect\n",
        "\n",
        "d[['allnone_high', 'emo_neg_high', 'Y']].head()\n",
        "\n",
        "# save\n",
        "\n",
        "d.to_csv(\n",
        "    'd_causal_liwc.csv',\n",
        "    index = True,\n",
        "    )"
      ],
      "metadata": {
        "id": "IcmeWtGQbjjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _$\\mathcal{D}$<sub>model 2 all blocks</sub>_: $N$ = _146K (incl empty `'W'` cells)_"
      ],
      "metadata": {
        "id": "E-NL4X0aWT1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# condense\n",
        "\n",
        "d = d[[\n",
        "    'date',\n",
        "    'p_au',\n",
        "    'id',\n",
        "    'n_cmnt',\n",
        "    'text',\n",
        "    'C',\n",
        "    'asp_pred',\n",
        "    'dep_pred',\n",
        "    'val_pred',\n",
        "    'prg_pred',\n",
        "    'tgd_pred',\n",
        "    'age_pred',\n",
        "    'race_pred',\n",
        "    'dbty_pred',\n",
        "    'sui_re',\n",
        "    'block_p_au_c_text',\n",
        "    'allnone',\n",
        "    'emo_neg',\n",
        "    'emo_anx',\n",
        "    'emo_anger',\n",
        "    'emo_sad',\n",
        "    'allnone_high',\n",
        "    'emo_neg_high',\n",
        "    'T',\n",
        "    'Y',\n",
        "    ]].copy()\n",
        "\n",
        "# rename\n",
        "\n",
        "d.rename(\n",
        "    columns = {\n",
        "        'Text': 'block_c_text',\n",
        "        'block_p_au_c_text': 'W',\n",
        "    }, inplace = True,\n",
        "    )\n",
        "\n",
        "# save .csv\n",
        "\n",
        "d.to_csv(\n",
        "    'd_model_02_all_blocks.csv',\n",
        "    index = True,\n",
        "    )\n",
        "\n",
        "# save .tsv\n",
        "\n",
        "d.to_csv(\n",
        "    'd_model_02_all_blocks.tsv',\n",
        "    sep = '\\t',\n",
        "    index = True,\n",
        "    )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wtY30V1Pq0He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _$\\mathcal{D}$<sub>model 2 covar blocks</sub>_: $N$ = _59K (incl populated `'W'` cells)_"
      ],
      "metadata": {
        "id": "I1zMsxH0wRZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop NaN, empty 'W' cells\n",
        "\n",
        "d = d.dropna(subset = ['W']).loc[d['W'].str.strip() != ' ']\n",
        "\n",
        "# reset idx\n",
        "\n",
        "d.reset_index(inplace = True)\n",
        "\n",
        "# save .csv\n",
        "\n",
        "d.to_csv(\n",
        "    'd_model_02_covar_blocks.csv',\n",
        "    index = True,\n",
        "    )\n",
        "\n",
        "# save .tsv\n",
        "\n",
        "d.to_csv(\n",
        "    'd_model_02_covar_blocks.tsv',\n",
        "    sep = '\\t',\n",
        "    index = True,\n",
        "    )"
      ],
      "metadata": {
        "id": "EW88U8XXozTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fit/estimate\n",
        "Fits meta-learner models to estimate $T_p$ &rarr; $y_c$ C/ATE.\n",
        "***"
      ],
      "metadata": {
        "id": "lJahbPAe5I0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/inputs/data\n",
        "\n",
        "d = pd.read_csv(\n",
        "    'd_model_02_all_blocks.csv',\n",
        "    index_col = [0],\n",
        "    )\n",
        "\n",
        "d.info()\n",
        "d.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BGj0Cc1mAqTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 2a. BAR policy salience ($T_p$) → fatalism CATE ($Y_c$)"
      ],
      "metadata": {
        "id": "xu2igRWrIopc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# restrict to TGD Redditors\n",
        "\n",
        "d = d[d['tgd_pred'] != 0]\n",
        "d.reset_index(inplace = True)\n",
        "\n",
        "d.shape\n",
        "d.head(3)"
      ],
      "metadata": {
        "id": "sr7u1rxbYZgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _2a: Sparse explicit text covar_ `'W'` _representations: TF-IDF via T-Learner_"
      ],
      "metadata": {
        "id": "ildxC8Ncc35M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ignore_cols = [\n",
        "    'level_0',\n",
        "    'index',\n",
        "    'date',\n",
        "    'p_au',\n",
        "    'id',\n",
        "    'n_cmnt',\n",
        "    'text',\n",
        "    'p_titl',\n",
        "    'asp_pred',\n",
        "    'dep_pred',\n",
        "    'val_pred',\n",
        "    'prg_pred',\n",
        "    'tgd_pred',\n",
        "    'age_pred',\n",
        "    'race_pred',\n",
        "    'dbty_pred',\n",
        "    'sui_re',\n",
        "    'allnone',\n",
        "    'emo_neg',\n",
        "    'emo_anx',\n",
        "    'emo_anger',\n",
        "    'emo_sad',\n",
        "    'allnone_high',\n",
        "    'emo_neg_high',\n",
        "    #'C',\n",
        "    ]\n",
        "\n",
        "# verify feature input dimensions\n",
        "\n",
        "T = d['T'].values\n",
        "W = d['W'].values.reshape(-1, 1) ### reshape\n",
        "y = d['Y'].values\n",
        "\n",
        "print(f\"T shape: {T.shape}\")\n",
        "print(f\"W shape: {W.shape}\")\n",
        "print(f\"Y shape: {y.shape}\")\n",
        "\n",
        "# fit\n",
        "\n",
        "cm = CausalInferenceModel(\n",
        "    d,\n",
        "    method = 't-learner',\n",
        "    learner = LGBMClassifier(num_leaves = 500),\n",
        "    treatment_col = 'T',\n",
        "    outcome_col = 'Y',\n",
        "    text_col = 'W',\n",
        "    #include_cols = ['C'], ### 'C' implicitly adjusted for _unless_ in ignore_cols\n",
        "    ngram_range = (1,3),\n",
        "    min_df = 0.01,\n",
        "    stop_words = 'english',\n",
        "    ignore_cols = ignore_cols,\n",
        "    verbose = -1,\n",
        "    )\n",
        "\n",
        "cm.fit()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F8t5kcY55yK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ate = cm.estimate_ate()\n",
        "ate"
      ],
      "metadata": {
        "id": "tEvY8hvE5yP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_features = cm.interpret(\n",
        "    plot = False,\n",
        "    method = 'feature_importance',\n",
        "    )[1][:20]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bNYj3gi75yUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(top_features)"
      ],
      "metadata": {
        "id": "DNPSANfw5yXm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _2a: Dense implicit text covar_ `'W'` _representations: embeddings via CausalBERT_"
      ],
      "metadata": {
        "id": "975N0kJkdTQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/inputs/data\n",
        "\n",
        "d = pd.read_csv(\n",
        "    'd_model_02_covar_blocks.tsv',\n",
        "    sep = '\\t',\n",
        "    on_bad_lines = 'skip',\n",
        "    )\n",
        "\n",
        "d['cpnd_pred'] = (d[[\n",
        "    'asp_pred',\n",
        "    'dep_pred',\n",
        "    'val_pred']].sum(axis = 1) == 3).astype(int)\n",
        "\n",
        "cpnd_n = d['cpnd_pred'].sum()\n",
        "print(cpnd_n)\n",
        "\n",
        "# remap 'C'\n",
        "\n",
        "d['C'] = d['C'].apply(lambda i: 0 if i == 5 else 1)"
      ],
      "metadata": {
        "id": "TXnUmVQuhFCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d.info()"
      ],
      "metadata": {
        "id": "EswykrdMlKpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify feature input dimensions\n",
        "\n",
        "T = d['sui_re'].values\n",
        "W = d['W'].values.reshape(-1, 1) ### reshape\n",
        "y = d['Y'].values\n",
        "\n",
        "print(f\"T shape: {T.shape}\")\n",
        "print(f\"W shape: {W.shape}\")\n",
        "print(f\"Y shape: {y.shape}\")\n",
        "\n",
        "# initialize\n",
        "\n",
        "cb = CausalBertModel(\n",
        "    batch_size = 32,\n",
        "    max_length = 128,\n",
        "    )\n",
        "\n",
        "# train\n",
        "\n",
        "cb.train(\n",
        "    d['W'], ### texts\n",
        "    d['C'], ### confounds\n",
        "    d['sui_re'], ### treatment\n",
        "    d['Y'], ### outcome\n",
        "    epochs = 1,\n",
        "    learning_rate = 2e-5,\n",
        "    )\n",
        "\n",
        "print(cb.estimate_ate(\n",
        "    d['C'],\n",
        "    d['W'],\n",
        "    ))"
      ],
      "metadata": {
        "id": "NUsNtkLtY-SG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 2b. Strain (_$\\hat{s}_{p}$_) → fatalism CATE ($Y_c$)."
      ],
      "metadata": {
        "id": "swx9ZjhnHNYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gen compound strain var\n",
        "\n",
        "d['cpnd_pred'] = (d[[\n",
        "    'asp_pred',\n",
        "    'dep_pred',\n",
        "    'val_pred']].sum(axis = 1) == 3).astype(int)\n",
        "\n",
        "cpnd_n = d['cpnd_pred'].sum()\n",
        "print(cpnd_n)"
      ],
      "metadata": {
        "id": "pcDQHXuyeCqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restrict to TGD Redditors\n",
        "\n",
        "d = d[d['tgd_pred'] != 0]\n",
        "#d.reset_index(inplace = True)\n",
        "\n",
        "d.shape\n",
        "d.head(3)"
      ],
      "metadata": {
        "id": "pQfxtO6qLLYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d.columns)"
      ],
      "metadata": {
        "id": "yYexjEDVM9su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _2b: Sparse explicit text covar_ `'W'` _representations: TF-IDF via T-Learner_"
      ],
      "metadata": {
        "id": "vR34wKgbL-TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ignore_cols = [\n",
        "    'level_0',\n",
        "    'index',\n",
        "    'date',\n",
        "    'p_au',\n",
        "    'id',\n",
        "    'n_cmnt',\n",
        "    'text',\n",
        "    'p_titl',\n",
        "    'asp_pred',\n",
        "    'dep_pred',\n",
        "    'val_pred',\n",
        "    #'cpnd_pred',\n",
        "    'prg_pred',\n",
        "    'tgd_pred',\n",
        "    'age_pred',\n",
        "    'race_pred',\n",
        "    'dbty_pred',\n",
        "    'sui_re',\n",
        "    'allnone',\n",
        "    'emo_neg',\n",
        "    'emo_anx',\n",
        "    'emo_anger',\n",
        "    'emo_sad',\n",
        "    'allnone_high',\n",
        "    'emo_neg_high',\n",
        "    'T',\n",
        "    #'C',\n",
        "    ]\n",
        "\n",
        "# verify feature input dimensions\n",
        "\n",
        "T = d['cpnd_pred'].values\n",
        "W = d['W'].values.reshape(-1, 1) ### reshape\n",
        "y = d['Y'].values\n",
        "\n",
        "print(f\"T shape: {T.shape}\")\n",
        "print(f\"W shape: {W.shape}\")\n",
        "print(f\"Y shape: {y.shape}\")\n",
        "\n",
        "# fit\n",
        "\n",
        "cm = CausalInferenceModel(\n",
        "    d,\n",
        "    method = 't-learner',\n",
        "    learner = LGBMClassifier(num_leaves = 500),\n",
        "    treatment_col = 'cpnd_pred',\n",
        "    outcome_col = 'Y',\n",
        "    text_col = 'W',\n",
        "    #include_cols = ['C'], ### 'C' implicitly adjusted for _unless_ in ignore_cols\n",
        "    ngram_range = (1,3),\n",
        "    min_df = 0.01,\n",
        "    stop_words = 'english',\n",
        "    ignore_cols = ignore_cols,\n",
        "    verbose = -1,\n",
        "    )\n",
        "\n",
        "cm.fit()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tbQLpbnBLLeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ate = cm.estimate_ate()\n",
        "ate"
      ],
      "metadata": {
        "id": "Uxf3S6pULLjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_features = cm.interpret(\n",
        "    plot = False,\n",
        "    method = 'feature_importance',\n",
        "    )[1][:20]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1x0N4Z2wMMi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(top_features)"
      ],
      "metadata": {
        "id": "Svr6lNK7MMnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### _Concordances_"
      ],
      "metadata": {
        "id": "FHreS-4JRQra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/inputs/data\n",
        "\n",
        "d = pd.read_csv(\n",
        "    'd_model_02_covar_blocks.csv',\n",
        "    index_col = [0],\n",
        "    )\n",
        "\n",
        "# restrict to TGD Redditors\n",
        "\n",
        "d = d[d['tgd_pred'] != 0]\n",
        "d.reset_index(inplace = True)\n",
        "\n",
        "d.shape\n",
        "d.head(3)"
      ],
      "metadata": {
        "id": "luhXcEJnTll2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# derived qualitatively (deductively)\n",
        "\n",
        "#tokens = [\n",
        "#    'criminal'\n",
        "#    ]\n",
        "\n",
        "# derived by feature importance (inductively)\n",
        "\n",
        "tokens = [\n",
        "'thank',\n",
        "'sense',\n",
        "'didn',\n",
        "'point',\n",
        "'did',\n",
        "    ]\n",
        "\n",
        "# parse by T_p\n",
        "\n",
        "d_parsed = d[d['sui_re'] == 1]\n",
        "\n",
        "# transform to nltk text object\n",
        "\n",
        "text_col = d_parsed['W'].dropna().tolist()\n",
        "#text_col = d['W'].dropna().tolist()\n",
        "joined_text = ' '.join(text_col)\n",
        "tokenized_text = nltk.word_tokenize(joined_text)\n",
        "nltk_text = Text(tokenized_text)\n",
        "\n",
        "# examine token in context\n",
        "\n",
        "for t in tokens:\n",
        "    nltk_text.concordance(\n",
        "        t,\n",
        "        lines = 50,\n",
        "        width = 100,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "3eJyXtxwRQEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> End of aim_ii_model_02.ipynb"
      ],
      "metadata": {
        "id": "toHJocsXyzbV"
      }
    }
  ]
}