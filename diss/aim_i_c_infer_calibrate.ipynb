{"cells":[{"cell_type":"markdown","metadata":{"id":"gMaLTXLY-8VJ"},"source":["# Passive suicidality in a repressive U.S. political context: Aim I(c)"]},{"cell_type":"markdown","metadata":{"id":"prqwIbCL_AA-"},"source":["_Labels primary analytic sample using tuned RoBERTa sequence classification models. Includes regex for additional explicit suicidal ideation encodings, GPT-4o-enabled disambiguation function to verify intended in-context usage, temperature scaling for calibration._"]},{"cell_type":"markdown","metadata":{"id":"MzCqBG0E_I5C"},"source":["> aim_i_c_infer_calibrate.ipynb<br>\n","> Simone J. Skeen (03-17-2025)"]},{"cell_type":"markdown","metadata":{"id":"3QaJxAlX_dE8"},"source":["1. [Prepare](#scrollTo=bIwQOmXN_1kp)<br>\n","2. [Write](#scrollTo=YRM3LeaUBxE-)<br>\n","&nbsp;&nbsp;[`roberta_predict.py`](#scrollTo=khjrYTtHKb2f)<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;[`load_model`](#scrollTo=2Fwh2tjnKf3G)<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;[`preprocess_data`](#scrollTo=XwxslZHIKoxg)<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;[`predict`](#scrollTo=Y-rIKdaEKw53)<br>\n","&nbsp;&nbsp;[`gpt_assist.py`](xx)<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;[`disambiguate_texts_with_gpt`](#scrollTo=CFF5OAzdgMpg)<br>\n","3. [Infer](#scrollTo=h7QN7U2GCG2M)<br>\n","4. [Visualize](xx)\n"]},{"cell_type":"markdown","metadata":{"id":"bIwQOmXN_1kp"},"source":["### 1. Prepare\n","Installs, imports, and downloads requisite models and packages. Organizes RAP-consistent directory structure.\n","***"]},{"cell_type":"markdown","metadata":{"id":"qYhBwEiTALfk"},"source":["##### _Install_"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8UcyoudQ-y6L"},"outputs":[],"source":["#!python -m spacy download en_core_web_lg --user"]},{"cell_type":"markdown","metadata":{"id":"tvxPo7RGANhu"},"source":["##### _Import_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UxOw27f_APEv"},"outputs":[],"source":["#import en_core_web_lg\n","import glob\n","import matplotlib.font_manager as fm\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as mticker\n","import nltk\n","import numpy as np\n","import openai\n","import os\n","import pandas as pd\n","import random\n","import re\n","import seaborn as sns\n","#import spacy\n","import time\n","import torch\n","import warnings\n","\n","from google.colab import drive\n","from matplotlib.lines import Line2D\n","from nltk.text import Text\n","from scipy.optimize import minimize\n","from textblob import TextBlob\n","from torch.utils.data import(\n","    DataLoader,\n","    TensorDataset,\n","    )\n","from tqdm import tqdm\n","from transformers import(\n","    RobertaForSequenceClassification,\n","    RobertaTokenizer,\n","    )\n","\n","#nltk.download('punkt_tab')\n","#spacy.cli.download('en_core_web_lg')\n","\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = 'all'\n","\n","pd.options.mode.copy_on_write = True\n","\n","pd.set_option(\n","    'display.max_columns',\n","    None,\n","    )\n","\n","pd.set_option(\n","    'display.max_rows',\n","    None,\n","    )\n","\n","warnings.simplefilter(\n","    action = 'ignore',\n","    category = FutureWarning,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"AXGsHFwABBUs"},"source":["##### _Set env variable_"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"t3LTpp_aBAMg"},"outputs":[],"source":["os.environ['OPENAI_API_KEY'] = ' '\n","#os.environ"]},{"cell_type":"markdown","metadata":{"id":"1i_z9TI8BFum"},"source":["##### _Mount gdrive_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S57tG0LMBAQW"},"outputs":[],"source":["drive.mount(\n","    '/content/drive',\n","    force_remount = True,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"1zrWZPPkBOfA"},"source":["##### _Structure dir_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5DE0zpsBAUO"},"outputs":[],"source":["%cd /content/drive/My Drive/Colab/bar_policy_suicidality\n","#%cd /content/drive/My Drive/#<my_project_folder>\n","\n","#%mkdir bar_policy_suicidality\n","#%cd bar_policy_suicidality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1h7O9fqBAXt"},"outputs":[],"source":["#%mkdir inputs outputs code temp"]},{"cell_type":"code","source":["#%cd inputs\n","#%mkdir annotation archives data"],"metadata":{"id":"bvqQ5BXV3kot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#%cd ../outputs\n","#%mkdir models tables figures"],"metadata":{"id":"nb9jASxB3k1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqMpShbZBAa4"},"outputs":[],"source":["bar_policy_suicidality/\n","├── inputs/\n","│   ├── archives\n","│   └── data\n","├── outputs/\n","│   ├── models\n","│   ├── tables\n","│   └── figures\n","├── code/\n","└── temp/"]},{"cell_type":"markdown","metadata":{"id":"YRM3LeaUBxE-"},"source":["### 2. Write\n","Writes and imports requisite custom scripts in .py.\n","***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NjuUvfh8gC4d"},"outputs":[],"source":["%cd code"]},{"cell_type":"markdown","metadata":{"id":"khjrYTtHKb2f"},"source":["#### `roberta_predict.py`"]},{"cell_type":"markdown","metadata":{"id":"2Fwh2tjnKf3G"},"source":["##### `load_model`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JOIbByb1BAeZ"},"outputs":[],"source":["%%writefile roberta_predict.py\n","\n","import torch\n","from torch.utils.data import(\n","    DataLoader,\n","    TensorDataset,\n","    )\n","from transformers import(\n","    RobertaTokenizer,\n","    RobertaForSequenceClassification,\n","    )\n","from tqdm import tqdm\n","import pandas as pd\n","\n","def load_model(model_path, model_class, pretrained_model_name):\n","    \"\"\"\n","    Loads a pre-trained fine-tined RoBERTa from prespecified path model_path.\n","    \"\"\"\n","    model = model_class.from_pretrained(pretrained_model_name)\n","    model.load_state_dict(torch.load(model_path))\n","\n","    # set model to eval mode\n","\n","    model.eval()\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"XwxslZHIKoxg"},"source":["##### `preprocess_data`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1l7lDsZ9BAiR"},"outputs":[],"source":["%%writefile -a roberta_predict.py\n","\n","import torch\n","from torch.utils.data import(\n","    DataLoader,\n","    TensorDataset,\n","    )\n","from transformers import(\n","    RobertaTokenizer,\n","    RobertaForSequenceClassification,\n","    )\n","from tqdm import tqdm\n","import pandas as pd\n","\n","def preprocess_data(tokenizer, texts):\n","    \"\"\"\n","    Tokenizes a list of texts using RobertaTokenizer.\n","    \"\"\"\n","    encoded_texts = tokenizer(\n","        texts,\n","        padding = True,\n","        truncation = True,\n","        return_tensors = 'pt'\n","    )\n","    return encoded_texts"]},{"cell_type":"markdown","metadata":{"id":"Y-rIKdaEKw53"},"source":["##### `predict`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2vVX0S-KwTe"},"outputs":[],"source":["%%writefile -a roberta_predict.py\n","\n","import torch\n","from torch.utils.data import(\n","    DataLoader,\n","    TensorDataset,\n","    )\n","from transformers import(\n","    RobertaTokenizer,\n","    RobertaForSequenceClassification,\n","    )\n","from tqdm import tqdm\n","import pandas as pd\n","\n","def predict(model, tokenizer, texts, batch_size = 8, use_cuda = True):\n","    \"\"\"\n","    Predicts labels and generates label probabilities for a list of texts using RobertaForSequenceClassification and RobertaTokenizer.\n","    \"\"\"\n","    print(f\"\\nTotal number of texts to predict: {len(texts)}\")\n","    encoded_texts = preprocess_data(tokenizer, texts)\n","    dataset = TensorDataset(\n","        encoded_texts['input_ids'],\n","        encoded_texts['attention_mask'],\n","        )\n","    data_loader = DataLoader(\n","        dataset,\n","        batch_size = batch_size,\n","        shuffle = False,\n","        )\n","\n","    print(f\"Batch size: {batch_size}\")\n","    print(f\"Total number of batches: {len(data_loader)}\")\n","\n","    if use_cuda:\n","        model.cuda()\n","\n","    all_predictions = []\n","    all_probabilities = []\n","    # ADDED: Create a list to store the raw logits\n","    all_logits = []\n","\n","    with torch.no_grad():\n","        progress_bar = tqdm(\n","            total = len(data_loader),\n","            desc = \"Predicting\",\n","            leave = False,\n","            )\n","        for batch in data_loader:\n","            input_ids, attention_mask = batch\n","            if use_cuda:\n","                input_ids, attention_mask = input_ids.cuda(), attention_mask.cuda()\n","\n","            outputs = model(\n","                input_ids,\n","                attention_mask = attention_mask,\n","                )\n","\n","            # ADDED: Store the raw logits before applying softmax\n","            logits = outputs.logits\n","            all_logits.extend(logits.cpu().tolist())\n","\n","            probabilities = torch.softmax(\n","                outputs.logits,\n","                #dim = 1,\n","                dim = -1,\n","                )\n","            #predictions = torch.argmax(probabilities, dim = 1).cpu().tolist()\n","            predictions = torch.argmax(probabilities, dim = -1).cpu().tolist()\n","            all_predictions.extend(predictions)\n","            all_probabilities.extend(probabilities.cpu().tolist())\n","            progress_bar.update(1)\n","        progress_bar.close()\n","    # CHANGED: Now also returns raw logits\n","    return all_predictions, all_probabilities, all_logits"]},{"cell_type":"markdown","metadata":{"id":"iyvsQyMyspmq"},"source":["#### `calibrate.py`"]},{"cell_type":"markdown","metadata":{"id":"K2ZQtiW8u8Px"},"source":["##### `temperature_scale`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzpVrwlEtaFM"},"outputs":[],"source":["%%writefile calibrate.py\n","\n","from scipy.optimize import minimize\n","import torch\n","\n","def temperature_scale(logits, temperature):\n","    \"\"\"Apply the temperature scaling to the logits.\"\"\"\n","    return torch.softmax(logits / temperature, dim=-1)"]},{"cell_type":"markdown","metadata":{"id":"UdKNgmXMvBsQ"},"source":["##### `nll_criterion`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZyj25MutaA-"},"outputs":[],"source":["%%writefile -a calibrate.py\n","\n","from scipy.optimize import minimize\n","import torch\n","\n","def nll_criterion(logits, labels, temperature):\n","    \"\"\"Calculate the negative log likelihood.\"\"\"\n","    scaled_probs = temperature_scale(logits, temperature)\n","    return -torch.mean(torch.log(scaled_probs[range(labels.size(0)), labels]))"]},{"cell_type":"markdown","metadata":{"id":"h-ET3n39vFLq"},"source":["##### `find_optimal_temperature`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNA87dlNtZ8v"},"outputs":[],"source":["%%writefile -a calibrate.py\n","\n","from scipy.optimize import minimize\n","import torch\n","\n","def find_optimal_temperature(logits, labels, device):\n","    \"\"\"\n","    Find the optimal temperature for scaling by minimizing the negative log-likelihood\n","    on the given (logits, labels).\n","    \"\"\"\n","    # Move data to the same device as 'device'\n","    logits, labels = logits.to(device), labels.to(device)\n","\n","    def objective(temp):\n","        # temp is a float, but nll_criterion expects a tensor\n","        temp_tensor = torch.tensor([temp], device=device)\n","        return nll_criterion(logits, labels, temp_tensor).item()\n","\n","    # We use 'minimize' from scipy.optimize to find the temperature\n","    # that yields the minimal NLL.\n","    res = minimize(\n","        objective,\n","        x0=1.0,\n","        #bounds=[(0.01, 5.0)],\n","        bounds=[(0.001, 10.0)],\n","        method='L-BFGS-B'\n","    )\n","    return res.x[0]"]},{"cell_type":"markdown","metadata":{"id":"5BBnll65vMWW"},"source":["##### `calibrate_probabilities`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0-shnPxtZ45"},"outputs":[],"source":["%%writefile -a calibrate.py\n","\n","def calibrate_probabilities(d_calibrate, target_col, device):\n","    \"\"\"\n","    Given a dataframe with raw logits in d_calibrate[f'{target_col}_logit']\n","    and ground truth labels in d_calibrate[target_col] (0 or 1),\n","    this function finds the optimal temperature, prints it,\n","    and adds a new column f'{target_col}_calibrated_prob' with the\n","    temperature-scaled probability distribution.\n","    \"\"\"\n","    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # define locally\n","\n","    # 1. Extract logits (list of lists) -> convert to tensor\n","    logits_np = np.stack(d_calibrate[f\"{target_col}_logit\"].values)  # shape: [N, num_labels]\n","    logits_tensor = torch.tensor(logits_np, dtype=torch.float32, device=device)\n","\n","    # 2. Extract the labels and convert to tensor\n","    labels_np = d_calibrate[target_col].values  # e.g., 0 or 1\n","    labels_tensor = torch.tensor(labels_np, dtype=torch.long, device=device)\n","\n","    # 3. Find the optimal temperature\n","    optimal_temp = find_optimal_temperature(logits_tensor, labels_tensor, device)\n","    print(f\"Optimal temperature for '{target_col}': {optimal_temp:.4f}\")\n","\n","    # 4. Apply temperature scaling\n","    with torch.no_grad():\n","        scaled_probs_tensor = temperature_scale(logits_tensor, optimal_temp)\n","    scaled_probs = scaled_probs_tensor.cpu().numpy()  # shape: [N, num_labels]\n","\n","    # 5. Store them in the dataframe\n","    d_calibrate[f\"{target_col}_calibrated_prob\"] = list(scaled_probs)\n","\n","    return d_calibrate"]},{"cell_type":"markdown","metadata":{"id":"4ZRu79TGvRAJ"},"source":["##### `calculate_ece`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_oixwCWmtZz9"},"outputs":[],"source":["%%writefile -a calibrate.py\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def calculate_ece(probabilities, labels, n_bins=10, plot=True):\n","    \"\"\"\n","    Calculate the Expected Calibration Error (ECE) for a binary classifier.\n","    Optionally generates a simple calibration plot (reliability diagram).\n","\n","    This function robustly handles a Pandas Series (or list-like) of probabilities,\n","    where each row can be either:\n","      - a single float (p(class_1)), or\n","      - a 2-element list/array [p(class_0), p(class_1)].\n","\n","    Parameters:\n","    -----------\n","    probabilities : array-like\n","        An iterable of length N, where each element is either:\n","          - A float for p(class_1), OR\n","          - A 2-element list/array for [p(class_0), p(class_1)].\n","    labels : array-like\n","        Ground-truth labels of shape [N], each 0 or 1.\n","    n_bins : int, optional\n","        Number of bins to use for calibration measurement. Default is 10.\n","    plot : bool, optional\n","        Whether to generate a calibration plot (Reliability Diagram). Default is True.\n","\n","    Returns:\n","    --------\n","    ece : float\n","        The expected calibration error.\n","    \"\"\"\n","\n","    # 1) Convert each row into a 2-element [p0, p1]\n","    prob_list = list(probabilities)  # force iteration-friendly structure\n","    processed = []\n","    for i, p in enumerate(prob_list):\n","        # Case A: p is a single float => interpret as p(class_1)\n","        if isinstance(p, float) or isinstance(p, int) or isinstance(p, np.number):\n","            p0 = 1.0 - float(p)\n","            p1 = float(p)\n","            processed.append([p0, p1])\n","\n","        # Case B: p is a list (or tuple, etc.) of length 2 => interpret as [p(class_0), p(class_1)]\n","        elif (isinstance(p, (list, tuple, np.ndarray)) and len(p) == 2):\n","            # Convert to float explicitly\n","            p0 = float(p[0])\n","            p1 = float(p[1])\n","            processed.append([p0, p1])\n","        else:\n","            raise ValueError(\n","                f\"Row {i} has invalid probability format: {p}.\\n\"\n","                \"Must be either a single number or a 2-element list/tuple/array.\"\n","            )\n","\n","    # Now convert processed to a float array of shape [N, 2]\n","    probabilities_2d = np.array(processed, dtype=float)\n","\n","    # 2) Convert labels to a NumPy array of ints\n","    labels = np.array(labels, dtype=int)\n","    if labels.shape[0] != probabilities_2d.shape[0]:\n","        raise ValueError(\"Number of labels must match number of probability rows.\")\n","\n","    # 3) Prepare bins\n","    bins = np.linspace(0, 1, n_bins + 1)\n","    ece = 0.0\n","\n","    # For plotting\n","    bin_accuracies = []\n","    bin_confidences = []\n","    bin_counts = []\n","\n","    # 4) Compute ECE across bins using p(class_1)\n","    for bin_lower, bin_upper in zip(bins[:-1], bins[1:]):\n","        # Indices of samples whose p(class_1) is in [bin_lower, bin_upper)\n","        in_bin = (probabilities_2d[:, 1] >= bin_lower) & (probabilities_2d[:, 1] < bin_upper)\n","\n","        if not in_bin.any():\n","            # If no samples in this bin, record placeholder values\n","            bin_accuracies.append(0.0)\n","            bin_confidences.append((bin_lower + bin_upper) / 2.0)\n","            bin_counts.append(0)\n","            continue\n","\n","        # Accuracy in this bin: fraction of correct predictions\n","        # (argmax(prob) vs label)\n","        bin_accuracy = np.mean(labels[in_bin] == np.argmax(probabilities_2d[in_bin], axis=1))\n","\n","        # Mean confidence for class_1 in this bin\n","        bin_confidence = np.mean(probabilities_2d[in_bin, 1])\n","\n","        # Proportion of total samples that fall into this bin\n","        bin_weight = np.mean(in_bin)\n","\n","        # Accumulate ECE\n","        ece += abs(bin_accuracy - bin_confidence) * bin_weight\n","\n","        # Save for plotting\n","        bin_accuracies.append(bin_accuracy)\n","        bin_confidences.append(bin_confidence)\n","        bin_counts.append(np.sum(in_bin))\n","\n","    # 5) Optional: Generate a calibration plot (Reliability Diagram)\n","    if plot:\n","        plt.figure(figsize=(6, 6))\n","        plt.plot(bin_confidences, bin_accuracies, marker='o', label='Calibration curve')\n","        plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfect calibration')\n","        plt.xlabel('Confidence (Predicted Probability of Positive)')\n","        plt.ylabel('Accuracy (Fraction of Positives)')\n","        plt.title('Calibration Plot (Reliability Diagram)')\n","        plt.legend(loc='upper left')\n","        plt.grid(True)\n","        plt.show()\n","\n","    return ece"]},{"cell_type":"markdown","metadata":{"id":"g1FDL9q_vV18"},"source":["##### `apply_optimized_temperature_scaling`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBic7_zjuOHO"},"outputs":[],"source":["%%writefile -a calibrate.py\n","\n","def apply_optimized_temperature_scaling(d_new, targets, temp_dict, device=\"cpu\"):\n","    \"\"\"\n","    Given a dictionary of previously found optimal temperatures (temp_dict),\n","    apply them to the raw logits in d_new[f\"{t}_logit\"] for each target 't'\n","    to produce calibrated probabilities in d_new[f\"{t}_calibrated_prob\"].\n","    \"\"\"\n","    for t in targets:\n","        # Convert the stored logits to a tensor\n","        logits_np = np.stack(d_new[f\"{t}_logit\"].values)\n","        logits_tensor = torch.tensor(logits_np, dtype=torch.float32, device=device)\n","\n","        # Fetch the saved temperature for this target\n","        temperature = temp_dict[t]\n","\n","        # Compute the scaled probabilities\n","        with torch.no_grad():\n","            scaled_probs_tensor = temperature_scale(logits_tensor, temperature)\n","\n","        scaled_probs = scaled_probs_tensor.cpu().numpy()\n","        d_new[f\"{t}_calibrated_prob\"] = list(scaled_probs)\n","\n","    return d_new"]},{"cell_type":"markdown","metadata":{"id":"zXgKgROjf_Zo"},"source":["#### `gpt_assist.py`"]},{"cell_type":"markdown","metadata":{"id":"CFF5OAzdgMpg"},"source":["##### `disambiguate_text_with_gpt`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cq-ixIiUf-vk"},"outputs":[],"source":["%%writefile gpt_assist.py\n","\n","import pandas as pd\n","import openai\n","import time\n","\n","def disambiguate_text_with_gpt(df, input_column, output_column, system_prompt, prompt_template, model = 'gpt-4o'):\n","    \"\"\"\n","    Transforms text data in a specified df column using GPT based on provided prompts.\n","\n","    Args:\n","        df (pd.DataFrame): df containing the text to be transformed.\n","        input_column (str): name of the input column in the df that contains the text to transform.\n","        output_column (str): name of the output column where the transformed text will be stored.\n","        system_prompt (str): system prompt that sets up the assistant's behavior.\n","        prompt_template (str): template string describing the transformation to be applied to each entry.\n","          Use '{input_text}' as a placeholder for the input text.\n","        model (str, optional): The name of the OpenAI GPT model to use (default = 'gpt-4o').\n","\n","    Returns:\n","        pd.DataFrame: df with new output column added, containing the transformed text.\n","    \"\"\"\n","\n","    # Fx to send row-wise API requests\n","\n","    def call_gpt(input_text):\n","        if pd.isnull(input_text) or input_text.strip() == ' ':\n","            return ' '\n","\n","        prompt = prompt_template.format(input_text = input_text)\n","\n","        try:\n","            response = openai.chat.completions.create(\n","                model = model,\n","                messages = [\n","                    {'role': 'system',\n","                    'content': system_prompt},\n","                    {'role': 'user',\n","                    'content': prompt},\n","                    ],\n","                #max_tokens = 500,\n","                #n = 1,\n","                #temperature = 0,\n","                )\n","\n","            # extract text from API response\n","\n","            result = response.choices[0].message.content.strip()\n","            return result\n","\n","        except Exception as e:\n","            print(f\"Error processing input text: {input_text}\\nError: {str(e)}\")\n","            return input_text ### returns input string in case of error\n","\n","        finally:\n","\n","            # impose delay between API calls\n","\n","            time.sleep(1)\n","\n","    df[output_column] = df[input_column].apply(call_gpt)\n","\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"pKKizUHBgprM"},"source":["#### Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkOmqiu4go9o"},"outputs":[],"source":["from gpt_assist import(\n","    disambiguate_text_with_gpt,\n","    )\n","\n","from roberta_predict import(\n","    load_model,\n","    preprocess_data,\n","    predict,\n","    )\n","\n","from calibrate import(\n","    temperature_scale,\n","    nll_criterion,\n","    find_optimal_temperature,\n","    calibrate_probabilities,\n","    calculate_ece,\n","    apply_optimized_temperature_scaling,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"h7QN7U2GCG2M"},"source":["### 3. Infer-Calibrate\n","Loads fine-tuned RoBERTa by target. Optimizes temperature param $T$ on held-out $\\mathcal{d}$<sub>calibrate</sub>. Labels $\\mathcal{D}$<sub>inference</sub> with strain, explicit targeting, implicit vulnerability encodings.\n","***"]},{"cell_type":"markdown","metadata":{"id":"QoqR30OwwrsR"},"source":["#### Define inference params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1BbkO8-wc_j"},"outputs":[],"source":["# define helper fx\n","\n","def create_params(targets, model_path_prefix, model_base = 'roberta-base'):\n","    return [\n","        {\n","        'target': target,\n","        'model_class': RobertaForSequenceClassification,\n","        'tokenizer_class': RobertaTokenizer,\n","        'pretrained_model_name': model_base,\n","        'model_path': f'{model_path_prefix}{target}_{model_base}_best_tuned_model.bin',\n","        }\n","        for target in targets\n","    ]\n","\n","# set targets\n","\n","targets = [\n","    'asp',\n","    'dep',\n","    'val',\n","    'prg',\n","    'tgd',\n","    'age',\n","    'race',\n","    'dbty',\n","    ]\n","\n","# set models_path_prefix\n","\n","models_path_prefix = '/content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/models/'\n","\n","# create inference params\n","\n","params = create_params(\n","    targets,\n","    models_path_prefix,\n","    )\n","\n","# inspect\n","\n","for p in params:\n","    print(p)"]},{"cell_type":"markdown","metadata":{"id":"tpHFbqmC3EtF"},"source":["#### Calibrate: $\\mathcal{d}$<sub>calibrate</sub>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYsRweVwL3Lx"},"outputs":[],"source":["%cd ../inputs/data\n","\n","# load d_calibrate\n","\n","d_calibrate = pd.read_excel(\n","    'd_calibrate.xlsx',\n","    index_col = [0],\n","    )\n","\n","# delete empty/NaN 'text' cells\n","\n","d_calibrate = d_calibrate[d_calibrate['text'].notnull() & (d_calibrate['text'].str.strip() != ' ')]\n","\n","d_calibrate.info()\n","d_calibrate.head(3)\n","#d_calibrate.tail(3)"]},{"cell_type":"markdown","metadata":{"id":"SVN62wVeoqw5"},"source":["##### _Optimize temperature param $T$_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yevXKYLHN4H"},"outputs":[],"source":["# Determine the device to use\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#model = model.to(device)  # Ensure model is on the right device\n","\n","# coerce 'text' to str\n","\n","d_calibrate['text'] = d_calibrate['text'].astype(str)\n","calibrate_texts = d_calibrate['text'].tolist()\n","\n","# inference loop\n","\n","for p in params:\n","    target = p['target']\n","\n","    # load tokenizers, models\n","\n","    tokenizer = p['tokenizer_class'].from_pretrained(p['pretrained_model_name'])\n","    model = load_model(\n","        p['model_path'],\n","        p['model_class'],\n","        p['pretrained_model_name'],\n","        )\n","\n","    # define device\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # model to device\n","\n","    model = model.to(device)\n","\n","    # infer predictions, probabilities, logits (raw)\n","\n","    predictions, probabilities, logits = predict(\n","        model,\n","        tokenizer,\n","        calibrate_texts,\n","        )\n","\n","    d_calibrate[f'{target}_logit'] = logits\n","    d_calibrate[f'{target}_pred'] = predictions\n","    d_calibrate[f'{target}_prob'] = probabilities\n","\n","    # 4. Calibrate and store the new calibrated probabilities\n","    d_calibrate = calibrate_probabilities(d_calibrate, target, device)\n","\n","# inspect\n","\n","d_calibrate.head(3)\n","\n","# export\n","\n","d_calibrate.to_excel('d_calibrate_labeled.xlsx')"]},{"cell_type":"markdown","metadata":{"id":"AiDpE9SnrEIj"},"source":["##### _Store optimal $T$ values_: `optimal_temps`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLWBGT9crC_R"},"outputs":[],"source":["# create optimal_temps dict\n","\n","optimal_temps = {\n","    'asp': 1.1755,\n","    'dep': 1.2069,\n","    'val': 1.6518,\n","    'prg': 1.2523,\n","    'tgd': 1.5852,\n","    'age': 1.2651,\n","    #'dbty':\n","    'race': 0.8056,\n","    }"]},{"cell_type":"markdown","metadata":{"id":"aPU7CzpPp_02"},"source":["##### _$\\mathcal{d}$<sub>calibrate</sub> ECE: pre-$T$-scaling_"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"mgSCK19Ag1F7"},"outputs":[],"source":["for p in params:\n","    target = p['target']\n","    probabilities = d_calibrate[f'{target}_prob']\n","    print(probabilities.shape)\n","    labels = d_calibrate[target]  # Assuming labels for evaluation are in d_calibrate\n","\n","    ece = calculate_ece(probabilities, labels, n_bins=5, plot=True)\n","    print(\"ECE (matrix input):\", ece)"]},{"cell_type":"markdown","metadata":{"id":"Ty7XZ3HcqHFK"},"source":["##### _$\\mathcal{d}$<sub>calibrate</sub> ECE: post-$T$-scaling_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iY27w89pg1LM"},"outputs":[],"source":["for p in params:\n","    target = p['target']\n","    probabilities = d_calibrate[f'{target}_calibrated_prob']\n","    print(probabilities.shape)\n","    labels = d_calibrate[target]  # Assuming labels for evaluation are in d_calibrate\n","\n","    ece = calculate_ece(probabilities, labels, n_bins=5, plot=True)\n","    print(\"ECE (matrix input):\", ece)"]},{"cell_type":"markdown","metadata":{"id":"z5KRUj-WxhAa"},"source":["#### Label: $\\mathcal{D}$<sub>inference</sub>"]},{"cell_type":"markdown","metadata":{"id":"YT0mkf5eyGsf"},"source":["##### _Import: $\\mathcal{d}$<sub>posts</sub> &rarr; $\\mathcal{D}$<sub>inference</sub>_"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0MxfyoUlxgj0"},"outputs":[],"source":["%cd ../inputs/data\n","\n","d_inference = pd.read_csv('d_posts.csv')\n","\n","# delete empty/NaN 'text' cells\n","\n","d_inference = d_inference[d_inference['text'].notnull() & (d_inference['text'].str.strip() != ' ')]\n","\n","# inspect\n","\n","d_inference.info()\n","counts = d_inference['p_sbrt'].value_counts()\n","print(\"\\n\")\n","print(counts)\n","print(\"\\n\")\n","d_inference.head(3)\n","d_inference.tail(3)"]},{"cell_type":"markdown","metadata":{"id":"tjoiApq8e4yo"},"source":["##### _Join: titles + texts_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W289rAg9e39K"},"outputs":[],"source":["d_inference['text'] = d_inference['p_titl'].astype(str) + ' ' + d_inference['text'].astype(str)\n","\n","d_inference.head(3)"]},{"cell_type":"markdown","metadata":{"id":"59RRv3JzSZZ3"},"source":["##### _Batch: $\\mathcal{D}$<sub>inference</sub> ($n$ = 1.2M) / 10 &rarr;_ `d_inf_{01,...,10}`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDmKoqLjx53j"},"outputs":[],"source":["# batch d_inference / 10\n","\n","d_batches = np.array_split(d_inference, 10)\n","\n","# loop over batches, save d_inf_{01,...,10}\n","\n","for i, batch in enumerate(d_batches, start = 1):\n","    filename = f'd_inf_batch_{i:02d}.csv' ### zero-pad batch number to 2 digits\n","    batch.to_csv(\n","        filename,\n","        index = False,\n","        )\n","    print(f\"Saved {filename} with {len(batch)} rows.\")\n"]},{"cell_type":"markdown","metadata":{"id":"NFPSKKgOnQC5"},"source":["##### _Inference loop:_ `d_inf_{01,...,10}`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xeWEwyfRpYkQ"},"outputs":[],"source":["%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"QeqZXUzanPTP"},"outputs":[],"source":["%cd ../inputs/data\n","\n","# set device\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# loop over d_inf_{01,...,10}\n","\n","for i in range(1, 11):\n","\n","    # construct input .csv filename by batch\n","\n","    csv_in_path = f'd_inf_batch_{i:02d}.csv'\n","    print(f\"\\n======================================================================\")\n","    print(f\"Labeling d_inference batch: {csv_in_path}\")\n","\n","    # read current batch\n","\n","    d_batch = pd.read_csv(csv_in_path)\n","\n","    # coerce 'text' to str\n","\n","    d_batch['text'] = d_batch['text'].astype(str)\n","    inference_texts = d_batch['text'].tolist()\n","\n","    # inference loop\n","\n","    for p in params:\n","        target = p['target']\n","\n","        # load tokenizer, model\n","\n","        tokenizer = p['tokenizer_class'].from_pretrained(p['pretrained_model_name'])\n","        model = load_model(\n","            p['model_path'],\n","            p['model_class'],\n","            p['pretrained_model_name'],\n","        )\n","\n","        # model to device\n","\n","        model.to(device)\n","\n","        # infer predictions, probabilities, (raw) logits\n","\n","        predictions, probabilities, logits = predict(\n","            model,\n","            tokenizer,\n","            inference_texts,\n","        )\n","\n","        # add to present d_batch\n","\n","        d_batch[f'{target}_logit'] = logits\n","        d_batch[f'{target}_pred'] = predictions\n","        d_batch[f'{target}_prob'] = probabilities\n","\n","    # construct output filename, save\n","\n","    csv_out_path = f'd_inf_batch_{i:02d}_labeled.csv'\n","    d_batch.to_csv(csv_out_path, index = False)\n","    print(f\"Saved labeled output: {csv_out_path}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"mg43GSNMEFc8"},"source":["##### _Concatenate:_ `d_inf_{01,...,10}_labeled` &rarr; $\\mathcal{D}$<sub>inf labeled</sub> ($n$ = 1.2M)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"RsznvMvrEEIK"},"outputs":[],"source":["%cd ../inputs/data\n","\n","# global .csv naming convention\n","\n","file_list = sorted(glob.glob(\"d_inf_batch_*_labeled.csv\"))\n","\n","# read, concatenate\n","\n","d_inf_labeled = pd.concat(\n","    (pd.read_csv(i) for i in file_list),\n","    ignore_index = True,\n","    )\n","\n","# inspect\n","\n","d_inf_labeled.info()\n","d_inf_labeled.head(3)\n","\n","# save\n","\n","d_inf_labeled.to_csv('d_inf_labeled.csv')"]},{"cell_type":"markdown","metadata":{"id":"Zypuya79qvbB"},"source":["##### _Apply optimized $T$ scaling post-inference_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MFTIaGvgR7t"},"outputs":[],"source":["%cd ../inputs/data\n","\n","d_inf_labeled = pd.read_csv(\n","    'd_inf_labeled.csv',\n","    index_col = 0,\n","    )\n","\n","d_inf_labeled.info()\n","d_inf_labeled.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-pqDHKQg1QI"},"outputs":[],"source":["# set device\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# define target\n","\n","targets = [\n","    'asp',\n","    'dep',\n","    'val',\n","    'prg',\n","    'tgd',\n","    'age',\n","    'race',\n","    #'dbty', ### T-scale error on dbty - resolve\n","    ]\n","\n","# calibrate {target}_prob in inference set\n","\n","d_inf_labeled = apply_optimized_temperature_scaling(\n","    d_inf_labeled,\n","    targets,\n","    optimal_temps,\n","    device = device,\n","    )\n","\n","d_inf_labeled.info()\n","d_inf_labeled.head(3)"]},{"cell_type":"markdown","metadata":{"id":"qWfxXUmn5Fbd"},"source":["#### RegEx: match\n"]},{"cell_type":"markdown","metadata":{"id":"3i_8_05IikYz"},"source":["##### _Neurodivergences:_ `adhd_re`, `aut_re`, `bpd_re`, `ptsd_re`"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"s6YzOt5L6faP"},"outputs":[],"source":["# 'ndvg' regex patterns\n","\n","patterns = {\n","    'adhd_re': r'\\bADD\\b|\\bADHD\\b',\n","    'aut_re': r'\\bautism\\b|\\bautistic\\b',\n","    'bpd_re': r'\\bborderline\\b|\\bbpd\\b',\n","    'ptsd_re': r'\\bPTSD\\b|\\bCPTSD\\b|\\bC-PTSD\\b'\n","    }\n","\n","# encode regex match\n","\n","for key, p in patterns.items():\n","    case_sensitive = (key == 'adhd') ### matches \"ADD\" not \"add\"\n","    d_inf_labeled[key] = d_inf_labeled['text'].str.contains(\n","        p,\n","        case = (key == 'adhd'),\n","        regex = True,\n","        ).astype(int)\n","\n","# inspect\n","\n","d_inf_labeled.head(3)"]},{"cell_type":"markdown","metadata":{"id":"35ya2svPi57g"},"source":["##### _Explicit disclosed SI:_ `sui_re`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBsJbHHazHAu"},"outputs":[],"source":["%cd /content/drive/My Drive/Colab/bar_policy_suicidality/inputs/data\n","\n","    ### SJS 2/17: redoing 'sui' - delete when done\n","\n","d_inf_labeled = pd.read_csv(\n","    'd_inf_labeled.csv',\n","    index_col = [0],\n","    )\n","\n","d_inf_labeled.info()\n","d_inf_labeled.head(3)\n","#d.tail(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ykoO60rDzorm"},"outputs":[],"source":["#d_inf_labeled = d_inf_labeled.drop(\n","#    'sui_re',\n","#    axis = 1,\n","#    )\n","\n","d_inf_labeled.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hnXn2-rO6fgc"},"outputs":[],"source":["# 'sui' regex pattern\n","\n","sui_re = re.compile(\n","    r'\\bsuicid\\S*|\\bkill\\s+myself\\b',\n","    re.I,\n","    )\n","\n","# pilot sui_re\n","\n","#test_strings = [\n","#    \"I might kill myself today\",\n","#    \"He was feeling suicidal\",\n","#    \"I might  hurt   myself tomorrow\"  # multiple spaces\n","#    ]\n","\n","#for t in test_strings:\n","#    match = sui_re.search(t)\n","#    if match:\n","#        print(f\"Matched: '{match.group(0)}' in '{t}'\")\n","#    else:\n","#        print(f\"No match in: '{t}'\")\n","\n","# match pattern\n","\n","d_inf_labeled['sui_re'] = d_inf_labeled['text'].str.contains(\n","    sui_re,\n","    regex = True,\n","    )\n","\n","# encode match\n","\n","d_inf_labeled['sui_re'] = d_inf_labeled['sui_re'].astype(int)\n","\n","d_inf_labeled.head(3)\n","\n","# reassign: d_inf_labeled_re\n","\n","d_inf_labeled_re = d_inf_labeled\n","\n","# save\n","\n","d_inf_labeled_re.to_csv('d_inf_labeled_re.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"XosN3C3a62n_"},"outputs":[],"source":["d_inf_labeled_re[[\n","    'adhd_re',\n","    'aut_re',\n","    'bpd_re',\n","    'ptsd_re',\n","    'sui_re',\n","    ]].sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7uVguZ-_bT7"},"outputs":[],"source":["# batch d_inf_labeled_re x num_splits\n","\n","num_splits = 10\n","split_size = len(d_inf_labeled_re) // num_splits\n","\n","dfs = {}\n","for i in range(num_splits):\n","    df_name = f'd_inf_labeled_re_{i+1:02d}' ### format idx: 01, 02, ..., 10\n","    dfs[df_name] = d_inf_labeled_re.iloc[i * split_size:(i + 1) * split_size]\n","    dfs[df_name].to_csv(\n","        f'{df_name}.csv',\n","        index = False,\n","        )\n","\n","# add leftover rows to final split\n","\n","if len(d_inf_labeled_re) % num_splits != 0:\n","    last_df_name = f'd_inf_labeled_re_{num_splits:02d}'\n","    dfs[last_df_name] = d_inf_labeled_re.iloc[(num_splits - 1) * split_size:]\n","    dfs[last_df_name].to_csv(\n","        f\"{last_df_name}.csv\",\n","        index = False,\n","        )"]},{"cell_type":"markdown","metadata":{"id":"mZQ4aQp8XUbV"},"source":["#### spaCy EntityRecognizer: encode"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K9lxvJtSCX1o"},"outputs":[],"source":["%cd /content/drive/My Drive/Colab/bar_policy_suicidality/inputs/data\n","\n","import spacy\n","import pandas as pd\n","import nltk\n","from nltk.text import Text\n","\n","# load spaCy model\n","\n","nlp = spacy.load('en_core_web_lg')\n","\n","# define GPE-extraction fx\n","\n","def extract_gpe(text):\n","    doc = nlp(text)\n","    gpes = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n","    return gpes, len(gpes)\n","\n","# loop over d_inf_labeled_re_{01,...,10}\n","\n","num_splits = 10\n","for i in range(1, num_splits + 1):\n","    file_name = f\"d_inf_labeled_re_{i:02d}.csv\"\n","    df_name = f\"d_inf_labeled_re_gpe_{i:02d}\"\n","\n","    df = pd.read_csv(file_name)\n","\n","    # apply GPE-extraction function\n","\n","    df['gpe'], _ = zip(*df['text'].apply(extract_gpe))\n","\n","    #df[[\n","    #    'gpe',\n","    #    'gpe_count',\n","    #    ]] = df['text'].apply(lambda i: pd.Series(extract_gpe(i)))\n","\n","    # save d_inf_labeled_re_gpe_{01,...,10}\n","\n","    df.to_csv(\n","        f\"{df_name}.csv\",\n","        index = False,\n","        )\n","\n","    print(f\"GPEs extracted, {df_name}.csv saved\")"]},{"cell_type":"markdown","metadata":{"id":"x8718JpFhGFQ"},"source":["#### GPT-4o: disambiguate"]},{"cell_type":"markdown","metadata":{"id":"Gv7PCTAt4Tqy"},"source":["##### _Prompts: disclosed U.S. GPE restriction_"]},{"cell_type":"code","source":["# retrieve OpenAI API key\n","\n","openai.api_key = os.getenv('OPENAI_API_KEY')\n","\n","# define system prompt\n","\n","system_prompt = '''\n","    You are an expert at reading social media posts and understanding when a post author is describing their location outside the U.S.\n","    '''\n","\n","# disclosed U.S. GPE prompt\n","\n","gpe_prompt = '''\n","    The following text contains a geographical place name, political entity, or other named entity:\n","\n","    {input_text}\n","\n","    Ensure that:\n","        1.) the geographical place name, political entity, or other named entity is outside the U.S. and\n","        2.) the post author is describing it as their own current location (e.g. \"I am writing from Pakistan\")\n","\n","    If the location is outside of the U.S., output a 1\n","\n","    Otherwise, output a 0.\n","'''\n","\n","# set wd\n","\n","input_dir = '/content/drive/MyDrive/Colab/bar_policy_suicidality/inputs/data'\n","\n","# loop over GPE-encoded batches\n","\n","for i in range(10, 11):\n","    input_filename = f\"d_inf_labeled_re_gpe_{i:02d}.csv\"\n","    output_filename = f\"d_inf_labeled_re_gpe_us_{i:02d}.csv\"\n","\n","    input_filepath = os.path.join(\n","        input_dir,\n","        input_filename,\n","        )\n","    output_filepath = os.path.join(\n","        input_dir,\n","        output_filename,\n","        )\n","\n","    if not os.path.exists(input_filepath):\n","        print(f\"{input_filename} not found.\")\n","        continue\n","\n","    print(f\"Disambiguating {input_filename}\")\n","\n","    d_inf_labeled = pd.read_csv(input_filepath)\n","\n","    # replace empty tuples with 0 in 'gpe'\n","\n","    d_inf_labeled['gpe'] = d_inf_labeled['gpe'].replace('[]', 0)\n","\n","    # extract rows where 'gpe' != 0\n","\n","    d_parsed = d_inf_labeled[d_inf_labeled['gpe'] != 0]\n","\n","    d_parsed = disambiguate_text_with_gpt(\n","        d_parsed,\n","        'text',\n","        'not_us',\n","        system_prompt,\n","        gpe_prompt,\n","    )\n","\n","    # merge to d_inf_labeled_re_gpe{01,...,10}\n","\n","    d_inf_labeled = d_inf_labeled.drop(\n","        columns = 'not_us',\n","        errors = 'ignore',\n","        )\n","    d_inf_labeled = d_inf_labeled.merge(\n","        d_parsed[['id', 'not_us']],\n","        on = 'id',\n","        how = 'left'\n","    )\n","\n","    # replace NaN w/ 0\n","\n","    d_inf_labeled.fillna(\n","        {'not_us': 0},\n","        inplace = True,\n","        )\n","\n","    # save\n","\n","    d_inf_labeled.to_csv(\n","        output_filepath,\n","        index = False,\n","        )\n","    print(f\"Saved: {output_filename}\")"],"metadata":{"id":"ow_rZPsge31-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e9YFKY6U4OTy"},"source":["##### _System prompt: indicator vars_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzXOQolcT_Am"},"outputs":[],"source":["# retrieve OpenAI API key\n","\n","openai.api_key = os.getenv('OPENAI_API_KEY')\n","\n","# define system prompt\n","\n","system_prompt = '''\n","    You are an expert at reading social media posts and understanding when a post author is sincere and referring to themself.\n","    '''"]},{"cell_type":"markdown","metadata":{"id":"sZ7-9HTKVxjX"},"source":["##### _Define static prompt:_ `'sui'` _disambiguation_"]},{"cell_type":"code","source":["# explicit suicidal ideation prompt\n","\n","sui_prompt = '''\n","    The following social media post contains a mention of suicidal ideation:\n","\n","    {input_text}\n","\n","    Ensure that the mention of suicidal ideation is not used in dark humor (\"I will kill myself if I don't win that game\"), refers to the\n","    person writing the post (e.g. \"I feel suicidal\" or \"I want to kill myself\"). If the mention is sincere and self-referential, output a 1.\n","\n","    If the mention is _not_ sincere and self-referential (e.g. \"My brother is suicidal\" or \"My sister said 'I want to kill myself'\"),\n","    output a 0.\n","    '''\n","\n","# set wd\n","\n","input_dir = '/content/drive/MyDrive/Colab/bar_policy_suicidality/inputs/data'\n","output_dir = input_dir\n","\n","# loop over sui-encoded batches\n","\n","for i in range(7, 11):\n","    file_name = f'd_inf_labeled_re_gpe_us_{i:02d}.csv'\n","    input_path = os.path.join(\n","        input_dir,\n","        file_name,\n","        )\n","\n","    if os.path.exists(input_path):\n","        print(f\"Disambiguating: {file_name}\")\n","        d_inf_labeled = pd.read_csv(input_path)\n","\n","        # parse by 'sui_re' == 1\n","\n","        d_parsed = d_inf_labeled[d_inf_labeled['sui_re'] == 1].copy()\n","\n","        if not d_parsed.empty:\n","\n","            # apply disambiguate_text_with_gpt()\n","\n","            d_parsed = disambiguate_text_with_gpt(\n","                d_parsed,\n","                'text',\n","                'sui',\n","                system_prompt,\n","                sui_prompt,\n","                )\n","\n","             # merge to d_inf_labeled_re_gpe_us_{01,...,10}\n","\n","            d_inf_labeled = d_inf_labeled.drop(\n","                columns = 'sui',\n","                errors = 'ignore',\n","                )\n","            d_inf_labeled = d_inf_labeled.merge(\n","                d_parsed[[\n","                    'id',\n","                    'sui']],\n","                on = 'id',\n","                how = 'left',\n","                )\n","\n","        # replace NaN w/ 0 in 'sui' column\n","\n","        d_inf_labeled['sui'].fillna(\n","            0,\n","            inplace = True,\n","            )\n","\n","        # save\n","\n","        output_file = f'd_inf_labeled_re_gpe_us_sui_{i:02d}.csv'\n","        output_path = os.path.join(\n","            output_dir,\n","            output_file,\n","            )\n","        d_inf_labeled.to_csv(\n","            output_path,\n","            index = False,\n","            )\n","\n","        print(f\"Saved: {output_file}\")\n","\n","    else:\n","        print(f\"File not found: {file_name}\")"],"metadata":{"id":"-ye8pD7UhcuG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"em8iFZMhWDYS"},"source":["##### _Define static prompt:_ `'aut'` _disambiguation_"]},{"cell_type":"code","source":["%cd ../inputs/data\n","\n","    ### SJS 3/14: _note_ - I'm doing this well after GPE and sui disambig\n","\n","d_inf_labeled = pd.read_csv(\n","    'd_inf_labeled_long.csv',\n","    index_col = [0],\n","    )\n","\n","d_inf_labeled.info()\n","d_inf_labeled.head(3)"],"metadata":{"id":"vQCdhD5XgoID"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["counts = d_inf_labeled['not_us'].value_counts()\n","print(counts)\n","\n","    ### SJS 3/14: post-'not_us' drop, as it should be"],"metadata":{"id":"x5rYpnV2jSuN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"difdIylhVqnQ"},"outputs":[],"source":["# autism prompt\n","\n","aut_prompt = '''\n","    The following social media post contains a mention of autism:\n","\n","    {input_text}\n","\n","    Ensure that the mention of autism refers to the person writing the post (e.g. \"I have autism\" or\n","    \"I am autistic\"). If the mention is self-referential, output a 1.\n","\n","    If the mention is _not_ self-referential (e.g. \"My brother has autism\" or \"My sister is autistic\"),\n","    output a 0.\n","    '''\n","\n","# d_parsed: 'sui_re' = 1\n","\n","d_parsed = d_inf_labeled[d_inf_labeled['aut_re'] == 1]\n","\n","d_parsed = disambiguate_text_with_gpt(\n","    d_parsed,\n","    'text',\n","    'aut',\n","    system_prompt,\n","    aut_prompt,\n","    )\n","\n","# merge to d_inf_labeled\n","\n","d_inf_labeled = d_inf_labeled.drop(\n","    columns = 'aut',\n","    errors = 'ignore',\n","    )\n","\n","d_inf_labeled = d_inf_labeled.merge(\n","    d_parsed[[\n","        'id',\n","        'aut',\n","        ]],\n","    on = 'id',\n","    how = 'left',\n","    )\n","\n","# replace NaN w/ 0\n","\n","d_inf_labeled.fillna(\n","    {'aut': 0},\n","    inplace = True,\n","    )\n","\n","# save\n","\n","d_inf_labeled.to_csv('d_inf_labeled_long_aut.csv')"]},{"cell_type":"markdown","metadata":{"id":"gAq0Tb47XRWH"},"source":["##### _Define dynamic prompt template:_ `'adhd'`, `'bpd'`, `'ptsd'` _disambiguation_"]},{"cell_type":"code","source":["%cd ../inputs/data\n","\n","    ### SJS 3/15: _note_ - I'm doing this well after GPE and sui disambig - reorganize as RAP asap\n","\n","d_inf_labeled = pd.read_csv(\n","    'd_inf_labeled_long_aut_adhd_bpd.csv',\n","    index_col = [0],\n","    )\n","\n","d_inf_labeled.info()\n","d_inf_labeled.head(3)"],"metadata":{"id":"lrLoPGDN9NiB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#adhd_re_counts = d_inf_labeled['adhd_re'].value_counts()\n","#print(adhd_re_counts)\n","\n","#bpd_re_counts = d_inf_labeled['bpd_re'].value_counts()\n","#print(bpd_re_counts)\n","\n","#ptsd_re_counts = d_inf_labeled['ptsd_re'].value_counts()\n","#print(ptsd_re_counts)"],"metadata":{"collapsed":true,"id":"SiCTzybK9UJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# AD(H)D prompt\n","\n","adhd_prompt = '''\n","    The following social media post contains a mention of ADD or ADHD:\n","\n","    {input_text}\n","\n","    Ensure that the mention of ADD or ADHD refers to the person writing the post (e.g. \"I have ADHD\" or\n","    \"I am struggling with attention deficit disorder\"). If the mention is self-referential, output a 1.\n","\n","    If the mention is _not_ self-referential (e.g. \"My brother is so ADD\" or \"My sister has ADHD\"),\n","    output a 0.\n","    '''\n","\n","# d_parsed: 'adhd_re' = 1\n","\n","d_parsed = d_inf_labeled[d_inf_labeled['adhd_re'] == 1]\n","\n","d_parsed = disambiguate_text_with_gpt(\n","    d_parsed,\n","    'text',\n","    'adhd',\n","    system_prompt,\n","    adhd_prompt,\n","    )\n","\n","# merge to d_inf_labeled\n","\n","d_inf_labeled = d_inf_labeled.drop(\n","    columns = 'adhd',\n","    errors = 'ignore',\n","    )\n","\n","d_inf_labeled = d_inf_labeled.merge(\n","    d_parsed[[\n","        'id',\n","        'adhd',\n","        ]],\n","    on = 'id',\n","    how = 'left',\n","    )\n","\n","# replace NaN w/ 0\n","\n","d_inf_labeled.fillna(\n","    {'adhd': 0},\n","    inplace = True,\n","    )\n","\n","# save\n","\n","d_inf_labeled.to_csv('d_inf_labeled_long_aut_adhd.csv')"],"metadata":{"id":"SvW5aA_V_UJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BPD prompt\n","\n","bpd_prompt = '''\n","    The following social media post contains a mention of borderline personality disorder (BPD):\n","\n","    {input_text}\n","\n","    Ensure that the mention of borderline personality disorder or BPD refers to the person writing the post (e.g.\n","    \"I have BPD\" or \"I struggle with borderline personality\"). If the mention is self-referential, output a 1.\n","\n","    If the mention is _not_ self-referential (e.g. \"My brother has BPD\" or \"My sister has borderline\"),\n","    output a 0.\n","    '''\n","\n","# d_parsed: 'bpd_re' = 1\n","\n","d_parsed = d_inf_labeled[d_inf_labeled['bpd_re'] == 1]\n","\n","d_parsed = disambiguate_text_with_gpt(\n","    d_parsed,\n","    'text',\n","    'bpd',\n","    system_prompt,\n","    bpd_prompt,\n","    )\n","\n","# merge to d_inf_labeled\n","\n","d_inf_labeled = d_inf_labeled.drop(\n","    columns = 'bpd',\n","    errors = 'ignore',\n","    )\n","\n","d_inf_labeled = d_inf_labeled.merge(\n","    d_parsed[[\n","        'id',\n","        'bpd',\n","        ]],\n","    on = 'id',\n","    how = 'left',\n","    )\n","\n","# replace NaN w/ 0\n","\n","d_inf_labeled.fillna(\n","    {'bpd': 0},\n","    inplace = True,\n","    )\n","\n","# save\n","\n","d_inf_labeled.to_csv('d_inf_labeled_long_aut_adhd_bpd.csv')"],"metadata":{"id":"GJfAYLJf1kH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (C)PTSD prompt\n","\n","ptsd_prompt = '''\n","    The following social media post contains a mention of post-traumatic stress disorder (PTSD):\n","\n","    {input_text}\n","\n","    Ensure that the mention of post-traumatic stress disorder or PTSD (CPTSD may also be used) refers to the person\n","    writing the post (e.g. \"I have PTSD\" or \"I struggle with post-traumatic stress\"). If the mention is self-referential,\n","    output a 1.\n","\n","    If the mention is _not_ self-referential (e.g. \"My brother has PTSD\" or \"My sister got CPTSD from abuse\"),\n","    output a 0.\n","    '''\n","\n","# d_parsed: 'ptsd_re' = 1\n","\n","d_parsed = d_inf_labeled[d_inf_labeled['ptsd_re'] == 1]\n","\n","d_parsed = disambiguate_text_with_gpt(\n","    d_parsed,\n","    'text',\n","    'ptsd',\n","    system_prompt,\n","    ptsd_prompt,\n","    )\n","\n","# merge to d_inf_labeled\n","\n","d_inf_labeled = d_inf_labeled.drop(\n","    columns = 'ptsd',\n","    errors = 'ignore',\n","    )\n","\n","d_inf_labeled = d_inf_labeled.merge(\n","    d_parsed[[\n","        'id',\n","        'ptsd',\n","        ]],\n","    on = 'id',\n","    how = 'left',\n","    )\n","\n","# replace NaN w/ 0\n","\n","d_inf_labeled.fillna(\n","    {'ptsd': 0},\n","    inplace = True,\n","    )\n","\n","# save\n","\n","d_inf_labeled.to_csv('d_inf_labeled_long_aut_adhd_bpd_ptsd.csv')"],"metadata":{"id":"3ufsuGdwgkZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cleanup\n","\n","%cd ../inputs/data\n","\n","d_inf_labeled = pd.read_csv(\n","    'd_inf_labeled_long_aut_adhd_bpd_ptsd.csv',\n","    index_col = [0],\n","    )\n","\n","d_inf_labeled.info()\n","d_inf_labeled.head(3)"],"metadata":{"id":"WS4HCxOYsBDr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = [\n","    'aut',\n","    'adhd',\n","    'bpd',\n","    'ptsd',\n","    ]\n","\n","d = d_inf_labeled.copy()\n","\n","d[cols] = d[cols].astype(str).applymap(lambda x: int('1' in x))\n","\n","d.dtypes\n","d.head(3)"],"metadata":{"id":"z0f7ZUx8sMBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save subsample to crosswalk\n","\n","#d_inspect_gpt = d_inf_labeled.sample(\n","#    n = 100000,\n","#    random_state = 56,\n","#    )\n","\n","d_inspect_gpt = d_inf_labeled.iloc[:100000]\n","d_inspect_gpt.to_csv('d_inspect_gpt.csv')\n","\n","#d_inspect_int = d.sample(\n","#    n = 100000,\n","#    random_state = 56,\n","#    )\n","\n","d_inspect_int = d.iloc[:100000]\n","d_inspect_int.to_csv('d_inspect_int.csv')"],"metadata":{"id":"5EsB_elnsZU5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d.to_csv('d_inf_labeled_long_ndvg.csv')"],"metadata":{"id":"SGNkg2hZw0Bv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####################################### LIWC-22 encoding #######################################"],"metadata":{"id":"bHLFFiKmRD2B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### _old old old_"],"metadata":{"id":"LfPu-3aIRSqy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1M_z_89jUkHe"},"outputs":[],"source":["    ### SJS 3/15: nested f-strings impossible to balance on short notice; pulling out of loop\n","\n","'''\n","# define neurodivergences\n","\n","neurodivergences = [\n","    'adhd',\n","    'bpd',\n","    'ptsd',\n","    ]\n","\n","for n in neurodivergences:\n","    # 1. Build dynamic references for columns and prompt text\n","    match_col = f'{n}_re'      # e.g. 'aut_re', 'bpd_re', 'ptsd_re'\n","    encode_col = n               # e.g. 'aut', 'bpd', 'ptsd'\n","\n","    # 2. Define the dynamic user prompt\n","    #    Here, we simply uppercase the mention to match the style from your example.\n","    user_prompt = f'''\n","        The following social media post contains a mention of \"{n.upper()}\":\n","\n","       {input_text}\n","\n","        Ensure that the mention of \"{n.upper()}\" refers to the person writing the post (e.g. \"I have {n.upper()}\").\n","        If the mention is self-referential, output a 1.\n","\n","        If the mention is _not_ self-referential (e.g. \"My friend has {n.upper()}\"), output a 0.\n","        '''\n","\n","    # 3. Subset rows where the regex column == 1\n","    d_parsed = d_inf_labeled[d_inf_labeled[match_col] == 1].copy()\n","\n","    # 4. Call your disambiguation function\n","\n","\n","    d_parsed = disambiguate_text_with_gpt(\n","        d_parsed,\n","        'text',\n","        encode_col,\n","        system_prompt,\n","        user_prompt,\n","        )\n","\n","    # This is your existing function signature for reference\n","    # def disambiguate_text_with_gpt(\n","    #     df,\n","    #     text_column,\n","    #     output_column,\n","    #     system_prompt,\n","    #     user_prompt,\n","    # ):\n","    #     ... (implementation here)\n","\n","    # 5. Merge the results back onto d_inference\n","    #    (First drop the output column if it already exists)\n","    d_inf_labeled = d_inf_labeled.drop(\n","        columns = [encode_col],\n","        errors='ignore',\n","        )\n","\n","    d_inf_labeled = d_inf_labeled.merge(\n","        d_parsed[['id', encode_col]],\n","        on = 'id',\n","        how = 'left',\n","        )\n","\n","    # 6. Replace any NaN with 0 (meaning no or non-self-referential mention)\n","    d_inf_labeled.fillna(\n","        {encode_col: 0},\n","        inplace = True,\n","        )\n","\n","# At this point, your d_inference dataframe now contains\n","# 'aut', 'bpd', and 'ptsd' columns (in addition to any others),\n","# each with 1 if the mention was self-referential, or 0 otherwise.\n","\n","\n","\n","# save\n","\n","d_inf_labeled.to_csv('d_inf_labeled_aut_adhd_bpd_ptsd.csv')"]},{"cell_type":"markdown","source":["##### _Reconcatenate disambiguation batches_"],"metadata":{"id":"eXdnW99oBDC5"}},{"cell_type":"code","source":["%cd ../inputs/data\n","\n","file_pattern = 'd_inf_labeled_re_gpe_us_sui_*.csv'\n","\n","csv_files = glob.glob(file_pattern)\n","\n","d = pd.concat(\n","    [pd.read_csv(file) for file in csv_files],\n","    ignore_index = True,\n","    )\n","\n","# detect mixed dtypes\n","\n","mixed_dtype_cols = [col for col in d.columns if d[col].map(type).nunique() > 1]\n","print(\"Columns with mixed data types:\", mixed_dtype_cols)"],"metadata":{"collapsed":true,"id":"EYzFqEx1BCic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# inspect mixed dtypes\n","\n","non_string_psbrt = d[~d['p_sbrt'].apply(lambda i: isinstance(i, str))]\n","non_string_ptitl = d[~d['p_titl'].apply(lambda i: isinstance(i, str))]\n","\n","invalid_not_us = d[~d['not_us'].isin([0, 1])]\n","invalid_sui = d[~d['sui'].isin([0, 1])]\n","\n","non_string_psbrt.to_csv('non_string_psbrt.csv')\n","non_string_ptitl.to_csv('non_string_ptitl.csv')\n","invalid_not_us.to_csv('invalid_not_us.csv')\n","invalid_sui.to_csv('invalid_sui.csv')"],"metadata":{"id":"tkexPIh8FEfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# drop 'p_titl' - already joined to 'text' for inference\n","\n","d.drop(\n","    columns = ['p_titl'],\n","    inplace = True,\n","    )\n","\n","# drop 'p_sbrt' = ' '\n","\n","d.dropna(subset = ['p_sbrt'])"],"metadata":{"id":"_Lef0H3PFElm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert GPT-4o neg explanations to 0\n","\n","d['not_us'] = pd.to_numeric(\n","    d['not_us'],\n","    errors = 'coerce',\n","    ).fillna(0).astype(int)\n","\n","d['sui'] = pd.to_numeric(\n","    d['sui'],\n","    errors = 'coerce',\n","    ).fillna(0).astype(int)\n","\n","# verify\n","\n","unique_not_us = d['not_us'].unique()\n","unique_sui = d['sui'].unique()\n","\n","print(\"Unique values in 'not_us':\", unique_not_us)\n","print(\"Unique values in 'sui':\", unique_sui)"],"metadata":{"collapsed":true,"id":"r3bP6SXtFEqg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# drop if disclosed GPE is non-U.S.\n","\n","d = d[d['not_us'] == 0]\n","\n","d.info()\n","d.head(3)"],"metadata":{"id":"dP6pdiFKXqxn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBmVK5LQUe1I"},"source":["##### _Aggregate 'compound strain'_ `'cpnd'` _var_"]},{"cell_type":"code","source":["%cd ../inputs/data\n","\n","d = pd.read_csv(\n","    'd_inf_labeled_long_ndvg_placebo.csv',\n","    index_col = [0],\n","    )\n","\n","d.info()\n","d.head(3)"],"metadata":{"id":"TuIYdhuBRadd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# drop '*_re' col\n","\n","d = d.drop([\n","    'adhd_re',\n","    'aut_re',\n","    'bpd_re',\n","    'ptsd_re'],\n","    axis = 1,\n","    )\n","\n","d.info()"],"metadata":{"id":"xQcZ62A4VjwQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute mdn\n","\n","tech_mdn = d['tech'].median()\n","\n","# encode >mdn\n","\n","d['tech_high'] = (d['tech'] > tech_mdn).astype(int)\n","\n","# display mdn\n","\n","print(f\"\\n'tech' Mdn: {tech_mdn:.6f}\")\n","\n","# inspect\n","\n","d.info()"],"metadata":{"collapsed":true,"id":"I3_TWlhva79B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count_ones = d['tech_high'].sum()\n","print(\"Count of 1s in 'tech_high':\", count_ones)\n","\n","d_sampled_tech_high = d[d['tech_high'] == 0].sample(\n","    n = 1000,\n","    random_state = 56,\n","    )\n","d_sampled_tech_high[['tech', 'tech_high']].head(100)"],"metadata":{"id":"8qj1NnIcb9wU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['adhd', 'aut', 'bpd', 'ptsd']\n","\n","# Count of 1s\n","count_ones = d[cols].sum()\n","\n","# Total rows per column\n","total_rows = d[cols].count()\n","\n","# Percentage of 1s\n","percent_ones = (count_ones / total_rows) * 100\n","\n","# Combine into a dataframe for better readability\n","d_summary = pd.DataFrame({'Count of 1s': count_ones, 'Percentage of 1s': percent_ones})\n","\n","d_summary.head(10)"],"metadata":{"id":"Qi-PBYumXchH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v161R0QJUeMh"},"outputs":[],"source":["d['cpnd_pred'] = (d[[\n","    'asp_pred',\n","    'dep_pred',\n","    'val_pred']].sum(axis = 1) == 3).astype(int)\n","\n","d.info()\n","d.head(3)"]},{"cell_type":"markdown","metadata":{"id":"lGeMarDgyYz4"},"source":["##### _$\\mathcal{D}$<sub>inf labeled long</sub>: includes texts, logits, softmax probs, $T$-scaled probs_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4gNNpO4JCeR"},"outputs":[],"source":["# save\n","\n","d.to_csv('d_inf_labeled_long.csv')"]},{"cell_type":"markdown","metadata":{"id":"1dLwc-PHzEKj"},"source":["##### _$\\mathcal{D}$<sub>inf labeled long</sub> &rarr; $\\mathcal{D}$<sub>inf labeled short</sub>: datetime and indicator vars only (Stata-friendly)_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31TL_-4oXBcI"},"outputs":[],"source":["d_inf_labeled_short = d[[\n","    'p_date',\n","    'id',\n","    'n_cmnt',\n","    'p_sbrt',\n","    'asp_pred',\n","    'dep_pred',\n","    'val_pred',\n","    'cpnd_pred',\n","    'prg_pred',\n","    'tgd_pred',\n","    'age_pred',\n","    'race_pred',\n","    'dbty_pred',\n","    'adhd',\n","    'aut',\n","    'bpd',\n","    'ptsd',\n","    'sui',\n","    'tech_high',\n","    ]].copy()\n","\n","\n","# convert 'p_date' so Stata datatime str\n","\n","d_inf_labeled_short['p_date_str'] = pd.to_datetime(d_inf_labeled_short['p_date']).dt.strftime('%-m/%-d/%Y')\n","\n","# rename\n","\n","d_inf_labeled_short.rename(\n","    columns = {\n","        'asp_pred': 'asp',\n","        'dep_pred': 'dep',\n","        'val_pred': 'val',\n","        'cpnd_pred': 'cpnd',\n","        'prg_pred': 'prg',\n","        'tgd_pred': 'tgd',\n","        'age_pred': 'age',\n","        'race_pred': 'race',\n","        'dbty_pred': 'dbty',\n","        'tech_high': 'tech',\n","    }, inplace = True,\n","    )\n","\n","# reset index\n","\n","d.reset_index(\n","    drop = True,\n","    inplace = True,\n","    )\n","\n","# verify\n","\n","d_inf_labeled_short.info()\n","d_inf_labeled_short.head(3)\n","\n","# save\n","\n","d_inf_labeled_short.to_csv('d_inf_labeled_short.csv')"]},{"cell_type":"markdown","metadata":{"id":"uSsSRRJ3aEfM"},"source":["> End of aim_i_c_infer_calibrate.ipynb"]}],"metadata":{"colab":{"collapsed_sections":["khjrYTtHKb2f","iyvsQyMyspmq","QoqR30OwwrsR","tpHFbqmC3EtF","YT0mkf5eyGsf","tjoiApq8e4yo","59RRv3JzSZZ3","NFPSKKgOnQC5","mg43GSNMEFc8","Zypuya79qvbB","3i_8_05IikYz","35ya2svPi57g","mZQ4aQp8XUbV","Gv7PCTAt4Tqy","e9YFKY6U4OTy","sZ7-9HTKVxjX","em8iFZMhWDYS","gAq0Tb47XRWH","LfPu-3aIRSqy","eXdnW99oBDC5"],"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNPW7q+HHRHYt0xJgpwQww5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}