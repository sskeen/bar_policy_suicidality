{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "M74zFq4IRmEw",
        "v55Ahsrf7lhk",
        "LvWy_YdufcZ8",
        "lZ_RbpSm7F4f",
        "3jc7s7biFt8X",
        "HRkvN5uMGFic",
        "8vNbiLhbHm65",
        "Z29zZpHtJ3r-",
        "Lp3UMUKzLGIL",
        "myuWu6YoLlpx",
        "ztDyCILbP10O",
        "1dsFrT1ZYOIv",
        "PSda-s9XYcc4",
        "5ssC99HPZH84",
        "ZzSVBMiubjEu",
        "aaI7x86-xZRR"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Passive suicidality in a repressive U.S. political context: Aim I"
      ],
      "metadata": {
        "id": "P_1FyjHT8rsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_WIP - NOT FOR DISTRIBUTION_\n",
        "\n",
        "_Imports, re-indexes by date, cleans, reduces, restricts by timeframe; permits regex pattern-matched purposive (Wave 1) and random (Wave 2) sampling and named entity redaction of PushShift/Arctic Shift .jsonl Reddit archives for .xlsx annotation. Computes Cohen's $\\kappa$ post-annotation. Performs LLM-assisted per-tag triangulation of annotation discrepancies._"
      ],
      "metadata": {
        "id": "eGACjGtI8uz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> aim_i_annotate_triangulate_iaa.ipynb<br>\n",
        "> Simone J. Skeen (10-23-2024)"
      ],
      "metadata": {
        "id": "QTAsNYPy87Ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Prepare](#scrollTo=R4qNxJPa9Cmq)\n",
        "2. [Write](#scrollTo=WTtuLBqA-z6Q)\n",
        "2. [Pre-annotation](#scrollTo=lZ_RbpSm7F4f)\n",
        "3. [Wave I: purposive](#scrollTo=ou-3A98QE_-T)\n",
        "4. [Wave II: random](#scrollTo=5ssC99HPZH84)\n",
        "5. [Post-annotation](#scrollTo=ZzSVBMiubjEu)\n",
        "6. [Human-LLM triangulation](#scrollTo=eMblXk-8_Bd4)\n",
        "8. [Visualize](#scrollTo=aaI7x86-xZRR)"
      ],
      "metadata": {
        "id": "LMwVi-rs8_A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Prepare\n",
        "Installs, imports, and downloads requisite models and packages.\n",
        "***"
      ],
      "metadata": {
        "id": "R4qNxJPa9Cmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install**"
      ],
      "metadata": {
        "id": "GIkPRC-t9iqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nQi-GlS8mJn"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "\n",
        "%pip install irrCAC\n",
        "%pip install openai\n",
        "#%pip install --upgrade openai\n",
        "#%pip install --upgrade pydantic\n",
        "\n",
        "!python -m spacy download en_core_web_lg --user"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import**"
      ],
      "metadata": {
        "id": "SwM5g9Ua9kPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_lg\n",
        "import gzip\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import spacy\n",
        "import time\n",
        "import warnings\n",
        "import webbrowser\n",
        "\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "from irrCAC.raw import CAC\n",
        "from openai import OpenAI\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "spacy.cli.download('en_core_web_lg')\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "pd.set_option(\n",
        "              'display.max_columns',\n",
        "              None,\n",
        "              )\n",
        "pd.set_option(\n",
        "              'display.max_rows',\n",
        "              None,\n",
        "              )\n",
        "\n",
        "warnings.simplefilter(\n",
        "                      action = 'ignore',\n",
        "                      category = FutureWarning,\n",
        "                      )\n",
        "\n",
        "#!python -m prodigy stats"
      ],
      "metadata": {
        "id": "-k0YY2Q29OqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set env variables**"
      ],
      "metadata": {
        "id": "sRUhDguA3TPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "os.environ"
      ],
      "metadata": {
        "id": "uqz8qViv3RBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount gdrive**"
      ],
      "metadata": {
        "id": "t-T6lCur9ngQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\n",
        "            '/content/drive',\n",
        "            #force_remount = True,\n",
        "            )"
      ],
      "metadata": {
        "id": "24OEbaCf9SCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Structure directories**"
      ],
      "metadata": {
        "id": "ESD3m6vO-K4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality\n",
        "#%cd /content/drive/My Drive/#<my_project_folder>\n",
        "\n",
        "#%mkdir bar_policy_suicidality\n",
        "#%cd bar_policy_suicidality"
      ],
      "metadata": {
        "id": "RVoEGq07-KU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%mkdir code inputs"
      ],
      "metadata": {
        "id": "-PZ_VuQX-RrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd inputs\n",
        "#%mkdir annotation archives data"
      ],
      "metadata": {
        "id": "Wl-TA8JI-VDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bar_policy_suicidality/\n",
        "├── code\n",
        "└── inputs/\n",
        "    ├── annotation\n",
        "    ├── archives\n",
        "    │   └── ### archive name TKTK\n",
        "    └── data"
      ],
      "metadata": {
        "id": "4lVIMkY4-hky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Write\n",
        "Writes and imports condense.py, redact.py, triangulate.py, llm_assist.py.\n",
        "***"
      ],
      "metadata": {
        "id": "WTtuLBqA-z6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd code"
      ],
      "metadata": {
        "id": "Xtpbph1QAsDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### condense.py"
      ],
      "metadata": {
        "id": "M74zFq4IRmEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_subreddit_dataframe_condense_**"
      ],
      "metadata": {
        "id": "LgUBDBFzRv7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile condense.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def subreddit_dataframe_condense(df):\n",
        "    \"\"\"\n",
        "    Reassigns Pushshift archives to condensed df for annotation, assigns columns for strain,\n",
        "    explicit targeting, implicit vulnerability tags\n",
        "    \"\"\"\n",
        "    df = df[[\n",
        "             'author',\n",
        "             'created_utc',\n",
        "             'date',\n",
        "             'id',\n",
        "             'num_comments',\n",
        "             'selftext',\n",
        "             'subreddit',\n",
        "             'title',\n",
        "             ]].copy()\n",
        "\n",
        "    df.rename(\n",
        "              columns = {\n",
        "                         'author': 'p_au',\n",
        "                         'created_utc': 'p_utc',\n",
        "                         'date': 'p_date',\n",
        "                         'id': 'p_id',\n",
        "                         'num_comments': 'n_cmnt',\n",
        "                         'selftext': 'text',\n",
        "                         'subreddit': 'sbrt',\n",
        "                         'title': 'p_titl',\n",
        "                         }, inplace = True,\n",
        "            )\n",
        "\n",
        "    df = df.assign(\n",
        "                   asp = ' ',      ### s_1...3 strains\n",
        "                   asp_rtnl = ' ',\n",
        "                   dep = ' ',\n",
        "                   dep_rtnl = ' ',\n",
        "                   val = ' ',\n",
        "                   val_rtnl = ' ',\n",
        "                   prg = ' ',      ### E_1,2 explicit targeting\n",
        "                   tgd = ' ',\n",
        "                   age = ' ',      ### I_1...3 implicit vulnerabilities\n",
        "                   race = ' ',\n",
        "                   dbty = ' ',\n",
        "                   insb = ' ',     ### insubstantial\n",
        "                   )\n",
        "\n",
        "    df = df[~df['text'].isin([\n",
        "                              '[deleted]',\n",
        "                              '[removed]',\n",
        "                              ])]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "I9PDiJDrRknq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_subreddit_parse_**"
      ],
      "metadata": {
        "id": "Ruq8Tg6FR1Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a condense.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def subreddit_parse(df, col):\n",
        "    \"\"\"\n",
        "    Parses df by subreddit, returns dict 'sub_d' of subreddit-specific df objects.\n",
        "    \"\"\"\n",
        "    uniq_val = df[col].unique()\n",
        "    sub_d = {}\n",
        "    for val in uniq_val:\n",
        "        sub_d[f'd_{val}'] = df[df[col] == val].copy()\n",
        "\n",
        "    return sub_d"
      ],
      "metadata": {
        "id": "CKm7MbRGRkuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### redact.py"
      ],
      "metadata": {
        "id": "v55Ahsrf7lhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile redact.py\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "def ner_redact_post_texts(p_text):\n",
        "    \"\"\"\n",
        "    Redacts all named entities recognized by spaCy EntityRecognizer, replaces with <|PII|> pseudo-word token.\n",
        "    \"\"\"\n",
        "    ne = list(\n",
        "              [\n",
        "               'PERSON',   ### people, including fictional\n",
        "               'NORP',     ### nationalities or religious or political groups\n",
        "               'FAC',      ### buildings, airports, highways, bridges, etc.\n",
        "               'ORG',      ### companies, agencies, institutions, etc.\n",
        "               #'GPE',     ### countries, cities, states\n",
        "               'LOC',      ### non-GPE locations, mountain ranges, bodies of water\n",
        "               'PRODUCT',  ### objects, vehicles, foods, etc. (not services)\n",
        "               'EVENT',    ### named hurricanes, battles, wars, sports events, etc.\n",
        "               ]\n",
        "                )\n",
        "\n",
        "    doc = nlp(p_text)\n",
        "    ne_to_remove = []\n",
        "    final_string = str(p_text)\n",
        "    for sent in doc.ents:\n",
        "        if sent.label_ in ne:\n",
        "            ne_to_remove.append(str(sent.text))\n",
        "    for n in range(len(ne_to_remove)):\n",
        "        final_string = final_string.replace(\n",
        "                                            ne_to_remove[n],\n",
        "                                            '<|PII|>',\n",
        "                                            )\n",
        "    return final_string"
      ],
      "metadata": {
        "id": "K9t5avvv7kwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### triangulate.py"
      ],
      "metadata": {
        "id": "LvWy_YdufcZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_calculate_kappa_by_cycle_**"
      ],
      "metadata": {
        "id": "cG77RaUffcJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile triangulate.py\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def calculate_kappa_by_cycle(cycle_num):\n",
        "    \"\"\"\n",
        "    Calculate Cohen's Kappa and encode disagreements between independent annotators across multiple cycles.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    cycle_num : int\n",
        "        Annotation cycle number, used to load the corresponding Excel files (e.g., cycle 0, cycle 1).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    d : pd.DataFrame\n",
        "        Processed df after merging, includes encoded disagreements in *_dis columns.\n",
        "\n",
        "    kappa_results : dict\n",
        "        A dictionary containing the Cohen's Kappa scores for each indepednently co-annotated target.\n",
        "    \"\"\"\n",
        "    # read independently annotated files\n",
        "\n",
        "    d_sd = pd.read_excel(f'd_cycle{cycle_num}_sd.xlsx', index_col = [0])\n",
        "    d_sd.columns = [f'{col}_sd' for col in d_sd.columns]\n",
        "\n",
        "    d_ss = pd.read_excel(f'd_cycle_{cycle_num}_ss.xlsx', index_col = [0])\n",
        "    d_ss.columns = [f'{col}_ss' for col in d_ss.columns]\n",
        "\n",
        "    # merge\n",
        "\n",
        "    d = pd.merge(\n",
        "                 d_sd,\n",
        "                 d_ss,\n",
        "                 left_index = True,\n",
        "                 right_index = True,\n",
        "                 )\n",
        "\n",
        "    # housekeeping\n",
        "\n",
        "    targets = [\n",
        "               'asp_sd', 'asp_ss',\n",
        "               'dep_sd', 'dep_ss',\n",
        "               'val_sd', 'val_ss',\n",
        "               'prg_sd', 'prg_ss',\n",
        "               'tgd_sd', 'tgd_ss',\n",
        "               'age_sd', 'age_ss',\n",
        "               'race_sd', 'race_ss',\n",
        "               'dbty_sd', 'dbty_ss',\n",
        "               'insb_sd', 'insb_ss',\n",
        "              ]\n",
        "\n",
        "    texts = [\n",
        "             'text_sd', 'text_ss',\n",
        "             'asp_rtnl_sd', 'asp_rtnl_ss',\n",
        "             'dep_rtnl_sd', 'dep_rtnl_ss',\n",
        "             'val_rtnl_sd', 'val_rtnl_ss',\n",
        "             ]\n",
        "\n",
        "    d[targets] = d[targets].apply(\n",
        "                                  pd.to_numeric,\n",
        "                                  errors = 'coerce',\n",
        "                                  )\n",
        "    d[targets] = d[targets].fillna(0)\n",
        "    d[texts] = d[texts].replace(' ', '.')\n",
        "\n",
        "    d = d[[\n",
        "           'p_id_sd', 'p_id_ss', ### sense-check for bad merge\n",
        "           'text_sd',\n",
        "           'asp_sd', 'asp_ss',\n",
        "           'asp_rtnl_sd', 'asp_rtnl_ss',\n",
        "           'dep_sd', 'dep_ss',\n",
        "           'dep_rtnl_sd', 'dep_rtnl_ss',\n",
        "           'val_sd', 'val_ss',\n",
        "           'val_rtnl_sd', 'val_rtnl_ss',\n",
        "           'prg_sd', 'prg_ss',\n",
        "           'tgd_sd', 'tgd_ss',\n",
        "           'age_sd', 'age_ss',\n",
        "           'race_sd', 'race_ss',\n",
        "           'dbty_sd', 'dbty_ss',\n",
        "           'insb_sd', 'insb_ss',\n",
        "           ]].copy()\n",
        "\n",
        "    d.rename(\n",
        "             columns = {\n",
        "                        'text_sd': 'text',\n",
        "                        }, inplace = True,\n",
        "            )\n",
        "\n",
        "    # kappa Fx\n",
        "\n",
        "    def calculate_kappa(d, col_sd, col_ss):\n",
        "        return cohen_kappa_score(d[col_sd], d[col_ss])\n",
        "\n",
        "    col_pairs = [\n",
        "                 ('asp_sd', 'asp_ss'),\n",
        "                 ('dep_sd', 'dep_ss'),\n",
        "                 ('val_sd', 'val_ss'),\n",
        "                 #('prg_sd', 'prg_ss'),\n",
        "                 #('tgd_sd', 'tgd_ss'),\n",
        "                 #('age_sd', 'age_ss'),\n",
        "                 #('race_sd', 'race_ss'),\n",
        "                 #('dbty_sd', 'dbty_ss'),\n",
        "                 ]\n",
        "\n",
        "    # initialize dict\n",
        "\n",
        "    kappa_results = {}\n",
        "\n",
        "    # kappa loop\n",
        "\n",
        "    for col_sd, col_ss in col_pairs:\n",
        "        kappa = calculate_kappa(d, col_sd, col_ss)\n",
        "        kappa_results[f'{col_sd} and {col_ss}'] = kappa\n",
        "\n",
        "    for pair, kappa in kappa_results.items():\n",
        "        print(f\"Cohen's Kappa for {pair}: {kappa:.2f}\")\n",
        "\n",
        "    # dummy code disagreements Fx\n",
        "\n",
        "    def encode_disagreements(row):\n",
        "        return 1 if row[0] != row[1] else 0\n",
        "\n",
        "    col_dis = [\n",
        "               ('asp_sd', 'asp_ss', 'asp_dis'),\n",
        "               ('dep_sd', 'dep_ss', 'dep_dis'),\n",
        "               ('val_sd', 'val_ss', 'val_dis'),\n",
        "               #('prg_sd', 'prg_ss', 'prg_dis'),\n",
        "               #('tgd_sd', 'tgd_ss', 'tgd_dis'),\n",
        "               #('age_sd', 'age_ss', 'age_dis'),\n",
        "               #('race_sd', 'race_ss', 'race_dis'),\n",
        "               #('dbty_sd', 'dbty_ss', 'dbty_dis'),\n",
        "               ]\n",
        "\n",
        "    for col1, col2, dis_col in col_dis:\n",
        "        d[dis_col] = d[[col1, col2]].apply(encode_disagreements, axis = 1)\n",
        "\n",
        "    # export: cycle-specific\n",
        "\n",
        "    d.to_excel(f'd_cycle{cycle_num}_iaa.xlsx')\n",
        "\n",
        "    return d, kappa_results"
      ],
      "metadata": {
        "id": "jUKY5gyifXfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### llm_assist.py"
      ],
      "metadata": {
        "id": "kHwyXdNFjrrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm_assist.py\n",
        "\n",
        "import time\n",
        "import openai\n",
        "\n",
        "api_key = 'OPENAI_API_KEY'\n",
        "client = openai.OpenAI(api_key = api_key)\n",
        "\n",
        "def annotate_post_per_tag(text, prompts):\n",
        "    \"\"\"\n",
        "    Applies annotation decisions, based on multiple prompts, to a given text; provides rationale and explanation.\n",
        "    Parameters:\n",
        "    - text: The text to annotate.\n",
        "    - prompts: A list of prompts to apply to the text.\n",
        "\n",
        "    Returns:\n",
        "    - result: The combined result from all prompts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        # concatenate prompts\n",
        "\n",
        "        prompt_content = ' '.join(prompts)\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model = 'gpt-4o',\n",
        "            temperature = 0.2,\n",
        "            messages = [\n",
        "                {\n",
        "                    'role': 'system',\n",
        "                    'content': prompt_content\n",
        "                },\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': text\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # collect results\n",
        "\n",
        "        result = ' '\n",
        "        for choice in response.choices:\n",
        "            result += choice.message.content\n",
        "\n",
        "        print(f'{text}: {result}')\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f'Exception: {e}')\n",
        "        return 'error'"
      ],
      "metadata": {
        "id": "CYoYaWqaecMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_annotate_dataframe_per_tag_**"
      ],
      "metadata": {
        "id": "Yr4N0Gv8ilNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a llm_assist.py\n",
        "\n",
        "def annotate_dataframe_per_tag(df, prompts_per_tag):\n",
        "    \"\"\"\n",
        "    Applies annotate_post_per_tag for multiple tags to each row in dataframe 'd'.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The dataframe containing texts to annotate.\n",
        "    - prompts_per_tag: A dictionary with tag names as keys and a list of prompts as values.\n",
        "\n",
        "    Returns:\n",
        "    - df: The updated dataframe with annotation results.\n",
        "    \"\"\"\n",
        "    for index, row in df.iterrows():\n",
        "        for tag, prompts in prompts_per_tag.items():\n",
        "            result = annotate_post_per_tag(row['text'], prompts)\n",
        "            if result == 'error':\n",
        "                continue\n",
        "\n",
        "            # extract rationale, chain of thought (\"explanation\")\n",
        "\n",
        "            rationale, explanation = None, None\n",
        "            if f'{tag}_1' in result:\n",
        "                tag_value = 1\n",
        "                rationale = result.split(f'{tag}_rationale:')[1].split(f'strained {tag}:')[0].strip() if f'{tag}_rationale:' in result else None\n",
        "\n",
        "            # excise {tag}_explanation and subsequent text from rationale\n",
        "\n",
        "                if rationale and f'{tag}_explanation:' in rationale:\n",
        "                    rationale = rationale.split(f'{tag}_explanation:')[0].strip()\n",
        "\n",
        "                #if f'{tag}_explanation:' in rationale:\n",
        "                #    rationale = rationale.split(f'{tag}_explanation:')[0].strip()\n",
        "\n",
        "                explanation = result.split(f'{tag}_explanation:')[1].strip() if f'{tag}_explanation:' in result else None\n",
        "            else:\n",
        "                tag_value = 0\n",
        "\n",
        "            # results to df\n",
        "\n",
        "            df.at[index, f'{tag}_gpt'] = tag_value\n",
        "            df.at[index, f'{tag}_rtnl_gpt'] = rationale\n",
        "            df.at[index, f'{tag}_expl_gpt'] = explanation\n",
        "\n",
        "            # impose delay between API calls\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "l4eGb93PfqnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import"
      ],
      "metadata": {
        "id": "cArd66i6jngd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from condense import(\n",
        "                     subreddit_dataframe_condense,\n",
        "                     subreddit_parse,\n",
        "                     )\n",
        "\n",
        "from redact import(\n",
        "                   ner_redact_post_texts,\n",
        "                   )\n",
        "\n",
        "from triangulate import(\n",
        "                        calculate_kappa_by_cycle,\n",
        "                        )\n",
        "\n",
        "from llm_assist import(\n",
        "                       annotate_post_per_tag,\n",
        "                       annotate_dataframe_per_tag,\n",
        "                       )"
      ],
      "metadata": {
        "id": "Nr3wAtUU-_eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Pre-annotation\n",
        "Import and format Reddit archives.\n",
        "***"
      ],
      "metadata": {
        "id": "lZ_RbpSm7F4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in .json.gz archives\n",
        "\n",
        "wd = '/content/drive/MyDrive/Colab/bar_policy_suicidality/inputs/archives' ### Colab - gdrive\n",
        "\n",
        "#wd = 'C:/Users/sskee/OneDrive/Documents/02_tulane/01_research/03_prospectus/d_posts' ### Jupyter - local\n",
        "\n",
        "ds = []\n",
        "\n",
        "# loop over .json.gz\n",
        "\n",
        "for filename in os.listdir(wd):\n",
        "    if filename.endswith('.json.gz'):\n",
        "        filepath = os.path.join(\n",
        "                                wd,\n",
        "                                filename,\n",
        "                                )\n",
        "        with gzip.open(\n",
        "                       filepath,\n",
        "                       'rt', ### 'open for reading', 'text mode'\n",
        "                       encoding = 'utf-8',\n",
        "                       ) as i:\n",
        "            data = [json.loads(line) for line in i]\n",
        "            d_gz = pd.DataFrame(data)\n",
        "            ds.append(d_gz)\n",
        "\n",
        "# concatenate\n",
        "\n",
        "d = pd.concat(\n",
        "              ds,\n",
        "              ignore_index = True,\n",
        "              )\n",
        "\n",
        "# harmonize\n",
        "\n",
        "d = d.dropna(\n",
        "             axis = 1,\n",
        "             how = 'any',\n",
        "             )\n",
        "\n",
        "# de-duplicate\n",
        "\n",
        "d = d.drop_duplicates(\n",
        "                      subset = 'id',\n",
        "                      )\n",
        "\n",
        "# re-index\n",
        "\n",
        "d['date'] = pd.to_datetime(\n",
        "                           d.created_utc,\n",
        "                           unit = 's',\n",
        "                           )\n",
        "\n",
        "d.set_index(\n",
        "            'date',\n",
        "            drop = False,\n",
        "            inplace = True,\n",
        "            )\n",
        "\n",
        "# inspect\n",
        "\n",
        "d.shape\n",
        "d.dtypes\n",
        "d.sample(3)"
      ],
      "metadata": {
        "id": "5Sq4yvyN7ENX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# housekeeping\n",
        "\n",
        "d = subreddit_dataframe_condense(d)\n",
        "\n",
        "# restrict timeframe\n",
        "\n",
        "d = d.loc[(d['p_date'] >= '2022-01-01') & (d['p_date'] <= '2022-12-31')]\n",
        "\n",
        "# verify\n",
        "\n",
        "d.shape\n",
        "sbrt = d['sbrt'].unique()\n",
        "print(sbrt)\n",
        "d.head(1)\n",
        "d.tail(1)\n",
        "\n",
        "# plot\n",
        "\n",
        "monthly_counts = d.resample('M').sbrt.value_counts().unstack().fillna(0)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "monthly_counts.plot(\n",
        "                    kind = 'line',\n",
        "                    ax = ax,\n",
        "                    )\n",
        "\n",
        "box = ax.get_position()\n",
        "\n",
        "ax.set_position(\n",
        "                [\n",
        "                 box.x0,\n",
        "                 box.y0,\n",
        "                 box.width * 0.8,\n",
        "                 box.height,\n",
        "                 ]\n",
        "                  )\n",
        "\n",
        "ax.legend(\n",
        "          loc = 'center left',\n",
        "          bbox_to_anchor=(1, 0.5),\n",
        "          )\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ECrxnrs7EWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse by subreddit\n",
        "\n",
        "sub_d = subreddit_parse(\n",
        "                        d,\n",
        "                        'sbrt',\n",
        "                        )"
      ],
      "metadata": {
        "id": "Jc0i1mVbBNpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subset A: strain (_$\\hat{s}_{1-3}$_) proxy**"
      ],
      "metadata": {
        "id": "gYeDWb-uCAlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#d_ax = sub_d['d_Anxiety'] ### deprecated\n",
        "d_dp = sub_d['d_depression']\n",
        "#d_mh = sub_d['d_mentalhealth'] ### deprecated\n",
        "d_sw = sub_d['d_SuicideWatch']\n",
        "\n",
        "#print('r/Anxiety')\n",
        "#d_ax.shape\n",
        "print(\"\\nr/depression\")\n",
        "d_dp.shape\n",
        "#print(\"\\nr/mentalhealth\")\n",
        "#d_mh.shape\n",
        "print(\"\\nr/SuicideWatch\")\n",
        "d_sw.shape"
      ],
      "metadata": {
        "id": "DqgFTBb1B-wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subset B: explicit targeting (_$E_{1,2}$_) proxy**"
      ],
      "metadata": {
        "id": "tDfBg3owCH18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_gs = sub_d['d_TheGirlSurvivalGuide']\n",
        "d_tr = sub_d['d_trans']\n",
        "d_tx = sub_d['d_TwoXChromosomes']\n",
        "\n",
        "print(\"r/TheGirlSurvivalGuide\")\n",
        "d_gs.shape\n",
        "print(\"\\nr/Trans\")\n",
        "d_tr.shape\n",
        "print(\"\\nr/TwoXChromosomes\")\n",
        "d_tx.shape"
      ],
      "metadata": {
        "id": "0B5aY5MUB-04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Wave I: purposive\n",
        "Iterates over regex formulations, subreddit curation, Cycle 0--4 sampling.Includes Cycle 999 training data sampling.\n",
        "***"
      ],
      "metadata": {
        "id": "ou-3A98QE_-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 0 (_$n$_ = 100)"
      ],
      "metadata": {
        "id": "3jc7s7biFt8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pilot annotation cycle using r/SuicideWatch\n",
        "\n",
        "%cd ../annotation\n",
        "\n",
        "'.gend\\S*|pregnan\\S*' ### a priori/canonical\n",
        "'trans|non-?binary|dysphor\\S*|hormone|abort\\S*|dobbs|roe' ### inductively derived\n",
        "\n",
        "rg = re.compile('.gend\\S*|pregnan\\S*|trans|non-?binary|dysphor\\S*|hormone|abort\\S*|dobbs|roe', re.I)\n",
        "\n",
        "d = d.loc[d['text'].str.contains(\n",
        "                                 rg,\n",
        "                                 regex = True,\n",
        "                                 )]\n",
        "\n",
        "d['text'] = d['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "d.shape\n",
        "\n",
        "# export: 'd_cycle*.xlsx'\n",
        "\n",
        "d.to_excel('d_cycle0.xlsx')"
      ],
      "metadata": {
        "id": "edrwVvuOB-4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 1 (_$n$_ = 100)"
      ],
      "metadata": {
        "id": "HRkvN5uMGFic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# subset A: r/anxiety, r/depression, r/mentalhealth, r/SuicideWatch\n",
        "\n",
        "d_a = pd.concat([\n",
        "                 d_ax,\n",
        "                 d_dp,\n",
        "                 d_mh,\n",
        "                 d_sw,\n",
        "                 ])\n",
        "\n",
        "# subset B: r/trans\n",
        "\n",
        "d_a.reset_index(\n",
        "                drop = True,\n",
        "                inplace = True,\n",
        "                )\n",
        "\n",
        "d_b = d_tr.copy()"
      ],
      "metadata": {
        "id": "hVuD2j45B-8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subset A\n",
        "\n",
        "'.gend\\S*|pregnan\\S*' ### a priori/canonical\n",
        "'trans|non-?binary|dysphor\\S*|hormone|abort\\S*|dobbs|roe' ### inductively derived\n",
        "\n",
        "rg_a = re.compile('.gend\\S*|pregnan\\S*|trans|non-?binary|dysphor\\S*|hormone|abort\\S*|dobbs|roe', re.I)\n",
        "\n",
        "d_a = d_a.loc[d_a['text'].str.contains(\n",
        "                                       rg_a,\n",
        "                                       regex = True,\n",
        "                                       )]\n",
        "\n",
        "d_a.shape\n",
        "\n",
        "# subset B\n",
        "\n",
        "'.criminal\\S*|restrict\\S*|.law|.legal\\S*' ### a priori/canonical\n",
        "\n",
        "rg_b = re.compile('.criminal\\S*|restrict\\S*|.law|.legal\\S*', re.I)\n",
        "\n",
        "d_b = d_b.loc[d_b['text'].str.contains(\n",
        "                                       rg_b,\n",
        "                                       regex = True,\n",
        "                                       )]\n",
        "\n",
        "d_b.shape"
      ],
      "metadata": {
        "id": "zjm6p2VXHUIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = pd.concat([\n",
        "               d_a, # n = 9740\n",
        "               d_b, # n = 1505\n",
        "               ])\n",
        "\n",
        "d.shape # N = 11245"
      ],
      "metadata": {
        "id": "VmGJrupwHW8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../annotation\n",
        "\n",
        "d = d.sample(n = 100)\n",
        "\n",
        "d['text'] = d['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "d.shape\n",
        "\n",
        "# export: 'd_cycle*.xlsx'\n",
        "\n",
        "d.to_excel('d_cycle1.xlsx')"
      ],
      "metadata": {
        "id": "azCJyMcWHZ51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 2 (_$n$_ = 100)"
      ],
      "metadata": {
        "id": "8vNbiLhbHm65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# subset A: r/depression, r/SuicideWatch\n",
        "\n",
        "d_a = pd.concat([\n",
        "                 d_dp,\n",
        "                 d_sw,\n",
        "                 ])\n",
        "\n",
        "d_a.reset_index(\n",
        "                drop = True,\n",
        "                inplace = True,\n",
        "                )\n",
        "\n",
        "# subset B: r/trans\n",
        "\n",
        "d_b = d_tr.copy()"
      ],
      "metadata": {
        "id": "gdb8j4jwHmEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subset A\n",
        "\n",
        "'.gend\\S*|pregnan\\S*' ### a priori/canonical\n",
        "'trans|non-?binary|dysphor\\S*|hormone|abort\\S*|dobbs|roe' ### inductively derived\n",
        "\n",
        "rg_a = re.compile('.gend\\S*|pregnan\\S*|trans|non-?binary|dysphor\\S*|hormone|abort\\S*|dobbs|roe', re.I)\n",
        "\n",
        "d_a = d_a.loc[d_a['text'].str.contains(\n",
        "                                       rg_a,\n",
        "                                       regex = True,\n",
        "                                       )]\n",
        "\n",
        "d_a.shape\n",
        "\n",
        "# subset B\n",
        "\n",
        "'.criminal\\S*|restrict\\S*|illegal\\S*|outlaw\\S*|suicid\\S*' ### a priori/canonical\n",
        "\n",
        "rg_b = re.compile('.criminal\\S*|restrict\\S*|illegal\\S*|outlaw\\S*|suicid\\S*', re.I)\n",
        "\n",
        "d_b = d_b.loc[d_b['text'].str.contains(\n",
        "                                       rg_b,\n",
        "                                       regex = True,\n",
        "                                       )]\n",
        "\n",
        "d_b.shape"
      ],
      "metadata": {
        "id": "JxlZY5mxHmJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = pd.concat([\n",
        "               d_a, # n = 5602\n",
        "               d_b, # n = 729\n",
        "               ])\n",
        "\n",
        "d.shape # N = 6331"
      ],
      "metadata": {
        "id": "yZ0r_bG_HmPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../annotation\n",
        "\n",
        "d = d.sample(n = 100)\n",
        "\n",
        "d['text'] = d['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "\n",
        "# export: 'd_cycle*.xlsx'\n",
        "\n",
        "d.to_excel('d_cycle2.xlsx')"
      ],
      "metadata": {
        "id": "FGklazHoIkzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 3 (_$n$_ = 150)"
      ],
      "metadata": {
        "id": "Z29zZpHtJ3r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# subset A: r/depression, r/SuicideWatch\n",
        "\n",
        "d_a = pd.concat([\n",
        "                 d_dp,\n",
        "                 d_sw,\n",
        "                 ])\n",
        "\n",
        "d_a.reset_index(\n",
        "                drop = True,\n",
        "                inplace = True,\n",
        "                )\n",
        "\n",
        "# subset B: r/TheGirlsSurvivalGuide, r/trans, r/TwoXChromosomes\n",
        "\n",
        "d_b = pd.concat([\n",
        "                 d_gs,\n",
        "                 d_tr,\n",
        "                 d_tx,\n",
        "                 ])\n",
        "\n",
        "d_b.reset_index(\n",
        "                drop = True,\n",
        "                inplace = True,\n",
        "                )"
      ],
      "metadata": {
        "id": "Ujq9HveiJz6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subset A\n",
        "\n",
        "'.gend\\S*|pregnan\\S*' ### a priori/canonical\n",
        "'trans|non-?binary|dysphor\\S*|hormone|abort\\S*|dobbs|roe' ### inductively derived\n",
        "\n",
        "rg_a = re.compile('.gend\\S*|pregnan\\S*|trans|non-?binary|dysphor\\S*|hormone|abort\\S*|dobbs|roe', re.I)\n",
        "\n",
        "d_a = d_a.loc[d_a['text'].str.contains(\n",
        "                                       rg_a,\n",
        "                                       regex = True,\n",
        "                                       )]\n",
        "\n",
        "d_a.shape\n",
        "\n",
        "# subset B\n",
        "\n",
        "'.criminal\\S*|restrict\\S*|illegal\\S*|outlaw\\S*|suicid\\S*' ### a priori/canonical\n",
        "\n",
        "rg_b = re.compile('.criminal\\S*|restrict\\S*|illegal\\S*|outlaw\\S*|suicid\\S*', re.I)\n",
        "\n",
        "d_b = d_b.loc[d_b['text'].str.contains(\n",
        "                                       rg_b,\n",
        "                                       regex = True,\n",
        "                                       )]\n",
        "\n",
        "d_b.shape"
      ],
      "metadata": {
        "id": "TdTgkgBjJ0AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = pd.concat([\n",
        "               d_a, # n = 5602\n",
        "               d_b, # n = 1971\n",
        "               ])\n",
        "\n",
        "d.shape # N = 7573"
      ],
      "metadata": {
        "id": "9dMG1O44J0GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../annotation\n",
        "\n",
        "d = d.sample(n = 150)\n",
        "\n",
        "d['text'] = d['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "\n",
        "# export: 'd_cycle*.xlsx'\n",
        "\n",
        "d.to_excel('d_cycle3.xlsx')"
      ],
      "metadata": {
        "id": "S5mniO7lJ0Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 999 (_$n$_ = 1,000): initial training data (purposive)"
      ],
      "metadata": {
        "id": "Lp3UMUKzLGIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_999 = d.sample(n = 1000)\n",
        "\n",
        "d_999['text'] = d_999['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "\n",
        "# export: 'd_cycle*.xlsx'\n",
        "\n",
        "d_999.to_excel('d_cycle999.xlsx')"
      ],
      "metadata": {
        "id": "5DKb_Rj6LFaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 4 (_$n$_ = 150)"
      ],
      "metadata": {
        "id": "myuWu6YoLlpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# supplementing _prg_: candidate subreddits\n",
        "\n",
        "%cd /content/gdrive/My Drive/Colab/bar_policy_suicidality/inputs/archives\n",
        "\n",
        "d_aw = pd.read_json(\n",
        "                    #'r_thegirlsurvivalguide_posts.jsonl', # d_gs.xlsx\n",
        "                    #'r_confession_posts.jsonl', # d_co.xlsx\n",
        "                    'r_askwomenadvice_posts.jsonl', # d_aw.xlsx\n",
        "                    #'r_traumatoolbox_posts.jsonl', # d_tb.xlsx\n",
        "                    #'r_birthcontrol_posts.jsonl', # d_bc.xlsx\n",
        "                    #'r_WomensHealth_posts.jsonl', # d_wh.xlsx\n",
        "                    lines = True,\n",
        "                    )\n",
        "\n",
        "    ### SJS 7/15: decision: adding r/askwomenadvice to Cycle 4\n",
        "\n",
        "d_aw.shape\n",
        "d_aw.head(3)\n",
        "d_aw.tail(3)"
      ],
      "metadata": {
        "id": "9SwB8CLVLkgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# supplementary housekeeping: r/askwomenadvice\n",
        "\n",
        "# harmonize\n",
        "\n",
        "d_aw = d_aw.dropna(\n",
        "                   axis = 1,\n",
        "                   how = 'any',\n",
        "                   )\n",
        "\n",
        "# de-duplicate\n",
        "\n",
        "d_aw = d_aw.drop_duplicates(\n",
        "                            subset = 'id',\n",
        "                            )\n",
        "\n",
        "# re-index\n",
        "\n",
        "d_aw['date'] = pd.to_datetime(\n",
        "                              d_aw.created_utc,\n",
        "                              unit = 's',\n",
        "                              )\n",
        "\n",
        "d_aw.set_index(\n",
        "               'date',\n",
        "               drop = False,\n",
        "               inplace = True,\n",
        "               )\n",
        "\n",
        "# housekeeping\n",
        "\n",
        "d_aw = subreddit_dataframe_condense(d_aw)\n",
        "\n",
        "# anonymize\n",
        "\n",
        "d_aw['text'] = d_aw['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "\n",
        "d_aw.shape\n",
        "d_aw.head(3)\n",
        "d_aw.tail(3)\n"
      ],
      "metadata": {
        "id": "HXwznTSDLkkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subset A: r/depression, r/SuicideWatch\n",
        "\n",
        "d_a = pd.concat([\n",
        "                 d_dp,\n",
        "                 d_sw,\n",
        "                 ])\n",
        "\n",
        "d_a.reset_index(\n",
        "                drop = True,\n",
        "                inplace = True,\n",
        "                )\n",
        "\n",
        "# subset B: r/trans, r/TwoXChromosomes\n",
        "\n",
        "d_b = pd.concat([\n",
        "                 d_tr,\n",
        "                 d_tx,\n",
        "                 ])\n",
        "\n",
        "d_b.reset_index(\n",
        "                drop = True,\n",
        "                inplace = True,\n",
        "                )\n",
        "\n",
        "# subset C: r/askwomenadvice, r/TheGirlsSurvivalGuide\n",
        "\n",
        "d_c = pd.concat([\n",
        "                 d_aw,\n",
        "                 d_gs,\n",
        "                 ])\n",
        "\n",
        "d_c.reset_index(\n",
        "                drop = True,\n",
        "                inplace = True,\n",
        "                )"
      ],
      "metadata": {
        "id": "Q5lghwtBLkqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = [\n",
        "      d_a,\n",
        "      d_b,\n",
        "      d_c,\n",
        "      ]\n",
        "\n",
        "for d in ds:\n",
        "    d = shuffle(d)"
      ],
      "metadata": {
        "id": "kR5n03VIN5rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subset A\n",
        "\n",
        "'.gend\\S*|pregnan\\S*' ### a priori/canonical\n",
        "'trans|non-?binary|dysphor\\S*|hormone|abort\\S*|dobbs|roe' ### inductively derived\n",
        "\n",
        "rg_a = re.compile('.gend\\S*|pregnan\\S*|trans|non-?binary|dysphor\\S*|hormone|abort\\S*|dobbs|roe', re.I)\n",
        "\n",
        "d_a = d_a.loc[d_a['text'].str.contains(\n",
        "                                       rg_a,\n",
        "                                       regex = True,\n",
        "                                       )]\n",
        "\n",
        "d_a.shape\n",
        "\n",
        "# subset B\n",
        "\n",
        "'.criminal\\S*|restrict\\S*|illegal\\S*|outlaw\\S*|suicid\\S*|dobbs|roe|pregnan\\S*' ### a priori/canonical\n",
        "\n",
        "rg_b = re.compile('.criminal\\S*|restrict\\S*|illegal\\S*|outlaw\\S*|suicid\\S*|dobbs|roe|pregnan\\S*', re.I)\n",
        "\n",
        "d_b = d_b.loc[d_b['text'].str.contains(\n",
        "                                       rg_b,\n",
        "                                       regex = True,\n",
        "                                       )]\n",
        "\n",
        "d_b.shape\n",
        "\n",
        "# subset C\n",
        "\n",
        "d_c = d_c.loc[d_c['text'].str.contains(\n",
        "                                       rg_b,\n",
        "                                       regex = True,\n",
        "                                       )]\n",
        "\n",
        "d_c.shape"
      ],
      "metadata": {
        "id": "-_ZGbIKhN5yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = pd.concat([\n",
        "               d_a, # n = 5602\n",
        "               d_b, # n = 5478\n",
        "               ])\n",
        "\n",
        "d.shape # N = 11080"
      ],
      "metadata": {
        "id": "28Kx7mSWOkjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = d.sample(n = 130)\n",
        "\n",
        "d['text'] = d['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))"
      ],
      "metadata": {
        "id": "PTiX74kmOkpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# oversampling for _prg_ - Cycle 4\n",
        "\n",
        "d_suppl = d_c.sample(n = 20)\n",
        "\n",
        "d_suppl['text'] = d_suppl['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))"
      ],
      "metadata": {
        "id": "0ICiuS6_OucL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = pd.concat([\n",
        "               d, # n = 130\n",
        "               d_suppl, # n = 20\n",
        "               ])\n",
        "\n",
        "d = shuffle(d)\n",
        "\n",
        "# export: 'd_cycle*.xlsx'\n",
        "\n",
        "d.to_excel('d_cycle4.xlsx')"
      ],
      "metadata": {
        "id": "Czv_BKWxPEdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 999 (_$n$_ = 200): supplementary training data (_$E_{1}$_: 'prg')"
      ],
      "metadata": {
        "id": "ztDyCILbP10O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../annotation\n",
        "\n",
        "# oversampling for _prg_ - Cycle 999\n",
        "\n",
        "d_suppl = d_c.sample(n = 200)\n",
        "\n",
        "d_suppl['text'] = d_suppl['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "\n",
        "# export: 'd_cycle*.xlsx'\n",
        "\n",
        "d_suppl.to_excel('d_cycle999_suppl.xlsx')"
      ],
      "metadata": {
        "id": "VgIYyCzZP0x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 999 (_$n$_ = 100): supplementary training data (_$I_{2}$_: 'dbty')"
      ],
      "metadata": {
        "id": "1dsFrT1ZYOIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# supplementing _dbty_: 2022 r/Disability posts\n",
        "\n",
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/inputs/archives\n",
        "\n",
        "d_db = pd.read_json(\n",
        "                    'r_Disability_posts.jsonl',\n",
        "                    lines = True,\n",
        "                    )\n",
        "\n",
        "d_db.info()\n",
        "d_db.head(3)\n",
        "d_db.tail(3)"
      ],
      "metadata": {
        "id": "ijQuYTbNQmaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# harmonize\n",
        "\n",
        "d_db = d_db.dropna(\n",
        "                   axis = 1,\n",
        "                   how = 'any',\n",
        "                   )\n",
        "\n",
        "# de-duplicate\n",
        "\n",
        "d_db = d_db.drop_duplicates(\n",
        "                            subset = 'id',\n",
        "                            )\n",
        "\n",
        "# re-index\n",
        "\n",
        "d_db['date'] = pd.to_datetime(\n",
        "                              d_db.created_utc,\n",
        "                              unit = 's',\n",
        "                              )\n",
        "\n",
        "d_db.set_index(\n",
        "               'date',\n",
        "               drop = False,\n",
        "               inplace = True,\n",
        "               )\n",
        "\n",
        "# housekeeping\n",
        "\n",
        "d_db = subreddit_dataframe_condense(d_db)\n",
        "\n",
        "d_db.shape\n",
        "d_db.head(3)"
      ],
      "metadata": {
        "id": "LDmTjfxBVfuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subset B (explicit target proxy) for re\n",
        "\n",
        "'.criminal\\S*|restrict\\S*|illegal\\S*|outlaw\\S*|suicid\\S*|dobbs|roe|pregnan\\S*' ### a priori/canonical\n",
        "\n",
        "rg_b = re.compile('.criminal\\S*|restrict\\S*|illegal\\S*|outlaw\\S*|suicid\\S*|dobbs|roe|pregnan\\S*', re.I)\n",
        "\n",
        "d_db = d_db.loc[d_db['text'].str.contains(\n",
        "                                         rg_b,\n",
        "                                         regex = True,\n",
        "                                         )]\n",
        "\n",
        "d_db.shape # N = 171"
      ],
      "metadata": {
        "id": "13ZyPvzwkXft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../annotation\n",
        "\n",
        "d_db_suppl = d_db.sample(n = 100)\n",
        "\n",
        "d_db_suppl['text'] = d_db_suppl['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "\n",
        "d_db_suppl = shuffle(d_db_suppl)\n",
        "\n",
        "# export\n",
        "\n",
        "d_db_suppl.to_excel('d_db_suppl.xlsx')"
      ],
      "metadata": {
        "id": "jfM1Jnq_kw5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 999 (_$n$_ = 100): supplementary training data (_$I_{3}$_: 'race')"
      ],
      "metadata": {
        "id": "PSda-s9XYcc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# supplementing _race_: 2022 r/blackpeople posts\n",
        "\n",
        "%cd ../archives\n",
        "\n",
        "d_bp = pd.read_json(\n",
        "                    'r_blackpeople_posts.jsonl',\n",
        "                    lines = True,\n",
        "                    )\n",
        "\n",
        "# harmonize\n",
        "\n",
        "d_bp = d_bp.dropna(\n",
        "                   axis = 1,\n",
        "                   how = 'any',\n",
        "                   )\n",
        "\n",
        "# de-duplicate\n",
        "\n",
        "d_bp = d_bp.drop_duplicates(\n",
        "                            subset = 'id',\n",
        "                            )\n",
        "\n",
        "# re-index\n",
        "\n",
        "d_bp['date'] = pd.to_datetime(\n",
        "                              d_bp.created_utc,\n",
        "                              unit = 's',\n",
        "                              )\n",
        "\n",
        "d_bp.set_index(\n",
        "               'date',\n",
        "               drop = False,\n",
        "               inplace = True,\n",
        "               )\n",
        "\n",
        "# housekeeping\n",
        "\n",
        "d_bp = subreddit_dataframe_condense(d_bp)\n",
        "\n",
        "d_bp.info()\n",
        "d_bp.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2_svo7gTVfq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop empty cells\n",
        "\n",
        "d_bp = d_bp[d_bp['text'].str.strip() != '']\n",
        "d_bp.shape # N = 674"
      ],
      "metadata": {
        "id": "IwM06bjgn4c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random sample - too few (n = 17) regex matches\n",
        "\n",
        "%cd ../annotation\n",
        "\n",
        "d_bp_suppl = d_bp.sample(n = 100)\n",
        "\n",
        "d_bp_suppl['text'] = d_bp_suppl['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "\n",
        "d_bp_suppl = shuffle(d_bp_suppl)\n",
        "\n",
        "# export\n",
        "\n",
        "d_bp_suppl.to_excel('d_bp_suppl.xlsx')"
      ],
      "metadata": {
        "id": "StmBdIdYVfkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Wave II: random\n",
        "Performs random sampling, 1:1 purposive:random ratio in initial training data.\n",
        "***"
      ],
      "metadata": {
        "id": "5ssC99HPZH84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    ### SJS 8/15: same _prg_ oversampling strategy: 1000 random from subsets A and B, plus 200 from r/TheGirlsSurvivalGuide and r/AskWomenAdvice\n",
        "\n",
        "# strain proxies\n",
        "\n",
        "d_dp.shape\n",
        "d_dp.head(3)\n",
        "d_sw.shape\n",
        "d_sw.head(3)\n",
        "\n",
        "# explicit targeting proxies\n",
        "\n",
        "d_aw.shape\n",
        "d_aw.head(3)\n",
        "d_gs.shape\n",
        "d_gs.head(3)\n",
        "d_tr.shape\n",
        "d_tr.head(3)\n",
        "d_tx.shape\n",
        "d_tx.head(3)"
      ],
      "metadata": {
        "id": "LhlVfqrwZHNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_a = pd.concat([\n",
        "                 d_dp, ### r/depression\n",
        "                 d_sw, ### r/SuicideWatch\n",
        "                 ])\n",
        "\n",
        "d_a = shuffle(d_a)\n",
        "\n",
        "d_a.reset_index(\n",
        "                drop = True,\n",
        "                inplace = True,\n",
        "                )\n",
        "\n",
        "d_b = pd.concat([\n",
        "                 d_tr, ### r/trans\n",
        "                 d_tx, ### r/TwoXChromosomes\n",
        "                 ])\n",
        "\n",
        "d_b = shuffle(d_b)\n",
        "\n",
        "d_b.reset_index(\n",
        "                drop = True,\n",
        "                inplace = True,\n",
        "                )\n",
        "\n",
        "d_c = pd.concat([\n",
        "                 d_aw, ### r/AskWomenAdvice,\n",
        "                 d_gs, ### r/TheGirlsSurvivalGuide\n",
        "                 ])\n",
        "\n",
        "d_c = shuffle(d_c)\n",
        "\n",
        "d_c.reset_index(\n",
        "                drop = True,\n",
        "                inplace = True,\n",
        "                )\n",
        "\n",
        "d_a.shape\n",
        "d_b.shape\n",
        "d_c.shape"
      ],
      "metadata": {
        "id": "67MQO90SaYQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = pd.concat([\n",
        "               d_a, # n = 178030\n",
        "               d_b, # n = 110213\n",
        "               ])\n",
        "\n",
        "d.shape # N = 288243"
      ],
      "metadata": {
        "id": "qtTBu32Tae0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = d.sample(n = 1000)\n",
        "\n",
        "d['text'] = d['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "\n",
        "d.shape # N = 1000"
      ],
      "metadata": {
        "id": "4VlKjU3OanU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_suppl = d_c.sample(n = 200)\n",
        "\n",
        "d_suppl['text'] = d_suppl['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))\n",
        "\n",
        "d_suppl.shape # N = 200"
      ],
      "metadata": {
        "id": "wETEElSYasCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = pd.concat([\n",
        "               d, # n = 1000\n",
        "               d_suppl, # n = 200\n",
        "               ])\n",
        "\n",
        "d.shape # N = 1200"
      ],
      "metadata": {
        "id": "CghARR-GauSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cycle 999 (_$n$_ = 1,200): initial training data (random)"
      ],
      "metadata": {
        "id": "XHvLPAvWbJKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../annotation\n",
        "\n",
        "d.to_excel('d_cycle999_rnd.xlsx')"
      ],
      "metadata": {
        "id": "alNNbpcAaynH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Post-annotation\n",
        "Computes Cohen's $\\kappa$ for SD-SS independently annotated dataframes per cycle, flags disagreements.\n",
        "***"
      ],
      "metadata": {
        "id": "ZzSVBMiubjEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cycle 4 - pre-LLM triangulation\n",
        "\n",
        "%cd ../inputs/annotation\n",
        "\n",
        "d, kappa_results = calculate_kappa_by_cycle(4)"
      ],
      "metadata": {
        "id": "1bJ4ar2Lbh86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cycle 4 - post-LLM triangulation\n",
        "\n",
        "%cd ../inputs/annotation\n",
        "\n",
        "# kappa Fx\n",
        "\n",
        "def calculate_kappa(d, col_sd, col_ss):\n",
        "    return cohen_kappa_score(d[col_sd], d[col_ss])\n",
        "\n",
        "# define SD-SS col pair list\n",
        "\n",
        "col_pairs = [\n",
        "             ('asp_sd', 'asp_ss'),\n",
        "             ('dep_sd', 'dep_ss'),\n",
        "             ('val_sd', 'val_ss'),\n",
        "             #('prg_sd', 'prg_ss'),\n",
        "             #('tgd_sd', 'tgd_ss'),\n",
        "             #('age_sd', 'age_ss'),\n",
        "             #('race_sd', 'race_ss'),\n",
        "             #('dbty_sd', 'dbty_ss'),\n",
        "             ]\n",
        "\n",
        "# initialize dict\n",
        "\n",
        "kappa_results = {}\n",
        "\n",
        "# kappa loop\n",
        "\n",
        "for col_sd, col_ss in col_pairs:\n",
        "    kappa = calculate_kappa(d, col_sd, col_ss)\n",
        "    kappa_results[f'{col_sd} and {col_ss}'] = kappa\n",
        "\n",
        "for pair, kappa in kappa_results.items():\n",
        "    print(f\"Cohen's Kappa for {pair}: {kappa:.2f}\")"
      ],
      "metadata": {
        "id": "d2FFxAl3v57W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Human-LLM triangulation\n",
        "Defines prompts for, and performs, GPT-4o-assisted human-in-the-loop annotation using chain-of-thought reasoning.\n",
        "***"
      ],
      "metadata": {
        "id": "eMblXk-8_Bd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$\\hat{s}_{1}$_: 'asp' prompt"
      ],
      "metadata": {
        "id": "Q3cxhxEm_NjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "role = '''\n",
        "You are tasked with applying qualitative codes to social media posts to categorize whether each post contains an expression of _aspiration strain_.\n",
        "'''\n",
        "\n",
        "definition = '''\n",
        "Definition of aspiration strain: 'any description of ambition, futurity, idealized or speculative lifecourse trajectories, or personal,\n",
        "professional, familial goals driving psychological strain and/or self-destructive cognitions.' Descriptions can be explicit or implicit.\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "Below I instruct on how to apply the codes.\n",
        "\n",
        "Respond with 'asp_1' if the post contains an expression of aspiration strain, and '0' if it does not.\n",
        "\n",
        "You must choose a 'asp_1' or a '0' response.\n",
        "\n",
        "If your response is 'asp_1,' then begin a new paragraph with 'asp_rationale' and excerpt the sentences or phrases that are the _most expressive of\n",
        "aspiration strain_. You are allowed to choose multiple sentences or phrases, divided by an '<SPL>' token.\n",
        "\n",
        "Then, if you have selected a 'asp_1,' begin a new paragraph with 'asp_explanation:' and provide a two sentence explanation for your response.\n",
        "'''\n",
        "\n",
        "#strain = '''\n",
        "#If your response is 'asp_1,' then begin a new paragraph with 'strained aspirations:' and concisely name the strained aspiration that is driving the distressful\n",
        "#cognitions.\n",
        "#'''\n",
        "\n",
        "clarification = '''\n",
        "Here are additional clarifying points based in human expertise:\n",
        "-\tRegret over, or wishing to redo, a past decision does _not_ warrant a 'asp_1' response\n",
        "-\tAspiration for physical impossibility (time travel, age reversion), does not warrant a 'asp_1' response\n",
        "-\tSparse decontextualized expressions of loneliness (e.g. 'I am lonely') do not warrant a 'asp_1' response; recognition of need or yearning for\n",
        "friendship, community, and/or intimacy must be explicit to warrant a 'asp_1' response\n",
        "-\tSparse decontextualized desire for a different assigned sex at birth does not warrant a 'asp_1' response; desire for gender transition or\n",
        "gender-expansive expression must be explicit.\n",
        "- Perceived inability to die by suicide does not warrant a 'asp_1' response\n",
        "'''\n",
        "\n",
        "asp_prompt = f'{role}{definition}{instruction}{clarification}'\n",
        "#print(asp_prompt)"
      ],
      "metadata": {
        "id": "VOBRy07D_A2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$\\hat{s}_{2}$_:  'dep' prompt"
      ],
      "metadata": {
        "id": "fWH__pCo_UgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "role = '''\n",
        "You are tasked with applying qualitative codes to social media posts to categorize whether each post contains an expression of _deprivation strain_.\n",
        "'''\n",
        "\n",
        "definition = '''\n",
        "Definition of deprivation strain: 'any description of present or past inequities or scarcities in financial capital, housing security, material circumstance,\n",
        "dignity, or decisional autonomy, driving intra-psychic strain and/or self-destructive cognitions.' Descriptions can be explicit or implicit.\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "Below I instruct on how to apply the codes.\n",
        "\n",
        "Respond with 'dep_1' if the post contains an expression of deprivation strain, and '0' if it does not.\n",
        "\n",
        "You must choose a 'dep_1' or a '0' response.\n",
        "\n",
        "If your response is 'dep_1,' then begin a new paragraph with 'dep_rationale' and excerpt the sentences or phrases that are the _most expressive of\n",
        "deprivation strain_. You are allowed to choose multiple sentences or phrases, divided by an '<SPL>' token.\n",
        "\n",
        "Then, if you have selected a 'dep_1,' begin a new paragraph with 'dep_explanation:' and provide a two sentence explanation for your response.\n",
        "'''\n",
        "\n",
        "#strain = '''\n",
        "#If your response is 'dep_1,' then begin a new paragraph with 'deprivation:' and concisely name the deprivation that is driving the distressful\n",
        "#cognitions.\n",
        "#'''\n",
        "\n",
        "clarification = '''\n",
        "Here are additional clarifying points based in human expertise:\n",
        "- Apprehensions over possible _future_ deprivation does not warrant a 'dep_1' response\n",
        "- Scarcity or inaccessibility of formal systems of care (hospitals, clinics) does warrant a 'dep_1' response\n",
        "'''\n",
        "\n",
        "dep_prompt = f'{role}{definition}{instruction}{clarification}'\n",
        "#print(dep_prompt)"
      ],
      "metadata": {
        "id": "OVyWPN-u_RFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$\\hat{s}_{3}$_:  'val' prompt"
      ],
      "metadata": {
        "id": "KlfikA_F_Zwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "role = '''\n",
        "You are tasked with applying qualitative codes to social media posts to categorize whether each post contains an expression of _value strain_.\n",
        "'''\n",
        "\n",
        "definition = '''\n",
        "Definition of value strain: 'any description of conflicting ideologies, norms, mores, morals, ethics, principles, or ontologies, particularly\n",
        "along traditionalist-authoritarian versus progressive-liberatory axes, driving intra-psychic strain and/or self-destructive cognitions.' Descriptions\n",
        "can be explicit or implicit.\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "Below I instruct on how to apply the codes.\n",
        "\n",
        "Respond with 'val_1' if the post contains an expression of value strain, and '0' if it does not.\n",
        "\n",
        "You must choose a 'val_1' or a '0' response.\n",
        "\n",
        "If your response is 'val_1,' then begin a new paragraph with 'val_rationale:' and excerpt the sentences or phrases that are the _most expressive of\n",
        "value strain_. You are allowed to choose multiple sentences or phrases, divided by an '<SPL>' token.\n",
        "\n",
        "Then, if you have selected a 'val_1,' begin a new paragraph with 'val_explanation:' and provide a two sentence explanation for your response.\n",
        "'''\n",
        "\n",
        "#strain = '''\n",
        "#If your response is 'val_1,' then begin a new paragraph with 'strained values:' and concisely name the strained value that is driving the distressful\n",
        "#cognitions.\n",
        "#'''\n",
        "\n",
        "clarification = '''\n",
        "Here are additional clarifying points based in human expertise:\n",
        "-\ta 'val_1' response can refer to interpersonal ideological discord and/or internalized ideologically determined self-denigration\n",
        "-\tAnticipated value strain does warrant a 'val_1' response\n",
        "-\tFamilial conflict alone is insufficient to warrant a 'val_1' response\n",
        "-\tPerceived failure to uphold self-imposed expectations is insufficient to warrant a 'val_1' response\n",
        "-\tStress or tension over legality of decisions, necessities does warrant a 'val_1' response\n",
        "-\tideological or normative aspect must be explicit\n",
        "'''\n",
        "\n",
        "val_prompt = f'{role}{definition}{instruction}{clarification}'\n",
        "#print(val_prompt)"
      ],
      "metadata": {
        "id": "tu8zmVlO_RJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$E_{1}$_:  'prg' prompt"
      ],
      "metadata": {
        "id": "Ufetx4Jj_iwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "role = '''\n",
        "You are tasked with applying qualitative codes to social media posts to categorize whether each post indicates that the author is a person who can get pregnant.\n",
        "'''\n",
        "\n",
        "definition = '''\n",
        "Definition of person who can get pregnant: 'disclosure of pregnancy capability, past pregnancy, specific anatomy (e.g. menstruation, possession\n",
        "of uterus, etc.), and associated socio-medical needs.' Descriptions must be explicit.\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "Below I instruct on how to apply the codes.\n",
        "\n",
        "Respond with 'prg_1' if the post post indicates that the author is a person who can get pregnant, and '0' if it does not.\n",
        "\n",
        "You must choose a 'prg_1' or a '0' response.\n",
        "\n",
        "If your response is 'prg_1,' then begin a new paragraph with 'prg_rationale:' and excerpt the sentences or phrases that are the _most indicative_ of\n",
        "pregnancy capability. You are allowed to choose multiple sentences or phrases, divided by an '<SPL>' token.\n",
        "\n",
        "Then, if you have selected a 'prg_1,' begin a new paragraph with 'prg_explanation:' and provide a two sentence explanation for your response.\n",
        "'''\n",
        "\n",
        "clarification = '''\n",
        "Here is an additional clarifying point based in human expertise:\n",
        "- Descriptions of pregnancy capability must refer, in first person, to the post author - not another person.\n",
        "- It is _not necessary_ for the post author to identify as a woman.\n",
        "'''\n",
        "\n",
        "prg_prompt = f'{role}{definition}{instruction}{clarification}'\n",
        "#print(prg_prompt)"
      ],
      "metadata": {
        "id": "6S70KKkv_RM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$E_{2}$_:  'tgd' prompt"
      ],
      "metadata": {
        "id": "Erj7Z1x__ocW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "role = '''\n",
        "You are tasked with applying qualitative codes to social media posts to categorize whether each post indicates that the author is transgender.\n",
        "'''\n",
        "\n",
        "definition = '''\n",
        "Definition of person who is transgender: 'disclosure of identity as transgender, transexual, non-binary, gender-diverse, questioning their\n",
        "gender, living or identifying as a gender that differes from their sex assigned at birth.' Descriptions must be explicit.\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "Below I instruct on how to apply the codes.\n",
        "\n",
        "Respond with 'tgd_1' if the post post indicates that the author is a person who can get pregnant, and '0' if it does not.\n",
        "\n",
        "You must choose a 'tgd_1' or a '0' response.\n",
        "\n",
        "If your response is 'tgd_1,' then begin a new paragraph with 'tgd_rationale:' and excerpt the sentences or phrases that are the _most indicative_ of\n",
        "transgender identity. You are allowed to choose multiple sentences or phrases, divided by an '<SPL>' token.\n",
        "\n",
        "Then, if you have selected a 'tgd_1,' begin a new paragraph with 'tgd_explanation:' and provide a two sentence explanation for your response.\n",
        "'''\n",
        "\n",
        "clarification = '''\n",
        "Here is an additional clarifying point based in human expertise:\n",
        "- Descriptions of transgender identity must refer, in first person, to the post author - not another person.\n",
        "- Post authors expressing envy of another gender's privilege does not warrant a 'tgd_1' response.\n",
        "'''\n",
        "\n",
        "tgd_prompt = f'{role}{definition}{instruction}{clarification}'\n",
        "#print(tgd_prompt)"
      ],
      "metadata": {
        "id": "wXQOaurV_RQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$I_{1}$_:  'age' prompt"
      ],
      "metadata": {
        "id": "ogQuoKV8_vyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "role = '''\n",
        "You are tasked with applying qualitative codes to social media posts to categorize whether each post indicates that the author is younger than 30 years old.\n",
        "'''\n",
        "\n",
        "definition = '''\n",
        "Definition of person who is younger than 30 years old: 'disclosure of age lower than 30 years, including via self-identification as an adolescent, teenager, or\n",
        "high-schooler' Descriptions must be explicit.\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "Below I instruct on how to apply the codes.\n",
        "\n",
        "Respond with 'age_1' if the post post indicates that the author is a person who can get pregnant, and '0' if it does not.\n",
        "\n",
        "You must choose an 'age_1' or a '0' response.\n",
        "\n",
        "If your response is 'age_1,' then begin a new paragraph with 'age_rationale:' and excerpt the sentences or phrases that are the _most indicative_ of\n",
        "transgender identity. You are allowed to choose multiple sentences or phrases, divided by an '<SPL>' token.\n",
        "\n",
        "Then, if you have selected an 'age_1,' begin a new paragraph with 'age_explanation:' and provide a two sentence explanation for your response.\n",
        "'''\n",
        "\n",
        "clarification = '''\n",
        "Here is an additional clarifying point based in human expertise:\n",
        "- Descriptions of age younger than 30 must refer, in first person, to the post author - not another person.\n",
        "- Post authors introducing themselves with their age and gender (for example 'Hi, 29F here') does warrant a 'age_1' response if the age is lower than 30.\n",
        "'''\n",
        "\n",
        "age_prompt = f'{role}{definition}{instruction}{clarification}'\n",
        "#print(age_prompt)"
      ],
      "metadata": {
        "id": "C4JZWufo_RT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$I_{2}$_:  'dbty' prompt"
      ],
      "metadata": {
        "id": "8phTrVr4_z8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "role = '''\n",
        "You are tasked with applying qualitative codes to social media posts to categorize whether each post indicates that the author is disabled.\n",
        "'''\n",
        "\n",
        "definition = '''\n",
        "Definition of person who is disabled: 'disclosure of a physical disability or debilitating chronic disease.' Descriptions must be explicit.\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "Below I instruct on how to apply the codes.\n",
        "\n",
        "Respond with 'dbty_1' if the post post indicates that the author is disabled, and '0' if it does not.\n",
        "\n",
        "You must choose a 'dbty_1' or a '0' response.\n",
        "\n",
        "If your response is 'dbty_1,' then begin a new paragraph with 'dbty_rationale:' and excerpt the sentences or phrases that are the _most indicative_ of\n",
        "transgender identity. You are allowed to choose multiple sentences or phrases, divided by an '<SPL>' token.\n",
        "\n",
        "Then, if you have selected a 'dbty_1,' begin a new paragraph with 'dbty_explanation:' and provide a two sentence explanation for your response.\n",
        "'''\n",
        "\n",
        "clarification = '''\n",
        "Here is an additional clarifying point based in human expertise:\n",
        "- Descriptions of disability must refer, in first person, to the post author - not another person.\n",
        "- Autism is not a disability.\n",
        "- Disability can include intellecutal or developmental disability, including cognitive decline, if described explicitly.\n",
        "'''\n",
        "\n",
        "dbty_prompt = f'{role}{definition}{instruction}{clarification}'\n",
        "#print(age_prompt)"
      ],
      "metadata": {
        "id": "8jQVY1BV_RXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$I_{3}$_:  'race' prompt"
      ],
      "metadata": {
        "id": "mpUVxokX_7XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "role = '''\n",
        "You are tasked with applying qualitative codes to social media posts to categorize whether each post indicates that the author discloses any non-white race or ethnicity.\n",
        "'''\n",
        "\n",
        "definition = '''\n",
        "Definition of person who of non-white race or ethnicity: 'disclosure of an identity as a Black, Latina, Latino, Latinx, or Latine person, or a person targeted\n",
        "by racism' Descriptions must be explicit.\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "Below I instruct on how to apply the codes.\n",
        "\n",
        "Respond with 'race_1' if the post post indicates that the author is a person who can get pregnant, and '0' if it does not.\n",
        "\n",
        "You must choose a 'race_1' or a '0' response.\n",
        "\n",
        "If your response is 'race_1,' then begin a new paragraph with 'race_rationale:' and excerpt the sentences or phrases that are the _most indicative_ of\n",
        "transgender identity. You are allowed to choose multiple sentences or phrases, divided by an '<SPL>' token.\n",
        "\n",
        "Then, if you have selected a 'race_1,' begin a new paragraph with 'race_explanation:' and provide a two sentence explanation for your response.\n",
        "'''\n",
        "\n",
        "clarification = '''\n",
        "Here is an additional clarifying point based in human expertise:\n",
        "- Descriptions of non-white race or ethnicity must refer, in first person, to the post author - not another person.\n",
        "'''\n",
        "\n",
        "race_prompt = f'{role}{definition}{instruction}{clarification}'\n",
        "#print(race_prompt)"
      ],
      "metadata": {
        "id": "ZHP-aPU9_5X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wave I (purposive): LLM co-annotation"
      ],
      "metadata": {
        "id": "9DzzeTDi1epR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iWyoISVI1NRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlVgUJ9o1NPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wave II (random): LLM co-annotation"
      ],
      "metadata": {
        "id": "iVkcYuLi1LmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "HO8OmlOL1loI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../inputs/annotation\n",
        "\n",
        "d_cycle999_rnd_ss_single = pd.read_excel(\n",
        "                                         'd_cycle999_rnd_ss_single.xlsx',\n",
        "                                         index_col = [0],\n",
        "                                         )\n",
        "\n",
        "d_cycle999_rnd_ss_single.reset_index(\n",
        "                                     drop = True,\n",
        "                                     inplace = True,\n",
        "                                     )\n",
        "\n",
        "d_cycle999_rnd_ss_single.info()\n",
        "d_cycle999_rnd_ss_single.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gg71dV5xxcst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename sjs annotations\n",
        "\n",
        "ss_annotations = [\n",
        "                  'asp',\n",
        "                  'asp_rtnl',\n",
        "                  'dep',\n",
        "                  'dep_rtnl',\n",
        "                  'val',\n",
        "                  'val_rtnl',\n",
        "                  'prg',\n",
        "                  'tgd',\n",
        "                  'age',\n",
        "                  'dbty',\n",
        "                  'race',\n",
        "                  ]\n",
        "\n",
        "for s in ss_annotations:\n",
        "    d_cycle999_rnd_ss_single.rename(\n",
        "                                    columns = {s: s + '_ss'},\n",
        "                                    inplace = True,\n",
        "                                    )\n",
        "\n",
        "d_cycle999_rnd_ss_single.info()\n",
        "d_cycle999_rnd_ss_single.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Oig8yReJ2YnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# halving df to check API usage rate\n",
        "\n",
        "d_cycle999_rnd_ss_single_01 = d_cycle999_rnd_ss_single.iloc[:600]  ### first 600 rows\n",
        "d_cycle999_rnd_ss_single_02 = d_cycle999_rnd_ss_single.iloc[600:]  ### last 600 rows\n",
        "\n",
        "d_cycle999_rnd_ss_single_01.shape\n",
        "d_cycle999_rnd_ss_single_01.head(1)\n",
        "d_cycle999_rnd_ss_single_02.shape\n",
        "d_cycle999_rnd_ss_single_02.head(1)"
      ],
      "metadata": {
        "id": "176lD0qCApzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "qEbeORiHDnOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "\n",
        "# define prompts per tag\n",
        "\n",
        "prompts_per_tag = {\n",
        "                   'asp': [asp_prompt],\n",
        "                   'dep': [dep_prompt],\n",
        "                   'val': [val_prompt],\n",
        "                   'prg': [prg_prompt],\n",
        "                   'tgd': [tgd_prompt],\n",
        "                   'age': [age_prompt],\n",
        "                   'dbty': [dbty_prompt],\n",
        "                   'race': [race_prompt],\n",
        "                   }\n",
        "\n",
        "# annotate df\n",
        "\n",
        "d_cycle999_rnd_ss_gpt_dual_02 = annotate_dataframe_per_tag(\n",
        "                                                        d_cycle999_rnd_ss_single_02,\n",
        "                                                        prompts_per_tag,\n",
        "                                                        )\n",
        "\n",
        "d_cycle999_rnd_ss_gpt_dual_02.info()\n",
        "d_cycle999_rnd_ss_gpt_dual_02.head(3)\n",
        "\n",
        "d_cycle999_rnd_ss_gpt_dual_02.to_excel('d_cycle999_rnd_ss_gpt_dual_02.xlsx')"
      ],
      "metadata": {
        "id": "SyEqygDx_5bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Visualize\n",
        "Plots cycle-wise IAA scores.\n",
        "***"
      ],
      "metadata": {
        "id": "aaI7x86-xZRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Line chart: $\\kappa$ by cycle**"
      ],
      "metadata": {
        "id": "X_JG2O3-xy-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prelim: ARMHR 9/20 talk\n",
        "\n",
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/outputs/tables\n",
        "\n",
        "d_v = pd.read_excel('d_cycle_kappas.xlsx')"
      ],
      "metadata": {
        "id": "7k6jLvCZxYlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%cd .../figures\n",
        "\n",
        "sns.set(style = 'whitegrid')\n",
        "\n",
        "# custom x-axis labels\n",
        "\n",
        "custom_labels = [\n",
        "                 'cycle 1',\n",
        "                 'cycle 2',\n",
        "                 'cycle 3',\n",
        "                 'pre-GTP: cycle 4',\n",
        "                 'post-GPT: cycle 4',\n",
        "                 ]\n",
        "\n",
        "# plot\n",
        "\n",
        "plt.figure(figsize = (\n",
        "                      8,\n",
        "                      6,\n",
        "                      )\n",
        "          )\n",
        "\n",
        "plt.plot(\n",
        "         d_v['cycle'],\n",
        "         d_v['asp_cohens_k'],\n",
        "         label = \"asp\",\n",
        "         marker = 's',\n",
        "         alpha = 0.6,\n",
        "         color = 'hotpink',\n",
        "         )\n",
        "\n",
        "plt.plot(\n",
        "         d_v['cycle'],\n",
        "         d_v['dep_cohens_k'],\n",
        "         label = \"dep\",\n",
        "         marker = 's',\n",
        "         alpha = 0.6,\n",
        "         color = 'tomato',\n",
        "         )\n",
        "\n",
        "plt.plot(\n",
        "         d_v['cycle'],\n",
        "         d_v['val_cohens_k'],\n",
        "         label = \"val\",\n",
        "         marker = 's',\n",
        "         alpha = 0.6,\n",
        "         color = 'mediumorchid',\n",
        "         )\n",
        "\n",
        "# labels, title\n",
        "\n",
        "#plt.xlabel('Cycle')\n",
        "plt.ylabel(\"Cohen's $\\kappa$\")\n",
        "#plt.title(\"Cohen's $\\kappa$ Value per Cycle for ASP, DEP, and VAL\")\n",
        "\n",
        "# custom x-axis labels, 45-degree angle\n",
        "\n",
        "plt.xticks(\n",
        "           ticks = d_v['cycle'],\n",
        "           labels=custom_labels,\n",
        "           rotation = 45,\n",
        "           )\n",
        "\n",
        "\n",
        "\n",
        "# horizontal gridlines\n",
        "\n",
        "plt.grid(axis='x')\n",
        "\n",
        "# set line at 0.7 threshold\n",
        "\n",
        "plt.axhline(\n",
        "            y = 0.7,\n",
        "            color = 'red',\n",
        "            linewidth = 0.6,\n",
        "            linestyle = '--',\n",
        "            )\n",
        "\n",
        "# x-axis at 0\n",
        "\n",
        "plt.ylim(\n",
        "         0,\n",
        "         None,\n",
        "         )\n",
        "\n",
        "# legend\n",
        "\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=3, frameon=False)\n",
        "\n",
        "# x-axis ticks\n",
        "\n",
        "plt.gca().tick_params(axis='x', which='both', direction='in', length=5)  # Adds ticks to the x-axis labels\n",
        "\n",
        "# optimized markers\n",
        "\n",
        "#x0 = [2, 5, 5]\n",
        "#y0 = [0.72, 0.96, 0.97]\n",
        "#plt.plot(x0, y0, \"s\", markersize = 7, color = 'red')\n",
        "\n",
        "#for a,b in zip(x0, y0):\n",
        "#    plt.text(a, b, str(b),fontsize=9, ha='right',va='top')\n",
        "\n",
        "sns.despine(\n",
        "            left = True,\n",
        "            )\n",
        "\n",
        "# tight layout\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# save\n",
        "\n",
        "plt.savefig('cycle_kappa_line.png')\n",
        "\n",
        "# display\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5sxlLECpyMcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> End of aim_i_annotate_triangulate_iaa.ipynb"
      ],
      "metadata": {
        "id": "0oDrvLbhyRu8"
      }
    }
  ]
}