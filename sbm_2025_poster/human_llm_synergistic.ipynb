{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A GPT-4o-assisted, human-in-the-loop solution for intercoder reliability: guide for behavioral scientists"
      ],
      "metadata": {
        "id": "pG6vcc1FQyh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Performs qualitative deductive coding consistent with the  [CHALET](https://arxiv.org/abs/2405.05758) (**C**ollaborative **H**uman-LLM **A**na**L**ysis for **E**mpowering Conceptualization in Quali**T**ative Research) approach. Requires Ollama and/or OpenAI API key._\n",
        "\n",
        "_Note: Provided to SBM 2025 attendees for illustrative purposes only. Please adapt as needed and pilot on toy data. Do not pass data to the OpenAI API without reviewing OpenAI's Data Usage Policies and obtaining proper IRB approvals._"
      ],
      "metadata": {
        "id": "IV7B3DTqRCCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> human_llm_synergistic.ipynb<br>\n",
        "> Simone J. Skeen (03-12-2025)"
      ],
      "metadata": {
        "id": "p_xtxjHnRGgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Prepare](#scrollTo=TMzbQWcLnD3k)\n",
        "2. [Write](#scrollTo=ro3vWHGknw3w)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[`code_texts_deductively_llama`](#scrollTo=0TXsMF50oDSi)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[`code_instance_deductively_gpt`](#scrollTo=LrgYlrmo1OUW)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;[`code_texts_deductively_gpt`](#scrollTo=I6V00vzh2Na1)<br>\n",
        "3. [Code](#scrollTo=zXYJT6i9pSPf)<br>\n",
        "[Llama 3.2: local](#scrollTo=6hHjuQXrAqLE)<br>\n",
        "[GPT-4o: OpenAI API](#scrollTo=G1v8sP42Ah-n)<br>\n",
        "4. [Fidelity](#scrollTo=6upq1MSmxvoW)<br>\n",
        "[Compute Cohen's $\\kappa$](#scrollTo=DuSQ858FR2Ab)<br>\n",
        "[Flag disagreements](#scrollTo=mC58zS16Zttc)\n"
      ],
      "metadata": {
        "id": "Ot75cPlMRakI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMzbQWcLnD3k"
      },
      "source": [
        "### Prepare\n",
        "Installs, imports, requisite packages; customizes outputs.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Rq0_Hj0lIb"
      },
      "source": [
        "**Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0_sFYCXmuGE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "%pip install irrCAC\n",
        "%pip install lime\n",
        "%pip install ollama\n",
        "%pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1SoN_HI0lId"
      },
      "source": [
        "**Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8zJyDOUnTOa",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import ollama\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from google.colab import drive\n",
        "from irrCAC.raw import CAC\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "pd.options.mode.copy_on_write = True\n",
        "\n",
        "pd.set_option(\n",
        "    'display.max_columns',\n",
        "    None,\n",
        "    )\n",
        "\n",
        "pd.set_option(\n",
        "    'display.max_rows',\n",
        "    None,\n",
        "    )\n",
        "\n",
        "warnings.simplefilter(\n",
        "    action = 'ignore',\n",
        "    category = FutureWarning,\n",
        "    )\n",
        "\n",
        "#from langchain_community.llms import Ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKXbAwfN0lIf"
      },
      "source": [
        "**Set env variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euvcHEZgq5mS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = ' ' ### insert API key between quotation marks\n",
        "os.environ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30-J7Rqx0lIh"
      },
      "source": [
        "**Directory structure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8MCGclr0lIh"
      },
      "outputs": [],
      "source": [
        "mhp_subtle_discrimination/\n",
        "└── CEAI_lunch_and_learn/\n",
        "    ├── code\n",
        "    ├── inputs\n",
        "    ├── outputs\n",
        "    └── temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBxmzqVQ0lIi"
      },
      "source": [
        "**Ollama**<br>\n",
        "http://localhost:11434/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-EvjhQp0lIi"
      },
      "source": [
        "#### Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMvnCLepq5vY"
      },
      "outputs": [],
      "source": [
        "# mount gdrive\n",
        "\n",
        "drive.mount(\n",
        "    '/content/drive',\n",
        "    force_remount = True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQMuvvNqrpGm"
      },
      "outputs": [],
      "source": [
        "# creates and structures relative paths in a Google Colab/Drive environment\n",
        "\n",
        "#%mkdir Colab\n",
        "#%cd Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%mkdir human_llm_synergistic\n",
        "#%cd human_llm_synergistic"
      ],
      "metadata": {
        "id": "LRqi-hMN3q6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E762GqgXv9zn"
      },
      "outputs": [],
      "source": [
        "#%mkdir inputs outputs code temp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/human_llm_synergistic"
      ],
      "metadata": {
        "id": "UGjzhFqT4Yff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "FvW_mxkE0lIk"
      },
      "source": [
        "#### JupyterLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzFPrjAS0lIk"
      },
      "outputs": [],
      "source": [
        "# set wd\n",
        "\n",
        "wd = ' '\n",
        "os.chdir(wd)\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro3vWHGknw3w"
      },
      "source": [
        "### Write\n",
        "Defines qualitative.py module.\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqd3EKQVvaZd"
      },
      "outputs": [],
      "source": [
        "%cd code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TXsMF50oDSi"
      },
      "source": [
        "#### _code_texts_deductively_llama_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i-GDuKEjCB6"
      },
      "outputs": [],
      "source": [
        "%%writefile qualitative.py\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def code_texts_deductively_llama(df, alias, text_column, endpoint_url, prompt_template, model_name):\n",
        "    \"\"\"\n",
        "    Classifies each row of 'text' column in provided df in accord with human-specified prompt,\n",
        "    includes chain-of-thought reasoning, returning explanations for classification decision.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        The DataFrame containing the text to classify.\n",
        "    alias : str\n",
        "        The alias (for brevity) of the qualitative code to be applied.\n",
        "    text_column : str\n",
        "        The column name in df containing the text to be analyzed.\n",
        "    endpoint_url : str\n",
        "        The URL where locally hosted Llama model runs.\n",
        "    prompt_template : str\n",
        "        The prompt text with a placeholder (e.g., '{text}') where the row's text will be inserted.\n",
        "    model_name : str\n",
        "        The model tasked with qualitative deductive coding.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        The original DataFrame with two new columns: '{alias}_llm' (either \"0\" or \"1\")\n",
        "        and '{alias}_expl' (the explanation).\n",
        "    \"\"\"\n",
        "\n",
        "    # dynamically create {alias} column names\n",
        "\n",
        "    label_column = f'{alias}_llm'\n",
        "    explanation_column = f'{alias}_expl'\n",
        "\n",
        "    # create empty tag ['*_llm'] and reasoning ['*_expl'] column\n",
        "\n",
        "    df[label_column] = None\n",
        "    df[explanation_column] = None\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        row_text = row[text_column]\n",
        "\n",
        "        # replace '{text}' in prompt_template with df 'text' data\n",
        "\n",
        "        prompt = prompt_template.format(text = row_text)\n",
        "\n",
        "        # send request to local Llama endpoint.\n",
        "\n",
        "        response = requests.post(\n",
        "            endpoint_url,\n",
        "            headers = {'Content-Type': 'application/json'},\n",
        "            json = {\n",
        "                'model': model_name,\n",
        "                'prompt': prompt,\n",
        "                'stream': False\n",
        "                },\n",
        "        )\n",
        "\n",
        "        # print statements for debugging\n",
        "\n",
        "        print(response.status_code)\n",
        "        print(response.text)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            try:\n",
        "                # parse top-level JSON\n",
        "\n",
        "                result_json = response.json()\n",
        "\n",
        "                # 'response' field contains JSON string\n",
        "\n",
        "                raw_response_str = result_json.get('response', ' ')\n",
        "\n",
        "                # extract only the JSON portion: identify first `{` and last `}` braces\n",
        "\n",
        "                start_idx = raw_response_str.find(\"{\")\n",
        "                end_idx = raw_response_str.rfind(\"}\") + 1\n",
        "\n",
        "                if start_idx != -1 and end_idx != -1:\n",
        "\n",
        "                # extract and parse JSON portion\n",
        "\n",
        "                    valid_json_str = raw_response_str[start_idx:end_idx]\n",
        "                    parsed_output = json.loads(valid_json_str)\n",
        "\n",
        "                # extract tag and reasoning fields\n",
        "\n",
        "                    label = parsed_output.get(label_column)\n",
        "                    explanation = parsed_output.get(explanation_column)\n",
        "                else:\n",
        "                    print(\"No valid JSON found in response.\")\n",
        "                    label = None\n",
        "                    explanation = None\n",
        "\n",
        "            except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
        "                print(\"Parsing error:\", e)\n",
        "                label = None\n",
        "                explanation = None\n",
        "\n",
        "        else:\n",
        "            label = None\n",
        "            explanation = None\n",
        "\n",
        "        # insert classification results into df\n",
        "\n",
        "        df.at[idx, label_column] = label\n",
        "        df.at[idx, explanation_column] = explanation\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _code_instance_deductively_gpt_"
      ],
      "metadata": {
        "id": "LrgYlrmo1OUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a qualitative.py\n",
        "\n",
        "import time\n",
        "import openai\n",
        "\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "#api_key = ' '\n",
        "client = openai.OpenAI(api_key = api_key)\n",
        "\n",
        "def code_instance_deductively_gpt(text, prompts):\n",
        "    \"\"\"\n",
        "    Applies annotation decisions, based on multiple prompts, to a given text; provides rationale and explanation.\n",
        "    Parameters:\n",
        "    - text: The text to annotate.\n",
        "    - prompts: A list of prompts to apply to the text.\n",
        "\n",
        "    Returns:\n",
        "    - result: The combined result from all prompts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        # concatenate prompts\n",
        "\n",
        "        prompt_content = ' '.join(prompts)\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model = 'gpt-4o',\n",
        "            temperature = 0.2,\n",
        "            messages = [\n",
        "                {\n",
        "                    'role': 'system',\n",
        "                    'content': prompt_content\n",
        "                },\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': text\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # collect results\n",
        "\n",
        "        result = ' '\n",
        "        for choice in response.choices:\n",
        "            result += choice.message.content\n",
        "\n",
        "        print(f'{text}: {result}')\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f'Exception: {e}')\n",
        "        return 'error'"
      ],
      "metadata": {
        "id": "qRH3oevj1NqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _code_texts_deductively_gpt_"
      ],
      "metadata": {
        "id": "I6V00vzh2Na1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a qualitative.py\n",
        "\n",
        "def code_texts_deductively_gpt(df, prompts_per_code):\n",
        "    \"\"\"\n",
        "    Applies code_instance_deductively_gpt for multiple codes to each row in dataframe 'df'.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The dataframe containing texts to annotate.\n",
        "    - prompts_per_code: A dictionary with tag names as keys and a list of prompts as values.\n",
        "\n",
        "    Returns:\n",
        "    - df: The updated dataframe with annotation results.\n",
        "    \"\"\"\n",
        "    for index, row in df.iterrows():\n",
        "        for tag, prompts in prompts_per_code.items():\n",
        "            result = code_instance_deductively_gpt(row['text'], prompts)\n",
        "            if result == 'error':\n",
        "                continue\n",
        "\n",
        "            # initialize variables for annotation outputs\n",
        "\n",
        "            rationale, explanation = None, None\n",
        "\n",
        "            if f'{tag}_1' in result:\n",
        "                tag_value = 1\n",
        "\n",
        "                # extract rationale\n",
        "\n",
        "                rationale = result.split(f'{tag}_rationale:')[1].split(f'{tag}_explanation:')[0].strip() if f'{tag}_rationale:' in result else None\n",
        "\n",
        "                # extract explanation\n",
        "\n",
        "                explanation = result.split(f'{tag}_explanation:')[1].strip() if f'{tag}_explanation:' in result else None\n",
        "\n",
        "            else:\n",
        "                tag_value = 0\n",
        "\n",
        "            # results to df\n",
        "\n",
        "            df.at[index, f'{tag}_gpt'] = tag_value\n",
        "            df.at[index, f'{tag}_rtnl_gpt'] = rationale\n",
        "            df.at[index, f'{tag}_expl_gpt'] = explanation\n",
        "\n",
        "            # impose delay between API calls\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ZM-BoptZ-uwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJvBy9EZy6yC"
      },
      "source": [
        "#### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxxb-t8Mne3_"
      },
      "outputs": [],
      "source": [
        "from qualitative import(\n",
        "    code_texts_deductively_llama,\n",
        "    code_instance_deductively_gpt,\n",
        "    code_texts_deductively_gpt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MQLVYNIpRKg"
      },
      "outputs": [],
      "source": [
        "%cd ../inputs\n",
        "\n",
        "d = pd.read_excel(\n",
        "    'd_cycle_3_sjs.xlsx', ### d_cycle_3_sjs - IAA comparison w/ GPT-4o\n",
        "    index_col = [0],\n",
        "    )\n",
        "\n",
        "# replace ' ' w/ NaN\n",
        "\n",
        "d[[\n",
        "    #'<my_var>', ### replace with relevant varlist of codes\n",
        "    ]] = d[[\n",
        "        #'<my_var>', ### replace with relevant varlist of codes\n",
        "        ]].replace(\n",
        "            r'^\\s*$',\n",
        "            np.nan,\n",
        "            regex = True,\n",
        "            )\n",
        "\n",
        "# replace NaN w/ 0\n",
        "\n",
        "d[[\n",
        "    #'<my_var>',\n",
        "    ]] = d[[\n",
        "        #'<my_var>',\n",
        "        ]].apply(\n",
        "            pd.to_numeric,\n",
        "            downcast = 'integer',\n",
        "            )\n",
        "\n",
        "d.fillna(\n",
        "    0,\n",
        "    inplace = True,\n",
        "    )\n",
        "\n",
        "# texts: delete '<|PII|>' pseudoword\n",
        "\n",
        "texts = [\n",
        "    'text',\n",
        "         ]\n",
        "\n",
        "pseudoword_tokens = [\n",
        "    #'<SPL>', ### replace with preprocessing/anonymization artifacts\n",
        "    #'<|PII|>',\n",
        "    ]\n",
        "\n",
        "for t in texts:\n",
        "    d[t] = d[t].replace(\n",
        "        pseudoword_tokens,\n",
        "        ' ',\n",
        "        regex = True,\n",
        "        )\n",
        "\n",
        "# rationales: replace NaN w/ '.'\n",
        "\n",
        "rationales = [\n",
        "    #'<my_rtnl>', ### replace with relevant varlist of extracted rationales\n",
        "              ]\n",
        "\n",
        "for r in rationales:\n",
        "    d[r] = d[r].astype(str)\n",
        "    d[r] = d[r].str.replace(\n",
        "        r'0',\n",
        "        '.',\n",
        "        regex = True,\n",
        "        )\n",
        "\n",
        "# inspect\n",
        "\n",
        "d.info()\n",
        "d.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXYJT6i9pSPf"
      },
      "source": [
        "### Code\n",
        "Enables human-LLM deductive coding: human-specified per-tag prompts, JSON-.xlsx structured outputs.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Llama 3.2: local"
      ],
      "metadata": {
        "id": "6hHjuQXrAqLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_my_code_ (alias: `<my_code>`): prompt formulation**"
      ],
      "metadata": {
        "id": "8MptoNX7InKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### replace all instances of <my_*> in angle brackets with the specifics of your analysis\n",
        "role = '''\n",
        "You are tasked with applying pre-defined qualitative codes to <my_data>\n",
        "\n",
        "You will be provided a definition, instructions, and key exemplars of text to guide your coding decisions.\n",
        "'''\n",
        "\n",
        "definition = '''\n",
        "Definition of \"<my_code>\": <my_definition>\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "You will be provided with a piece of text. For each piece of text:\n",
        "- If it meets the definition of \"<my_code>,\" output <code_var> as \"1\".\n",
        "- Otherwise, output <code_var> as \"0\".\n",
        "- Also provide a short explanation in exactly two sentences, stored in <code_expl>.\n",
        "\n",
        "Please respond in valid JSON with keys \"<code_var>\" and \"<code_expl>\" only.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "'''\n",
        "\n",
        "clarification = '''\n",
        "- \"<my_code>\": <my_clarification>\n",
        "'''\n",
        "\n",
        "examples = '''\n",
        "Below are human-validated examples of \"<my_code>\"\n",
        "\n",
        "- \"<my_example>.\"\n",
        "'''"
      ],
      "metadata": {
        "id": "Z-ZgYvz7AhGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code deductively**"
      ],
      "metadata": {
        "id": "Jr7p63SDJLam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# concatenate prompt as f-string\n",
        "\n",
        "<my_code_prompt> = f'{role}{definition}{instruction}{clarification}{examples}' ### update with <my_code>_prompt\n",
        "print(\n",
        "    #<my_code_prompt>\n",
        "    )\n",
        "\n",
        "# locally hosted Llama endpoint\n",
        "\n",
        "llama_endpoint = 'http://localhost:11434/api/generate'\n",
        "\n",
        "# classify texts and update df\n",
        "\n",
        "d = code_texts_deductively_llama(\n",
        "    d,\n",
        "    alias = #'<my_code>', ### update with <my_code>\n",
        "    text_column = 'text',\n",
        "    endpoint_url = llama_endpoint,\n",
        "    prompt_template = refl_prompt,\n",
        "    model_name = 'llama3',\n",
        "    )\n"
      ],
      "metadata": {
        "id": "OLNNLfpZI5Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPT-4o: OpenAI API"
      ],
      "metadata": {
        "id": "G1v8sP42Ah-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Role assignment**"
      ],
      "metadata": {
        "id": "Mz52ct374QHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "role = '''\n",
        "You are tasked with applying pre-defined qualitative codes to <my_data>\n",
        "\n",
        "You will be provided a definition, instructions, and key exemplars of text to guide your coding decisions.\n",
        "'''"
      ],
      "metadata": {
        "id": "29Y0eN5w4PCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **_my_code_ (alias: `<my_code>`): prompt formulation**"
      ],
      "metadata": {
        "id": "QNG5CskHg5XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### replace all instances of <my_*> in angle brackets with the specifics of your analysis\n",
        "\n",
        "role = '''\n",
        "You are tasked with applying pre-defined qualitative codes to <my_data>\n",
        "\n",
        "You will be provided a definition, instructions, and key exemplars of text to guide your coding decisions.\n",
        "'''\n",
        "\n",
        "definition = '''\n",
        "Definition of \"<my_code>\": <my_definition>\n",
        "'''\n",
        "\n",
        "instruction = '''\n",
        "You will be provided with a piece of text. For each piece of text:\n",
        "- If it meets the definition of \"<my_code>,\" output <code_var> as \"1\".\n",
        "- Otherwise, output <code_var> as \"0\".\n",
        "- Also provide a short explanation in exactly two sentences, stored in <code_expl>.\n",
        "\n",
        "Please respond in valid JSON with keys \"<code_var>\" and \"<code_expl>\" only.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "'''\n",
        "\n",
        "clarification = '''\n",
        "- \"<my_code>\": <my_clarification>\n",
        "'''\n",
        "\n",
        "examples = '''\n",
        "Below are human-validated examples of \"<my_code>\"\n",
        "\n",
        "- \"<my_example>.\"\n",
        "'''\n",
        "\n",
        "# concatenate prompt as f-string\n",
        "\n",
        "<my_code_prompt> = f'{role}{definition}{instruction}{clarification}{examples}' ### update with <my_code>_prompt\n",
        "print(\n",
        "    #<my_code_prompt>\n",
        "    )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_6nFLSQ_g4n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code deductively**"
      ],
      "metadata": {
        "id": "yIjbO2-m5T6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "\n",
        "# define prompts per code\n",
        "\n",
        "prompts_per_code = {\n",
        "  #'<my_code>': [<my_code_prompt>],\n",
        "  }\n",
        "\n",
        "# annotate df\n",
        "\n",
        "d = code_texts_deductively_gpt(\n",
        "  d,\n",
        "  prompts_per_code,\n",
        "  )"
      ],
      "metadata": {
        "id": "vWfWk4EH5TPa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKFrQ9vU0lIo"
      },
      "outputs": [],
      "source": [
        "# inspect\n",
        "\n",
        "#print(d)\n",
        "d.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOfDGl5yp4AH"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "\n",
        "%cd ../outputs\n",
        "\n",
        "d.to_excel('d_coded.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Fidelity\n",
        "Calculates inter-coder reliability scores over indepednent coding applications, dummy codes disagreements for deliberation.\n",
        "***"
      ],
      "metadata": {
        "id": "6upq1MSmxvoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute Cohen's $\\kappa$"
      ],
      "metadata": {
        "id": "DuSQ858FR2Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../outputs\n",
        "\n",
        "d = pd.read_excel(\n",
        "    'd_coded.xlsx',\n",
        "    index_col = [0],\n",
        "    )\n",
        "\n",
        "#print(d.columns)\n",
        "\n",
        "# drop NaN\n",
        "\n",
        "d = d.dropna(subset = [\n",
        "    #'<my_code_gpt>', ### <my_code_gpt> = GPT-4o-output coding applications (dummy-coded)\n",
        "    ]\n",
        "             )\n",
        "\n",
        "# inspect\n",
        "\n",
        "d.info()\n",
        "d.head(3)"
      ],
      "metadata": {
        "id": "9bMn_VFvT88s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define kappa fx\n",
        "\n",
        "def calculate_kappa(d, col1, col2):\n",
        "    return cohen_kappa_score(d[col1], d[col2])\n",
        "\n",
        "col_pairs = [\n",
        "    #('<my_code>', '<my_code_gpt>'),\n",
        "    ]\n",
        "\n",
        "# initialize dict\n",
        "\n",
        "kappa_results = {}\n",
        "\n",
        "# % agreement loop\n",
        "\n",
        "def calculate_percent_agreement(df, col_pairs):\n",
        "    results = {}\n",
        "    for col1, col2 in col_pairs:\n",
        "        agreement = df[col1] == df[col2]\n",
        "        percent_agreement = (agreement.sum() / len(df)) * 100\n",
        "        results[f\"{col1} & {col2}\"] = percent_agreement\n",
        "    return results\n",
        "\n",
        "percent_agreement_results = calculate_percent_agreement(d, col_pairs)\n",
        "\n",
        "for pair, percent in percent_agreement_results.items():\n",
        "    print(f\"Percent agreement for {pair}: {percent:.2f}%\")\n",
        "\n",
        "# kappa loop\n",
        "\n",
        "for col1, col2 in col_pairs:\n",
        "    kappa = calculate_kappa(d, col1, col2)\n",
        "    kappa_results[f'{col1} and {col2}'] = kappa\n",
        "\n",
        "for pair, kappa in kappa_results.items():\n",
        "    print(f\"Cohen's Kappa for {pair}: {kappa:.2f}\")\n"
      ],
      "metadata": {
        "id": "zt2p47yQRyrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Flag disagreements"
      ],
      "metadata": {
        "id": "mC58zS16Zttc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# flag disagreements fx\n",
        "\n",
        "def encode_disagreements(row):\n",
        "    return 1 if row[0] != row[1] else 0\n",
        "\n",
        "col_dis = [\n",
        "    #('<my_code>', '<my_code_gpt>', '<my_code_dis>'), ### <my_code_dis> = dummy-coded disagreements between first two cols passed to encode_disagreements fx\n",
        "  ]\n",
        "\n",
        "for col1, col2, dis_col in col_dis:\n",
        "    d[dis_col] = d[[col1, col2]].apply(\n",
        "        encode_disagreements,\n",
        "        axis = 1,\n",
        "        )\n",
        "\n",
        "# export\n",
        "\n",
        "d.to_excel(f'd_coded_iaa.xlsx')"
      ],
      "metadata": {
        "id": "1U3ee6SkRyzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egPe72USqUis"
      },
      "source": [
        "> End of human_llm_synergistic.ipynb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}