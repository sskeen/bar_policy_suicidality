{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "wDa-A40SbzxS",
        "SS_mrveOcVru",
        "PCDusDJoc_3r",
        "DYuotbl5dsV8",
        "woZYUo6JgN1X",
        "qkBdAMlV1gjG",
        "7xoLLpCnlkpO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Passive suicidality in a repressive U.S. political context: Aim I"
      ],
      "metadata": {
        "id": "Q3dO24toXoAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_WIP - NOT FOR DISTRIBUTION_"
      ],
      "metadata": {
        "id": "pl9inWkyX3HI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> aim_i_train_tune_predict.ipynb<br>\n",
        "> Simone J. Skeen (10-07-24)"
      ],
      "metadata": {
        "id": "C3S8qRUNX577"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Prepare](xx)\n",
        "2. [Write](xx)\n",
        "3. [Preprocess](xx)\n",
        "4. [Train-Adapt-Test](xx)\n",
        "5. [Tune](xx)\n",
        "6. [Infer](xx)\n",
        "7. [Explain](xx)\n",
        "Interrogate\n",
        "Calibrate"
      ],
      "metadata": {
        "id": "oVjqU-H_ZRZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Prepare\n",
        "Installs, imports, and downloads requisite models and packages. Organizes RAP-consistent directory structure.\n",
        "***"
      ],
      "metadata": {
        "id": "8z4T1_xNZXFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install**"
      ],
      "metadata": {
        "id": "gg9PRwHCZlKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKF9nw1xXeLg",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "%pip install accelerate\n",
        "%pip install bitsandbytes\n",
        "%pip install causalnlp\n",
        "%pip install contractions\n",
        "%pip install datasets\n",
        "%pip install evaluate\n",
        "%pip install lime\n",
        "%pip install peft\n",
        "#%pip install trl\n",
        "%pip install unidecode\n",
        "%pip install wandb\n",
        "\n",
        "#%pip uninstall -y pyarrow datasets\n",
        "#%pip install pyarrow datasets\n",
        "\n",
        "#!python -m spacy download en_core_web_lg --user"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import**"
      ],
      "metadata": {
        "id": "WdMnyx7TZt44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "import ast\n",
        "import bitsandbytes as bnb\n",
        "import contractions\n",
        "#import en_core_web_lg\n",
        "import gzip\n",
        "import huggingface_hub\n",
        "import json\n",
        "import lime\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import peft\n",
        "import random\n",
        "import re\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import spacy\n",
        "import string\n",
        "import torch\n",
        "#import trl\n",
        "import wandb.sdk\n",
        "import warnings\n",
        "\n",
        "from causalnlp import Autocoder, CausalInferenceModel\n",
        "from google.colab import drive\n",
        "from lightgbm import LGBMClassifier\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from nltk.text import Text\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.model_selection import (\n",
        "                                     KFold,\n",
        "                                     ParameterGrid,\n",
        "                                     StratifiedKFold,\n",
        "                                     train_test_split,\n",
        "                                     )\n",
        "from sklearn.metrics import (\n",
        "                             average_precision_score,\n",
        "                             classification_report,\n",
        "                             f1_score,\n",
        "                             matthews_corrcoef,\n",
        "                             )\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "from textblob import TextBlob\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import (\n",
        "                              DataLoader,\n",
        "                              Dataset,\n",
        "                              TensorDataset,\n",
        "                              )\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "                          AdamW,\n",
        "                          BertForSequenceClassification,\n",
        "                          BertTokenizer,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          DistilBertForSequenceClassification,\n",
        "                          DistilBertTokenizer,\n",
        "                          RobertaForSequenceClassification,\n",
        "                          RobertaTokenizer,\n",
        "                          Trainer,\n",
        "                          TrainingArguments,\n",
        "                          )\n",
        "from unidecode import unidecode\n",
        "\n",
        "#spacy.cli.download('en_core_web_lg')\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "pd.options.mode.copy_on_write = True\n",
        "\n",
        "pd.set_option(\n",
        "              'display.max_columns',\n",
        "              None,\n",
        "              )\n",
        "\n",
        "pd.set_option(\n",
        "              'display.max_rows',\n",
        "              None,\n",
        "              )\n",
        "\n",
        "warnings.simplefilter(\n",
        "                      action = 'ignore',\n",
        "                      category = FutureWarning,\n",
        "                      )\n",
        "\n",
        "#!python -m prodigy stats"
      ],
      "metadata": {
        "id": "FiUj7-fcZs6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount gdrive**"
      ],
      "metadata": {
        "id": "1MY3yO_FZ0qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\n",
        "            '/content/drive',\n",
        "            #force_remount = True,\n",
        "            )"
      ],
      "metadata": {
        "id": "VjzlnRXkZs_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Structure directories**"
      ],
      "metadata": {
        "id": "tPQCYAa6aHrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality\n",
        "#%cd /content/drive/My Drive/#<my_project_folder>\n",
        "\n",
        "#%mkdir bar_policy_suicidality\n",
        "#%cd bar_policy_suicidality"
      ],
      "metadata": {
        "id": "vIS1Ed8wZtC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%mkdir inputs outputs code temp"
      ],
      "metadata": {
        "id": "TGHF0HLnZtG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd inputs\n",
        "#%mkdir annotation archives data"
      ],
      "metadata": {
        "id": "DLMtdecmZtKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd ../outputs\n",
        "#%mkdir models tables figures"
      ],
      "metadata": {
        "id": "J9__408paVgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bar_policy_suicidality/\n",
        "├── inputs/\n",
        "│   ├── archives\n",
        "│   │   └── ### archive name TKTK\n",
        "│   └── data\n",
        "│       └── d_annotated.xlsx\n",
        "├── outputs/\n",
        "│   ├── models\n",
        "│   ├── tables\n",
        "│   └── figures\n",
        "├── code/\n",
        "└── temp/"
      ],
      "metadata": {
        "id": "Z8If94wBM0a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Write\n",
        "Writes and imports preprocess.py, train.py, redact.py, predict.py.\n",
        "***"
      ],
      "metadata": {
        "id": "vA0d1kH7bOYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd code"
      ],
      "metadata": {
        "id": "0lDM3ZhLaVlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### preprocess.py"
      ],
      "metadata": {
        "id": "wDa-A40SbzxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_augment_training_data_with_rationales_**"
      ],
      "metadata": {
        "id": "zQ47VdBub3-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def augment_training_data_with_rationales(df):\n",
        "    \"\"\"\n",
        "    Identifies all pos_1 strn, duplicates as new row below, replaces new row 'text' with appended concatenated asp-dep-val 'rtnl'.\n",
        "    \"\"\"\n",
        "    augmented_rows = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        augmented_rows.append(row)\n",
        "\n",
        "        if row['strn'] == 1:\n",
        "            duplicate_row = row.copy()\n",
        "            duplicate_row['text'] = duplicate_row['rtnl']\n",
        "            augmented_rows.append(duplicate_row)\n",
        "\n",
        "    df_augmented = pd.DataFrame(augmented_rows)\n",
        "\n",
        "    return df_augmented"
      ],
      "metadata": {
        "id": "QvnNBR4saVos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_dummy_code_augmented_rows_**"
      ],
      "metadata": {
        "id": "YkGb9oK3cFKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a preprocess.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def dummy_code_augmented_rows(df):\n",
        "    \"\"\"\n",
        "    Identifies all rationale-augmented rows in df, dummy codes for deletion prior to evaluation.\n",
        "    \"\"\"\n",
        "    df = df.reset_index(drop = True)\n",
        "\n",
        "    df['aug'] = 0\n",
        "\n",
        "    for i in range(1, len(df)):\n",
        "        if df.at[i, 'rtnl'] != '.' and df.at[i, 'rtnl'] == df.at[i-1, 'rtnl']:\n",
        "            df.at[i, 'aug'] = 1\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "5Xl1XCTXaVsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_read_and_append_jsonl_archives_**"
      ],
      "metadata": {
        "id": "8aNRos-NGcPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a preprocess.py\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def read_and_append_jsonl_archives(directory, chunk_size = 10000):\n",
        "    \"\"\"\n",
        "    Reads and appends JSONL archives from Arctic Shift archives dir.\n",
        "    \"\"\"\n",
        "    d_posts = pd.DataFrame()\n",
        "    d_comments = pd.DataFrame()\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\"_posts.jsonl\"):\n",
        "            filepath = os.path.join(\n",
        "                                    directory,\n",
        "                                    filename,\n",
        "                                    )\n",
        "            for chunk in pd.read_json(\n",
        "                                      filepath,\n",
        "                                      lines = True,\n",
        "                                      chunksize = chunk_size,\n",
        "                                      ):\n",
        "                d_posts = pd.concat([d_posts,\n",
        "                                     chunk], ignore_index = True,\n",
        "                                    )\n",
        "\n",
        "        elif filename.endswith(\"_comments.jsonl\"):\n",
        "            filepath = os.path.join(\n",
        "                                    directory,\n",
        "                                    filename,\n",
        "                                    )\n",
        "            for chunk in pd.read_json(\n",
        "                                      filepath,\n",
        "                                      lines = True,\n",
        "                                      chunksize = chunk_size,\n",
        "                                      ):\n",
        "                d_comments = pd.concat([d_comments,\n",
        "                                        chunk], ignore_index = True,\n",
        "                                       )\n",
        "\n",
        "    return d_posts, d_comments"
      ],
      "metadata": {
        "id": "6iXXjPmwGYJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train.py"
      ],
      "metadata": {
        "id": "SS_mrveOcVru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_set_seed_**"
      ],
      "metadata": {
        "id": "oixV7l2zPITQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Set random seeds for reproducibility in Pytorch.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "ZqT7_DnFPBfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_train_eval_save_bl_models_**"
      ],
      "metadata": {
        "id": "aZSdSJH1jfap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a train.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import (\n",
        "                          BertForSequenceClassification,\n",
        "                          RobertaForSequenceClassification,\n",
        "                          DistilBertForSequenceClassification,\n",
        "                          BertTokenizer,\n",
        "                          RobertaTokenizer,\n",
        "                          DistilBertTokenizer,\n",
        "                          get_linear_schedule_with_warmup,\n",
        "                          )\n",
        "from sklearn.metrics import f1_score, matthews_corrcoef, average_precision_score\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def train_eval_save_bl_models(target_datasets, targets_and_class_weights, models, save_path, cycle, hyperparameter_grid):\n",
        "    \"\"\"\n",
        "    Fine-tune and eval pre-trained baseline LMs on multiple targets using target-specific train and test datasets.\n",
        "\n",
        "    Training sets d_train_{target} are split using 5-fold cross-validation: 4 folds for training and 1 fold for validation. Training\n",
        "    folds use augmented data; validation folds use original data. The best model is selected based on average performance across the\n",
        "    folds and evaluated on separate test sets d_test_{target}.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    target_datasets : dict\n",
        "        A dictionary where keys are target names and values are tuples containing the train and test datasets for each target.\n",
        "        For example: {'asp': (d_train_asp, d_test_asp), 'dep': (d_train_dep, d_test_dep)}\n",
        "\n",
        "    targets_and_class_weights : dict\n",
        "        A dictionary where keys are target names and values are lists of class weights corresponding to each target.\n",
        "\n",
        "    models : dict\n",
        "        A dictionary of models to evaluate. Each key is a model name, and each value is a tuple containing:\n",
        "        - the model class,\n",
        "        - the tokenizer class,\n",
        "        - the name of the pre-trained model.\n",
        "\n",
        "    save_path : str, optional\n",
        "        The path where the best models will be saved.\n",
        "\n",
        "    cycle : str\n",
        "        The cycle identifier: 'benchmark', indicating performance with default params; 'adapted', indicating performance\n",
        "        with in-domain adapted params.\n",
        "\n",
        "    hyperparameter_grid : dict\n",
        "        Dictionary containing hyperparameter values: batch_size, gradient_accumulation_steps, learning_rate, num_epochs, warmup_steps, and weight_decay.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    d_{cycle}_performance : pd.DataFrame\n",
        "        A df containing performance metrics for each target and model per fold per pre-specified cycle,\n",
        "        and final evaluation on test data.\n",
        "    \"\"\"\n",
        "\n",
        "    # verify cycle\n",
        "\n",
        "    print(f\"CYCLE: {cycle}\")\n",
        "\n",
        "    # check CUDA\n",
        "\n",
        "    print(\"CUDA: \", torch.cuda.is_available())\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "\n",
        "    # set seed\n",
        "\n",
        "    set_seed(56)\n",
        "\n",
        "    # unpack hyperparameters\n",
        "\n",
        "    batch_size = hyperparameter_grid.get('batch_size', 4)\n",
        "    gradient_accumulation_steps = hyperparameter_grid.get('gradient_accumulation_steps', 1)\n",
        "    learning_rate = hyperparameter_grid.get('learning_rate', 2e-5)\n",
        "    num_epochs = hyperparameter_grid.get('num_epochs', 2)\n",
        "    warmup_steps = hyperparameter_grid.get('warmup_steps', 0)\n",
        "    weight_decay = hyperparameter_grid.get('weight_decay', 0.0)\n",
        "\n",
        "    # best target x model F1 tracking\n",
        "\n",
        "    best_f1_scores = {target: {'score': 0, 'model': None, 'model_instance': None} for target in targets_and_class_weights}\n",
        "    results = []\n",
        "\n",
        "    # training loop: target x model\n",
        "\n",
        "    for target, class_weights in targets_and_class_weights.items():\n",
        "        print(\"\\n======================================================================================\")\n",
        "        print(f\"Label: {target}\")\n",
        "        print(\"======================================================================================\")\n",
        "\n",
        "        # target-specific datasets\n",
        "\n",
        "        d_train, d_test = target_datasets[target]\n",
        "\n",
        "        # split augmented v. non-augmented data\n",
        "\n",
        "        d_train_aug = d_train[d_train['aug'] == 1]\n",
        "        d_train_no_aug = d_train[d_train['aug'] == 0]\n",
        "\n",
        "        # prepare fold-wise training v. validation data\n",
        "\n",
        "        X_train_aug, y_train_aug = d_train_aug['text'].values, d_train_aug[target].values\n",
        "        X_train_no_aug, y_train_no_aug = d_train_no_aug['text'].values, d_train_no_aug[target].values\n",
        "        X_test, y_test = d_test['text'].values, d_test[target].values\n",
        "\n",
        "        # determine target type, encode (as needed)\n",
        "\n",
        "        target_type = 'binary' if len(np.unique(y_train_aug)) <= 2 else 'multiclass'\n",
        "        le = LabelEncoder() # Using separate LabelEncoder for each target data group to avoid encoding mismatch issues.\n",
        "        if target_type == 'binary':\n",
        "            #le = LabelEncoder()\n",
        "            y_train_aug = le.fit_transform(y_train_aug)\n",
        "            y_train_no_aug = le.fit_transform(y_train_no_aug) # Re-encode with new encoder\n",
        "            y_test = le.transform(y_test)\n",
        "\n",
        "        # define k folds\n",
        "\n",
        "        k_fold = StratifiedKFold(\n",
        "                                 n_splits = 5,\n",
        "                                 shuffle = True,\n",
        "                                 random_state = 56,\n",
        "                                 )\n",
        "\n",
        "        for model_name, (model_class, tokenizer_class, pretrained_model_name) in models.items():\n",
        "            print(f\"\\nFine-tuning {model_name} for {target}\")\n",
        "            print(\"--------------------------------------------------------------------------------------\")\n",
        "\n",
        "            fold_f1, fold_mcc, fold_auprc = [], [], []  ### store fold-wise performance metrics\n",
        "\n",
        "            # initialize tokenizer\n",
        "\n",
        "            tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "\n",
        "            for fold_idx, (train_no_aug_idx, valid_idx) in enumerate(k_fold.split(X_train_no_aug, y_train_no_aug)):\n",
        "                print(f\"\\nFold {fold_idx + 1}/5\")\n",
        "\n",
        "                # create training set: combine aug = 1 (augmented) with fold-specific aug = 0 (non-augmented)\n",
        "\n",
        "                X_train_fold_aug = X_train_aug\n",
        "                y_train_fold_aug = y_train_aug\n",
        "\n",
        "                X_train_fold_no_aug, X_valid_fold = X_train_no_aug[train_no_aug_idx], X_train_no_aug[valid_idx]\n",
        "                y_train_fold_no_aug, y_valid_fold = y_train_no_aug[train_no_aug_idx], y_train_no_aug[valid_idx]\n",
        "\n",
        "                # combine augmented and non-augmented training data\n",
        "\n",
        "                X_train_fold = np.concatenate([X_train_fold_aug, X_train_fold_no_aug])\n",
        "                y_train_fold = np.concatenate([y_train_fold_aug, y_train_fold_no_aug])\n",
        "\n",
        "                # tokenize training and validation data\n",
        "\n",
        "                encoded_train = tokenizer(\n",
        "                                          X_train_fold.tolist(),\n",
        "                                          padding = True,\n",
        "                                          truncation = True,\n",
        "                                          return_tensors = 'pt',\n",
        "                                          )\n",
        "\n",
        "                encoded_valid = tokenizer(\n",
        "                                          X_valid_fold.tolist(),\n",
        "                                          padding = True,\n",
        "                                          truncation = True,\n",
        "                                          return_tensors = 'pt',\n",
        "                                          )\n",
        "\n",
        "                train_dataset = TensorDataset(\n",
        "                                              encoded_train['input_ids'],\n",
        "                                              encoded_train['attention_mask'],\n",
        "                                              torch.tensor(y_train_fold),\n",
        "                                              )\n",
        "\n",
        "                valid_dataset = TensorDataset(\n",
        "                                              encoded_valid['input_ids'],\n",
        "                                              encoded_valid['attention_mask'],\n",
        "                                              torch.tensor(y_valid_fold),\n",
        "                                              )\n",
        "\n",
        "                train_loader = DataLoader(\n",
        "                                          train_dataset,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle = True,\n",
        "                                          )\n",
        "\n",
        "                valid_loader = DataLoader(\n",
        "                                          valid_dataset,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle = False,\n",
        "                                          )\n",
        "\n",
        "                # instantiate model\n",
        "\n",
        "                model = model_class.from_pretrained(pretrained_model_name)\n",
        "\n",
        "                # migrate to CUDA\n",
        "\n",
        "                use_cuda = torch.cuda.is_available()\n",
        "                if use_cuda:\n",
        "                    model = model.cuda()\n",
        "\n",
        "                # set optimizer + scheduler\n",
        "\n",
        "                optimizer = torch.optim.AdamW(\n",
        "                                              model.parameters(),\n",
        "                                              lr = learning_rate,\n",
        "                                              weight_decay = weight_decay,\n",
        "                                              )\n",
        "\n",
        "                total_steps = len(train_loader) * num_epochs\n",
        "                #scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "                #                                              optimizer,\n",
        "                #                                              start_factor = 0.1,\n",
        "                #                                              #total_iters = warmup_steps,\n",
        "                #                                              total_iters = total_steps, # Fix: corrected from warmup_steps to total_steps\n",
        "                #                                              )\n",
        "\n",
        "                scheduler = get_linear_schedule_with_warmup(\n",
        "                                                            optimizer,\n",
        "                                                            num_warmup_steps = warmup_steps,\n",
        "                                                            num_training_steps = total_steps\n",
        "                                                            )\n",
        "\n",
        "                # fine-tune model on training folds (x4)\n",
        "\n",
        "                model.train()\n",
        "                criterion = CrossEntropyLoss(weight = torch.tensor(\n",
        "                                                                   class_weights,\n",
        "                                                                   dtype = torch.float\n",
        "                                                                   ).cuda() if use_cuda else torch.tensor(\n",
        "                                                                                                          class_weights,\n",
        "                                                                                                          dtype = torch.float\n",
        "                                                                                                          )\n",
        "                                             )\n",
        "\n",
        "                for epoch in range(num_epochs):\n",
        "                    for i, batch in enumerate(train_loader):\n",
        "                        input_ids, attention_mask, labels = batch\n",
        "                        labels = labels.long()\n",
        "\n",
        "                        if use_cuda:\n",
        "                            input_ids, attention_mask, labels = input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
        "\n",
        "                        outputs = model(input_ids, attention_mask = attention_mask)\n",
        "                        logits = outputs.logits\n",
        "                        loss = criterion(logits, labels)\n",
        "\n",
        "                        # accumulate gradients, normalize loss\n",
        "\n",
        "                        loss = loss / gradient_accumulation_steps\n",
        "                        loss.backward()\n",
        "\n",
        "                        # update model weights post-accumulation steps\n",
        "\n",
        "                        if (i + 1) % gradient_accumulation_steps == 0:\n",
        "                            optimizer.step()\n",
        "                            optimizer.zero_grad()\n",
        "\n",
        "                        # apply learning rate scheduler\n",
        "\n",
        "                        scheduler.step()\n",
        "\n",
        "                # evaluate on validation fold (x1)\n",
        "\n",
        "                model.eval()\n",
        "                all_predictions, all_true_labels = [], []\n",
        "                with torch.no_grad():\n",
        "                    for batch in valid_loader:\n",
        "                        input_ids, attention_mask, labels = batch\n",
        "\n",
        "                        if use_cuda:\n",
        "                            input_ids, attention_mask, labels = input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
        "\n",
        "                        outputs = model(input_ids, attention_mask = attention_mask)\n",
        "                        logits = outputs.logits\n",
        "                        predictions = torch.argmax(logits, dim = 1).tolist()\n",
        "                        all_predictions.extend(predictions)\n",
        "                        all_true_labels.extend(labels.tolist())\n",
        "\n",
        "                # performance metrics per validation fold\n",
        "\n",
        "                f1_macro = f1_score(\n",
        "                                    all_true_labels,\n",
        "                                    all_predictions,\n",
        "                                    average = 'macro',\n",
        "                                    )\n",
        "\n",
        "                mcc = matthews_corrcoef(\n",
        "                                        all_true_labels,\n",
        "                                        all_predictions,\n",
        "                                        )\n",
        "\n",
        "                auprc = average_precision_score(\n",
        "                                                all_true_labels,\n",
        "                                                all_predictions,\n",
        "                                                average = 'macro',\n",
        "                                                )\n",
        "\n",
        "                fold_f1.append(f1_macro)\n",
        "                fold_mcc.append(mcc)\n",
        "                fold_auprc.append(auprc)\n",
        "\n",
        "            # mean results over folds, track best model\n",
        "\n",
        "            mean_f1 = np.mean(fold_f1)\n",
        "            if mean_f1 > best_f1_scores[target]['score']:\n",
        "                best_f1_scores[target]['score'] = mean_f1\n",
        "                best_f1_scores[target]['model'] = model_name\n",
        "                best_f1_scores[target]['model_instance'] = model\n",
        "\n",
        "                save_model_name = f'{target}_{model_name}_best_{cycle}_model.pt'\n",
        "                torch.save(model.state_dict(), save_path + save_model_name)\n",
        "\n",
        "            # store results for each fold\n",
        "\n",
        "            for i in range(5):\n",
        "                results.append({\n",
        "                                'target': target,\n",
        "                                'model': model_name,\n",
        "                                'fold': i + 1,\n",
        "                                'f1_macro': fold_f1[i],\n",
        "                                'mcc': fold_mcc[i],\n",
        "                                'auprc': fold_auprc[i]\n",
        "                                })\n",
        "\n",
        "        # test on held-out d_test_{target} df\n",
        "\n",
        "        print(f\"\\nTest on held-out d_test_{target} using the best {best_f1_scores[target]['model']} model\")\n",
        "        print(\"--------------------------------------------------------------------------------------\")\n",
        "        test_model = best_f1_scores[target]['model_instance']\n",
        "        test_model.eval()\n",
        "\n",
        "        #tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "        tokenizer = models[best_f1_scores[target]['model']][1].from_pretrained(pretrained_model_name)  # Fix: ensure correct tokenizer for testing\n",
        "        encoded_test = tokenizer(\n",
        "                                 X_test.tolist(),\n",
        "                                 padding = True,\n",
        "                                 truncation = True,\n",
        "                                 return_tensors = 'pt',\n",
        "                                 )\n",
        "\n",
        "        test_dataset = TensorDataset(\n",
        "                                     encoded_test['input_ids'],\n",
        "                                     encoded_test['attention_mask'],\n",
        "                                     torch.tensor(y_test),\n",
        "                                     )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "                                 test_dataset,\n",
        "                                 batch_size = batch_size,\n",
        "                                 shuffle = False,\n",
        "                                 )\n",
        "\n",
        "        all_test_predictions, all_test_true_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids, attention_mask, labels = batch\n",
        "\n",
        "                if use_cuda:\n",
        "                    input_ids, attention_mask, labels = input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
        "\n",
        "                outputs = test_model(input_ids, attention_mask = attention_mask)\n",
        "                logits = outputs.logits\n",
        "                test_predictions = torch.argmax(logits, dim = 1).tolist()\n",
        "                all_test_predictions.extend(test_predictions)\n",
        "                all_test_true_labels.extend(labels.tolist())\n",
        "\n",
        "        # preformance metrics for held-out d_test_{target} df\n",
        "\n",
        "        test_f1_macro = f1_score(\n",
        "                                 all_test_true_labels,\n",
        "                                 all_test_predictions,\n",
        "                                 average='macro',\n",
        "                                 )\n",
        "\n",
        "        test_mcc = matthews_corrcoef(\n",
        "                                     all_test_true_labels,\n",
        "                                     all_test_predictions,\n",
        "                                     )\n",
        "\n",
        "        test_auprc = average_precision_score(\n",
        "                                             all_test_true_labels,\n",
        "                                             all_test_predictions,\n",
        "                                             average = 'macro',\n",
        "                                             )\n",
        "\n",
        "        # display\n",
        "\n",
        "        print(f\"Test F1 (macro) for {target}: {test_f1_macro}\")\n",
        "        print(f\"Test MCC for {target}: {test_mcc}\")\n",
        "        print(f\"Test AUPRC for {target}: {test_auprc}\")\n",
        "\n",
        "        # store\n",
        "\n",
        "        results.append({\n",
        "                        'target': target,\n",
        "                        'model': best_f1_scores[target]['model'],\n",
        "                        'fold': 'Test',\n",
        "                        'f1_macro': test_f1_macro,\n",
        "                        'mcc': test_mcc,\n",
        "                        'auprc': test_auprc\n",
        "                        })\n",
        "\n",
        "    # summarize + return d_{cycle}_performance df\n",
        "\n",
        "    print(\"\\n--------------------------------------------------------------------------------------\")\n",
        "    print(f\"Summary\")\n",
        "    print(\"--------------------------------------------------------------------------------------\")\n",
        "\n",
        "    for target, info in best_f1_scores.items():\n",
        "        print(f\"Best F1 (macro) for {target}: {info['score']} achieved by {info['model']}\")\n",
        "\n",
        "    d_performance = pd.DataFrame(results)\n",
        "    print(f\"\\nd_{cycle}_performance:\")\n",
        "    print(d_performance.head(5))\n",
        "    d_performance.to_excel(f'{save_path}d_{cycle}_performance.xlsx')"
      ],
      "metadata": {
        "id": "3i3sJ8qhHNKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_performance_barplot_**"
      ],
      "metadata": {
        "id": "gsmsZ8TrQVLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a train.py\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def performance_barplot(df, plot_name):\n",
        "    \"\"\"\n",
        "    Creates a barplot based on train_eval_save_bl_models d_{cycle}_performance output.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        The input dataframe that should contain columns 'target', 'f1_macro', and 'model'.\n",
        "\n",
        "    plot_name : str\n",
        "        The name of the dataframe (used for naming the saved file).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Matplotlib Axes object containing the barplot.\n",
        "    \"\"\"\n",
        "\n",
        "    model_colors = [\n",
        "                    'paleturquoise',   # BERT\n",
        "                    'palegreen', # RoBERTa\n",
        "                    'lightpink', # DistilBERT\n",
        "                    ]\n",
        "\n",
        "    fig, ax = plt.subplots(\n",
        "                           figsize=(\n",
        "                                    14,   # width\n",
        "                                    5.5,   # height\n",
        "                                    )\n",
        "                           )\n",
        "\n",
        "\n",
        "    ax = sns.barplot(\n",
        "                     data = df,\n",
        "                     x = 'target',\n",
        "                     y = 'f1_macro',\n",
        "                     hue = 'model',\n",
        "                     saturation = 0.75,\n",
        "                     dodge = 'auto',\n",
        "                     palette = model_colors,\n",
        "                     linewidth = 0.5,\n",
        "                     alpha = 0.8,\n",
        "                     #edgecolor = 'gray',\n",
        "                     errcolor = 'darkgray',\n",
        "                     errwidth = 0.60,\n",
        "                     capsize = 0.04,\n",
        "                     )\n",
        "\n",
        "    # legend\n",
        "\n",
        "    ax.legend(\n",
        "              loc = 'upper left',\n",
        "              bbox_to_anchor = (\n",
        "                                0.35,\n",
        "                                1.1,\n",
        "                                ),\n",
        "              ncol = 3,\n",
        "              #fancybox = True,\n",
        "              #shadow = True\n",
        "              title = None,\n",
        "              frameon = False,\n",
        "              )\n",
        "\n",
        "    plt.ylim(\n",
        "             0,\n",
        "             1,\n",
        "             )\n",
        "\n",
        "    sns.set_style(\n",
        "                  style = 'whitegrid',\n",
        "                  rc = None,\n",
        "                  )\n",
        "\n",
        "    sns.despine(\n",
        "                left = True,\n",
        "                )\n",
        "\n",
        "\n",
        "    # label axes\n",
        "\n",
        "    ax.set_ylabel(\n",
        "                  '$F_1$ (macro)',\n",
        "                  fontsize = 14,\n",
        "                  labelpad = 10,\n",
        "                  )\n",
        "\n",
        "    ax.set_xlabel(\n",
        "                  'Target',\n",
        "                  fontsize = 14,\n",
        "                  labelpad = 10,\n",
        "                  )\n",
        "\n",
        "    plt.setp(ax.get_legend().get_texts(), fontsize = 12)\n",
        "    plt.yticks(fontsize = 12)\n",
        "\n",
        "    ax.set_xticklabels(\n",
        "                       labels = [\n",
        "                                 'asp',\n",
        "                                 'dep',\n",
        "                                 'val',\n",
        "                                 'prg',\n",
        "                                 'tgd',\n",
        "                                 'age',\n",
        "                                 'race',\n",
        "                                 'dbty',\n",
        "                                ],\n",
        "                        #rotation = 45,\n",
        "                        horizontalalignment = 'right',\n",
        "                        fontsize = '12',\n",
        "                        )\n",
        "\n",
        "    # label bars\n",
        "\n",
        "    for i in ax.containers:\n",
        "        ax.bar_label(\n",
        "                     i,\n",
        "                     fmt = '%.2f',\n",
        "                     label_type = 'edge',\n",
        "                     #color = 'red',\n",
        "                     rotation = 45,\n",
        "                     fontsize = 8,\n",
        "                     padding = 5,\n",
        "                     )\n",
        "    # save\n",
        "\n",
        "    file_name = f'{plot_name}_bar.png'\n",
        "    plt.savefig(file_name)\n",
        "\n",
        "    # display\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "yWDdeLumQQyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_performance_scatterplot_**"
      ],
      "metadata": {
        "id": "84AhevrqZZ_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a train.py\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "def performance_scatterplot(df, plot_name):\n",
        "    \"\"\"\n",
        "    Creates a categorical scatterplot with custom aesthetics, markers, error bars, and a legend.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): Input dataframe containing columns 'target', 'f1_macro', 'model', and 'fold'.\n",
        "\n",
        "    plot_name (str): The name used for saving the output plot file (without extension).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Matplotlib Axes object containing the barplot.\n",
        "    \"\"\"\n",
        "\n",
        "    # aesthetics\n",
        "\n",
        "    model_colors = [\n",
        "                    '#87bc45',   # BERT\n",
        "                    '#27aeef',   # RoBERTa\n",
        "                    '#b33dc6',   # DistilBERT\n",
        "                   ]\n",
        "\n",
        "      ### SJS 10/1: last three colors in \"Retro Metro (Default)\" https://www.heavy.ai/blog/12-color-palettes-for-telling-better-stories-with-your-data\n",
        "\n",
        "    sns.set_style(\n",
        "                  style = 'whitegrid',\n",
        "                  rc = None,\n",
        "                  )\n",
        "\n",
        "    # map target: numeric position\n",
        "\n",
        "    target_mapping = {\n",
        "                      'asp': 0,\n",
        "                      'dep': 2,\n",
        "                      'val': 4,\n",
        "                      'prg': 6,\n",
        "                      'tgd': 8,\n",
        "                      'age': 10,\n",
        "                      'race': 12,\n",
        "                      'dbty': 14\n",
        "                     }\n",
        "\n",
        "    df['target_numeric'] = df['target'].map(target_mapping)\n",
        "    df['target_numeric'] = pd.to_numeric(df['target_numeric'])\n",
        "\n",
        "    # inject noise for jitter\n",
        "\n",
        "    df['target_jitter'] = df['target_numeric'] + np.random.uniform(\n",
        "                                                                   -0.35,\n",
        "                                                                   0.35,\n",
        "                                                                   size = len(df),\n",
        "                                                                   )\n",
        "\n",
        "    # plot\n",
        "\n",
        "    plt.figure(figsize = (12, 5.5))\n",
        "\n",
        "    # distinguish markers: fold v. held-out test set\n",
        "\n",
        "    for fold_value, marker in [('Test', 'o'), ('non-Test', '.')]:\n",
        "        if fold_value == 'Test':\n",
        "            data_subset = df[df['fold'] == 'Test']\n",
        "        else:\n",
        "            data_subset = df[df['fold'] != 'Test']\n",
        "\n",
        "        sns.scatterplot(\n",
        "                        data = data_subset,\n",
        "                        x = 'target_jitter',\n",
        "                        y = 'f1_macro',\n",
        "                        hue = 'model',\n",
        "                        palette = model_colors,\n",
        "                        s = 40,\n",
        "                        alpha = 0.6,\n",
        "                        marker = marker,\n",
        "                       )\n",
        "\n",
        "    # mean and SD of f1_macro for each target x model\n",
        "\n",
        "    mean_std_df = df.groupby(['target', 'model']).agg(\n",
        "                                                      mean_f1_macro=('f1_macro', 'mean'),\n",
        "                                                      std_f1_macro=('f1_macro', 'std')\n",
        "                                                      ).reset_index()\n",
        "\n",
        "    # add target_numeric values to mean_std_df for plotting means and error bars\n",
        "\n",
        "    mean_std_df['target_numeric'] = mean_std_df['target'].map(target_mapping).astype(float)\n",
        "\n",
        "    # x-axis offsets\n",
        "\n",
        "    model_offsets = {\n",
        "                     'bert-base-uncased': -0.3,\n",
        "                     'roberta-base': 0.0,\n",
        "                     'distilbert-base-uncased': 0.3,\n",
        "                     }\n",
        "\n",
        "    mean_std_df['target_offset'] = mean_std_df['target_numeric'] + mean_std_df['model'].map(model_offsets)\n",
        "\n",
        "    # means (SDs)\n",
        "\n",
        "    for model in mean_std_df['model'].unique():\n",
        "        model_data = mean_std_df[mean_std_df['model'] == model]\n",
        "\n",
        "    # inspect for NaNs\n",
        "\n",
        "        if not model_data[['target_offset', 'mean_f1_macro', 'std_f1_macro']].isnull().any().any():\n",
        "            plt.errorbar(\n",
        "                         model_data['target_offset'],\n",
        "                         model_data['mean_f1_macro'],\n",
        "                         yerr = model_data['std_f1_macro'],\n",
        "                         fmt = 'D',\n",
        "                         markersize = 6,\n",
        "                         capsize = 0,\n",
        "                         label = f'{model} M (SD)',\n",
        "                         color = model_colors[mean_std_df['model'].unique().tolist().index(model)]\n",
        "                        )\n",
        "\n",
        "    # x-tick: map to targets\n",
        "\n",
        "    plt.xticks(\n",
        "               [0, 2, 4, 6, 8, 10, 12, 14],\n",
        "               ['asp', 'dep', 'val', 'prg', 'tgd', 'age', 'race', 'dbty']\n",
        "              )\n",
        "\n",
        "    # label axes\n",
        "\n",
        "    plt.ylim(0, 1)\n",
        "    ax = plt.gca()\n",
        "    ax.set_ylabel(\n",
        "                  '$F_1$ (macro)',\n",
        "                  fontsize = 12,\n",
        "                  labelpad = 10,\n",
        "                  )\n",
        "\n",
        "    ax.set_xlabel(\n",
        "                  'Target',\n",
        "                  fontsize = 12,\n",
        "                  labelpad = 10,\n",
        "                  )\n",
        "\n",
        "    sns.despine(left = True)\n",
        "    ax.grid(axis = 'x')\n",
        "\n",
        "    # set line at 0.9 threshold\n",
        "\n",
        "    ax.axhline(\n",
        "               y = 0.9,\n",
        "               color = 'r',\n",
        "               linewidth = 0.6,\n",
        "               linestyle = '--',\n",
        "               )\n",
        "\n",
        "    # custom legend\n",
        "\n",
        "    legend_elements = [\n",
        "                       Line2D([0], [0], marker = 'o', color = 'w', label = 'bert', markersize = 8, markerfacecolor = '#87bc45', lw = 0),\n",
        "                       Line2D([0], [0], marker = 'o', color = 'w', label = 'roberta', markersize = 8, markerfacecolor = '#27aeef', lw = 0),\n",
        "                       Line2D([0], [0], marker = 'o', color = 'w', label = 'distilbert', markersize = 8, markerfacecolor = '#b33dc6', lw = 0),\n",
        "                       Line2D([0], [0], marker = 'D', color = 'black', label = 'M (SD)', markersize = 7, lw = 1, linestyle = '-', markeredgewidth = 1)\n",
        "                      ]\n",
        "\n",
        "    ax.legend(\n",
        "              handles = legend_elements,\n",
        "              loc = 'upper center',\n",
        "              bbox_to_anchor = (0.5, 1.15),\n",
        "              ncol = 4,\n",
        "              fontsize = 9,\n",
        "              frameon = False,\n",
        "              )\n",
        "\n",
        "    # save\n",
        "\n",
        "    file_name = f'{plot_name}_scatter.png'\n",
        "    plt.savefig(file_name)\n",
        "\n",
        "    # display\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "muGmQSbhZZGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_iterative_stratified_train_test_split_with_rationales_**"
      ],
      "metadata": {
        "id": "Op5JVhV0ceia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a train.py\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def iterative_stratified_train_test_split_with_rationales(df, targets, test_size, random_state):\n",
        "    \"\"\"\n",
        "    Splits df into target-stratified train and test sets for each target in targets list:\n",
        "    d_train_{target}, d_test_{target}, respectively. Partitions 'rationales' (aug = 1) to train\n",
        "    set. Returns a dict with target names as keys\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize dict\n",
        "\n",
        "    target_datasets = {}\n",
        "\n",
        "    for target in targets:\n",
        "\n",
        "        # create 'targets' col for stratification\n",
        "\n",
        "        df_target = df.copy()\n",
        "        df_target['targets'] = df[target]\n",
        "\n",
        "        # split augmented vs. non-augmented rows\n",
        "\n",
        "        aug_rows = df_target[df_target['aug'] == 1]\n",
        "        non_aug_rows = df_target[df_target['aug'] != 1]\n",
        "\n",
        "        if non_aug_rows.empty:\n",
        "            print(f\"No non-augmented rows for target {target}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # stratified train-test split on non-augmented rows only\n",
        "\n",
        "        train_non_aug, test_non_aug = train_test_split(\n",
        "                                                       non_aug_rows,\n",
        "                                                       test_size = test_size,\n",
        "                                                       stratify = non_aug_rows['targets'],\n",
        "                                                       random_state = random_state,\n",
        "                                                      )\n",
        "\n",
        "        # concat augmented rows back into train set\n",
        "\n",
        "        d_train = pd.concat([train_non_aug, aug_rows])\n",
        "\n",
        "        # shuffle + reset index: train set\n",
        "\n",
        "        d_train = d_train.sample(\n",
        "                                 frac = 1,\n",
        "                                 random_state = random_state,\n",
        "                                 ).reset_index(drop = True)\n",
        "\n",
        "        # retain 'text', 'aug', target cols\n",
        "\n",
        "        d_train = d_train[['text', 'aug', target]]\n",
        "        d_test = test_non_aug[['text', 'aug', target]]\n",
        "\n",
        "        # reset index: test set\n",
        "\n",
        "        d_test = d_test.reset_index(drop = True)\n",
        "\n",
        "        # add train and test sets as tuples to target_datasets dict\n",
        "\n",
        "        target_datasets[target] = (d_train, d_test)\n",
        "\n",
        "        # inspect\n",
        "\n",
        "        print(f\"\\nVerify: d_train_{target} 'aug' count\")\n",
        "        print(d_train['aug'].value_counts(normalize = False))\n",
        "        print(f\"\\nVerify: d_test_{target} 'aug' count\")\n",
        "        print(d_test['aug'].value_counts(normalize = False))\n",
        "\n",
        "        print(f\"\\n--------------------------------------------------------------------------------------\")\n",
        "        print(f\"d_train_{target}: Augmented training data for target '{target}'\")\n",
        "        print(f\"--------------------------------------------------------------------------------------\")\n",
        "        print(d_train.shape)\n",
        "        print(d_train[target].value_counts(normalize = True))\n",
        "        print(d_train.head(6))\n",
        "\n",
        "        print(f\"\\n--------------------------------------------------------------------------------------\")\n",
        "        print(f\"d_test_{target}: De-augmented testing data for target '{target}'\")\n",
        "        print(f\"--------------------------------------------------------------------------------------\")\n",
        "        print(d_test.shape)\n",
        "        print(d_test[target].value_counts(normalize = True))\n",
        "        print(d_test.head(6))\n",
        "\n",
        "    return target_datasets\n"
      ],
      "metadata": {
        "id": "Qwk1WTf-aVvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_tune_and_optimize_model_hyperparams_**"
      ],
      "metadata": {
        "id": "ZH6tQ422ctK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a train.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def tune_and_optimize_model_hyperparams(tokenizer, model_class, pretrained_model_name, d_train, d_test, target, class_weights, save_path, hyperparameter_grid):\n",
        "    \"\"\"\n",
        "    Tune and optimize model hyperparameters for a specific model-target combination.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "\n",
        "    tokenizer:\n",
        "        Pre-trained tokenizer.\n",
        "\n",
        "    model_class:\n",
        "        Pre-trained model class.\n",
        "\n",
        "    pretrained_model_name:\n",
        "        Name of the pre-trained model.\n",
        "\n",
        "    d_train : pd.DataFrame\n",
        "        Training dataset.\n",
        "\n",
        "    d_test : pd.DataFrame\n",
        "        Test dataset.\n",
        "\n",
        "    target : str\n",
        "        The target variable for classification.\n",
        "\n",
        "    class_weights : torch.tensor\n",
        "        Weights for each class.\n",
        "\n",
        "    save_path : str\n",
        "        Path to save the best model.\n",
        "\n",
        "    hyperparameter_grid : dict\n",
        "        Dictionary where keys are hyperparameter names and values are lists of possible values.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    d_test : pd.DataFrame\n",
        "        Test dataset with predictions and probabilities.\n",
        "\n",
        "    d_tuned_performance : pd.DataFrame\n",
        "        DataFrame with the performance metrics for each hyperparameter configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    # check CUDA\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    print(\"CUDA: \", use_cuda)\n",
        "\n",
        "    # set seed\n",
        "\n",
        "    set_seed(56)\n",
        "\n",
        "    print(\"======================================================================================\")\n",
        "    print(f\"Optimizing: {pretrained_model_name}\\nTarget: {target}\")\n",
        "    print(\"======================================================================================\")\n",
        "\n",
        "    # tokenize train and test sets\n",
        "\n",
        "    encoded_train = tokenizer(\n",
        "                              d_train['text'].tolist(),\n",
        "                              padding = True,\n",
        "                              truncation = True,\n",
        "                              return_tensors = 'pt',\n",
        "                              )\n",
        "\n",
        "    encoded_test = tokenizer(\n",
        "                             d_test['text'].tolist(),\n",
        "                             padding = True,\n",
        "                             truncation = True,\n",
        "                             return_tensors = 'pt',\n",
        "                             )\n",
        "\n",
        "\n",
        "    # accept dynamic target variables\n",
        "\n",
        "    train_labels = torch.tensor(d_train[target].values)\n",
        "    test_labels = torch.tensor(d_test[target].values)\n",
        "\n",
        "    # prepare datasets\n",
        "\n",
        "    train_dataset = TensorDataset(\n",
        "                                  encoded_train['input_ids'],\n",
        "                                  encoded_train['attention_mask'],\n",
        "                                  train_labels,\n",
        "                                  )\n",
        "\n",
        "    test_dataset = TensorDataset(\n",
        "                                 encoded_test['input_ids'],\n",
        "                                 encoded_test['attention_mask'],\n",
        "                                 test_labels,\n",
        "                                 )\n",
        "\n",
        "    #train_loader = DataLoader(\n",
        "    #                          train_dataset,\n",
        "    #                          batch_size = 8,  ### to be updated within grid search\n",
        "    #                          shuffle = True,\n",
        "    #                          )\n",
        "\n",
        "    #test_loader = DataLoader(\n",
        "    #                         test_dataset,\n",
        "    #                         batch_size = 8,  ### to be updated within grid search\n",
        "    #                         shuffle = False,\n",
        "    #                         )\n",
        "\n",
        "    # initialize class weights\n",
        "\n",
        "    if use_cuda:\n",
        "        class_weights = class_weights.cuda()\n",
        "\n",
        "    # initialize tracking variables\n",
        "\n",
        "    best_f1_macro = 0\n",
        "    best_params = None\n",
        "    best_model_state = None\n",
        "    best_predictions = []\n",
        "    best_probabilities = []\n",
        "\n",
        "    f1_scores = []\n",
        "    performance_data = []\n",
        "\n",
        "    # hyperparam grid search: ParameterGrid\n",
        "\n",
        "    for hyperparams in ParameterGrid(hyperparameter_grid):\n",
        "        print(f\"\\nOptimizing with hyperparameters: {hyperparams}\")\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "                                  train_dataset,\n",
        "                                  batch_size = hyperparams['batch_size'],\n",
        "                                  shuffle = True\n",
        "                                  )\n",
        "        test_loader = DataLoader(\n",
        "                                 test_dataset,\n",
        "                                 batch_size = hyperparams['batch_size'],\n",
        "                                 shuffle = False\n",
        "                                 )\n",
        "\n",
        "        print(f\"\\nTotal training rows: {len(train_dataset)}\")\n",
        "        print(f\"Total evaluation rows: {len(test_dataset)}\")\n",
        "        print(f\"Training batch size: {hyperparams['batch_size']}\")\n",
        "        print(f\"Evaluation batch size: {hyperparams['batch_size']}\")\n",
        "        print(f\"Total training batches: {len(train_loader)}\")\n",
        "        print(f\"Total evaluation batches: {len(test_loader)}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # initialize model\n",
        "\n",
        "        model = model_class.from_pretrained(pretrained_model_name)\n",
        "        if use_cuda:\n",
        "            model.cuda()\n",
        "\n",
        "        # initialize optimizer and lr scheduler\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "                                      model.parameters(),\n",
        "                                      lr = hyperparams['learning_rate'],\n",
        "                                      weight_decay = hyperparams['weight_decay']\n",
        "                                      )\n",
        "\n",
        "        # calculate total steps\n",
        "\n",
        "        total_steps = len(train_loader) * hyperparams['num_epochs']\n",
        "\n",
        "        # add scheduler with warmup steps\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "                                                    optimizer,\n",
        "                                                    num_warmup_steps = hyperparams['warmup_steps'],\n",
        "                                                    num_training_steps=total_steps\n",
        "                                                    )\n",
        "\n",
        "        criterion = CrossEntropyLoss(weight = class_weights)\n",
        "\n",
        "        # training loop\n",
        "\n",
        "        for epoch in range(hyperparams['num_epochs']):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            for i, batch in enumerate(tqdm(train_loader, desc = f\"Training Epoch {epoch + 1}/{hyperparams['num_epochs']}\", leave=True)):\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                if use_cuda:\n",
        "                    input_ids, attention_mask, labels = input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "                loss = loss / hyperparams['gradient_accumulation_steps']\n",
        "                loss.backward()\n",
        "                if (i + 1) % hyperparams['gradient_accumulation_steps'] == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()  ### update learning rate\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "        # eval loop\n",
        "\n",
        "        model.eval()\n",
        "        all_predictions = []\n",
        "        all_true_labels = []\n",
        "        all_probabilities = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                if use_cuda:\n",
        "                    input_ids, attention_mask, labels = input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                probabilities = torch.softmax(outputs.logits, dim = 1)\n",
        "                predictions = torch.argmax(probabilities, dim = 1).cpu().tolist()\n",
        "                all_predictions.extend(predictions)\n",
        "                all_true_labels.extend(labels.cpu().tolist())\n",
        "                all_probabilities.extend(probabilities.cpu().tolist())\n",
        "\n",
        "        # calculate F1 (macro)\n",
        "\n",
        "        current_f1_macro = f1_score(all_true_labels, all_predictions, average='macro')\n",
        "        f1_scores.append(current_f1_macro)\n",
        "        print(f\"\\nCurrent F1 macro with params {hyperparams}: {current_f1_macro}\")\n",
        "\n",
        "        # append F1 and current performance data\n",
        "\n",
        "        performance_data.append({\n",
        "                                 'pretrained_model_name': pretrained_model_name,\n",
        "                                 'target': target,\n",
        "                                 'f1_score': current_f1_macro,\n",
        "                                 'batch_size': hyperparams['batch_size'],\n",
        "                                 'weight_decay': hyperparams['weight_decay'],\n",
        "                                 'learning_rate': hyperparams['learning_rate'],\n",
        "                                 'warmup_steps': hyperparams['warmup_steps'],\n",
        "                                 'num_epochs': hyperparams['num_epochs'],\n",
        "                                 'gradient_accumulation_steps': hyperparams['gradient_accumulation_steps'],\n",
        "        })\n",
        "\n",
        "        if current_f1_macro > best_f1_macro:\n",
        "            best_f1_macro = current_f1_macro\n",
        "            best_params = hyperparams\n",
        "            best_model_state = model.state_dict()\n",
        "            best_predictions = all_predictions\n",
        "            best_probabilities = all_probabilities\n",
        "\n",
        "    #if len(best_predictions) == len(d_test):\n",
        "    #    d_test['predicted_labels'] = best_predictions\n",
        "    #    d_test['predicted_probabilities'] = best_probabilities\n",
        "    #else:\n",
        "    #    print(\"Error: Length of predictions does not match length of test set\")\n",
        "\n",
        "    d_test['predicted_labels'] = best_predictions\n",
        "    d_test['predicted_probabilities'] = best_probabilities\n",
        "\n",
        "    # save d_test_{target} with pred and prob\n",
        "\n",
        "    print(\"--------------------------------------------------------------------------------------\")\n",
        "    print(f\"Summary: {target}\")\n",
        "    print(\"--------------------------------------------------------------------------------------\")\n",
        "\n",
        "    print(d_test.head(6))\n",
        "    d_test.to_excel(f'{save_path}/d_test_tuned_preds_{target}.xlsx')\n",
        "\n",
        "    if best_model_state:\n",
        "        model_path = f\"{save_path}/{target}_{pretrained_model_name}_best_tuned_model.bin\"\n",
        "        torch.save(best_model_state, model_path)\n",
        "        print(\"\\nBest model saved with F1 macro:\", best_f1_macro)\n",
        "        print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "    # display F1 scores\n",
        "\n",
        "    f1_mean = sum(f1_scores) / len(f1_scores)\n",
        "    f1_std = (sum((x - f1_mean) ** 2 for x in f1_scores) / len(f1_scores)) ** 0.5\n",
        "    print(f\"Mean F1 macro: {f1_mean}\")\n",
        "    print(f\"Standard deviation of F1 macro: {f1_std}\")\n",
        "\n",
        "    # df: target-wise\n",
        "\n",
        "    d_tuned_performance = pd.DataFrame(performance_data)\n",
        "    print(d_tuned_performance.head(10))\n",
        "\n",
        "    # save: target-wise df\n",
        "\n",
        "    d_tuned_performance.to_excel(f'{save_path}/d_tuned_performance_{target}.xlsx')\n",
        "\n",
        "    return d_test, d_tuned_performance\n"
      ],
      "metadata": {
        "id": "xJGLu_wiPUXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### redact.py"
      ],
      "metadata": {
        "id": "PCDusDJoc_3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_ner_redact_post_texts_**"
      ],
      "metadata": {
        "id": "dm-O7GK7dCZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile redact.py\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "def ner_redact_post_texts(p_text):\n",
        "    \"\"\"\n",
        "    Redacts all named entities recognized by spaCy EntityRecognizer, replaces with <|PII|> pseudo-word token.\n",
        "    \"\"\"\n",
        "    ne = list(\n",
        "              [\n",
        "               'PERSON',   ### people, including fictional\n",
        "               'NORP',     ### nationalities or religious or political groups\n",
        "               'FAC',      ### buildings, airports, highways, bridges, etc.\n",
        "               'ORG',      ### companies, agencies, institutions, etc.\n",
        "               #'GPE',     ### countries, cities, states\n",
        "               'LOC',      ### non-GPE locations, mountain ranges, bodies of water\n",
        "               'PRODUCT',  ### objects, vehicles, foods, etc. (not services)\n",
        "               'EVENT',    ### named hurricanes, battles, wars, sports events, etc.\n",
        "               ]\n",
        "                )\n",
        "\n",
        "    doc = nlp(p_text)\n",
        "    ne_to_remove = []\n",
        "    final_string = str(p_text)\n",
        "    for sent in doc.ents:\n",
        "        if sent.label_ in ne:\n",
        "            ne_to_remove.append(str(sent.text))\n",
        "    for n in range(len(ne_to_remove)):\n",
        "        final_string = final_string.replace(\n",
        "                                            ne_to_remove[n],\n",
        "                                            '<|PII|>',\n",
        "                                            )\n",
        "    return final_string"
      ],
      "metadata": {
        "id": "tgQeh7_Jb__2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### predict.py"
      ],
      "metadata": {
        "id": "DYuotbl5dsV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_load_model_**"
      ],
      "metadata": {
        "id": "um7zclzxdQtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile a predict.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import (\n",
        "                          DistilBertTokenizer,\n",
        "                          DistilBertForSequenceClassification,\n",
        "                          BertTokenizer,\n",
        "                          BertForSequenceClassification,\n",
        "                          RobertaTokenizer,\n",
        "                          RobertaForSequenceClassification,\n",
        "                          )\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def load_model(model_path, model_class, pretrained_model_name):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained fine-tined LM from a specified path.\n",
        "    \"\"\"\n",
        "    model = model_class.from_pretrained(pretrained_model_name)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "D0X75L47cADX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_preprocess_data_**"
      ],
      "metadata": {
        "id": "oPOjsBILdXMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a predict.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import (\n",
        "                          DistilBertTokenizer,\n",
        "                          DistilBertForSequenceClassification,\n",
        "                          BertTokenizer,\n",
        "                          BertForSequenceClassification,\n",
        "                          RobertaTokenizer,\n",
        "                          RobertaForSequenceClassification,\n",
        "                          )\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_data(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Tokenizes a list of texts using the specified LM-specific tokenizer.\n",
        "    \"\"\"\n",
        "    encoded_texts = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return encoded_texts"
      ],
      "metadata": {
        "id": "NfE5xRrVcAHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_predict_**"
      ],
      "metadata": {
        "id": "mcu7TSlBdexo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a predict.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import (\n",
        "                          DistilBertTokenizer,\n",
        "                          DistilBertForSequenceClassification,\n",
        "                          BertTokenizer,\n",
        "                          BertForSequenceClassification,\n",
        "                          RobertaTokenizer,\n",
        "                          RobertaForSequenceClassification,\n",
        "                          )\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def predict(model, tokenizer, texts, batch_size = 8, use_cuda = True):\n",
        "    \"\"\"\n",
        "    Predicts labels and probabilities for a list of texts using the specified model and tokenizer.\n",
        "    \"\"\"\n",
        "    print(f\"Total number of texts to predict: {len(texts)}\")\n",
        "    encoded_texts = preprocess_data(tokenizer, texts)\n",
        "    dataset = TensorDataset(encoded_texts['input_ids'], encoded_texts['attention_mask'])\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Total number of batches: {len(data_loader)}\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(total=len(data_loader), desc=\"Predicting\", leave=False)\n",
        "        for batch in data_loader:\n",
        "            input_ids, attention_mask = batch\n",
        "            if use_cuda:\n",
        "                input_ids, attention_mask = input_ids.cuda(), attention_mask.cuda()\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "            predictions = torch.argmax(probabilities, dim=1).cpu().tolist()\n",
        "            all_predictions.extend(predictions)\n",
        "            all_probabilities.extend(probabilities.cpu().tolist())\n",
        "            progress_bar.update(1)\n",
        "        progress_bar.close()\n",
        "\n",
        "    return all_predictions, all_probabilities"
      ],
      "metadata": {
        "id": "8wKygXEFcALG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import"
      ],
      "metadata": {
        "id": "hr3Dsh6zd1K6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocess import (\n",
        "                        augment_training_data_with_rationales,\n",
        "                        dummy_code_augmented_rows,\n",
        "                        read_and_append_jsonl_archives,\n",
        "                        )\n",
        "\n",
        "#from redact import (\n",
        "#                    ner_redact_post_texts,\n",
        "#                    )\n",
        "\n",
        "#from predict import (\n",
        "#                     load_model,\n",
        "#                     preprocess_data,\n",
        "#                     predict,\n",
        "#                     )\n",
        "\n",
        "from train import (\n",
        "                   set_seed,\n",
        "                   train_eval_save_bl_models,\n",
        "                   performance_barplot,\n",
        "                   performance_scatterplot,\n",
        "                   iterative_stratified_train_test_split_with_rationales,\n",
        "                   tune_and_optimize_model_hyperparams,\n",
        "                   )"
      ],
      "metadata": {
        "id": "_QcKdI-mcASI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Preprocess\n",
        "Takes $\\mathcal{d}$<sub>annotated</sub>, builds independent $\\mathcal{d}$<sub>calibrate</sub> and rationale-augmented $\\mathcal{d}$<sub>augmented</sub> to train. Builds $\\mathcal{V}$ corpus, $\\mathcal{d}$<sub>adapt</sub> for domain adaptation. Merges, NER-anonymizes Aim II analytic sample $\\mathcal{D}$<sub>inference</sub>.\n",
        "***"
      ],
      "metadata": {
        "id": "woZYUo6JgN1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge Wave 1 (purposive) and Wave 2 (random)"
      ],
      "metadata": {
        "id": "ThJ315SGh0fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../inputs/data"
      ],
      "metadata": {
        "id": "7fTcqzw9v2db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_1 = pd.read_excel('d_cycle999_prp_ss_single.xlsx')\n",
        "d_2 = pd.read_excel('d_cycle999_rnd_ss_single.xlsx')\n",
        "\n",
        "d = pd.concat([\n",
        "               d_1,\n",
        "               d_1,\n",
        "               ])\n",
        "\n",
        "d = d.drop(\n",
        "           'Unnamed: 0',\n",
        "           axis = 1,\n",
        "           )\n",
        "\n",
        "d = d[d.text != ' ']\n",
        "\n",
        "d.replace(\n",
        "          ' ',\n",
        "          0,\n",
        "          inplace = True,\n",
        "          )\n",
        "\n",
        "ints = [\n",
        "        'asp',\n",
        "        'dep',\n",
        "        'val',\n",
        "        'prg',\n",
        "        'tgd',\n",
        "        'age',\n",
        "        'race',\n",
        "        'dbty',\n",
        "        ]\n",
        "\n",
        "for i in ints:\n",
        "    d[i] = pd.to_numeric(d[i], errors = 'coerce')\n",
        "    d[i] = d[i].fillna(0).astype('int64')\n",
        "\n",
        "d_annotated = shuffle(\n",
        "                      d,\n",
        "                      random_state = 56,\n",
        "                      )\n",
        "\n",
        "d_annotated.reset_index(\n",
        "                        drop = True,\n",
        "                        inplace = True,\n",
        "                        )\n",
        "\n",
        "sbrt_cnt = d_annotated['sbrt'].value_counts()\n",
        "sbrt_pct = d_annotated['sbrt'].value_counts(normalize = True) * 100\n",
        "\n",
        "sbrts = pd.DataFrame({\n",
        "                      'Count': sbrt_cnt,\n",
        "                      'Percentage': sbrt_pct,\n",
        "                      })\n",
        "\n",
        "print(sbrts)\n",
        "\n",
        "d_annotated[[\n",
        "             'asp',\n",
        "             'dep',\n",
        "             'val',\n",
        "             'prg',\n",
        "             'tgd',\n",
        "             'age',\n",
        "             'race',\n",
        "             'dbty',\n",
        "              ]].apply(pd.Series.value_counts)\n",
        "\n",
        "#d.dtypes\n",
        "d_annotated.info()\n",
        "d_annotated.head(3)"
      ],
      "metadata": {
        "id": "1WEmQf_vgM1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathcal{d}$<sub>calibrate</sub> ($n$<sub>posts</sub> = 400)"
      ],
      "metadata": {
        "id": "1IPem2mmjf4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_calibrate = d_annotated.iloc[:400]\n",
        "\n",
        "d_calibrate.shape\n",
        "d_calibrate.head(3)\n",
        "\n",
        "d_calibrate.to_excel('d_calibrate.xlsx')"
      ],
      "metadata": {
        "id": "ZKu0OIDVgM5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathcal{d}$<sub>augmented</sub> (unique $n$<sub>posts</sub> = 2,000)"
      ],
      "metadata": {
        "id": "vyBe4kf8j9i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = d_annotated.iloc[400:]\n",
        "\n",
        "d.reset_index(\n",
        "              drop = True,\n",
        "              inplace = True,\n",
        "              )\n",
        "\n",
        "# 'strn' = any pos_instance of strain\n",
        "\n",
        "d['strn'] = (d['asp'] == 1) | (d['dep'] == 1) | (d['val'] == 1)\n",
        "d['strn'] = d['strn'].astype(int)\n",
        "\n",
        "# append rationales\n",
        "\n",
        "rationales = [\n",
        "              'asp_rtnl',\n",
        "              'dep_rtnl',\n",
        "              'val_rtnl',\n",
        "               ]\n",
        "\n",
        "for r in rationales:\n",
        "    d[r] = d[r].astype(str)\n",
        "    d[r] = d[r].str.replace(\n",
        "                            r'0',\n",
        "                            '.',\n",
        "                            regex = True,\n",
        "                            )\n",
        "\n",
        "d['rtnl'] = d['asp_rtnl'] + ' ' + d['dep_rtnl'] + ' ' + d['val_rtnl']\n",
        "\n",
        "d['rtnl'] = d['rtnl'].str.replace(\n",
        "                                  r'. . .',\n",
        "                                  '.',\n",
        "                                  regex = False,\n",
        "                                  )\n",
        "\n",
        "print(\"pre-augmentation\")\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "d.shape\n",
        "d.head(3)"
      ],
      "metadata": {
        "id": "JYA6jiMNgNAj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_augmented = augment_training_data_with_rationales(d)\n",
        "\n",
        "d_augmented.reset_index(\n",
        "                        drop = True,\n",
        "                        inplace = True,\n",
        "                        )\n",
        "\n",
        "d_augmented = dummy_code_augmented_rows(d_augmented)\n",
        "\n",
        "print(\"post-augmentation\")\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "\n",
        "d_augmented[[\n",
        "             'asp',\n",
        "             'dep',\n",
        "             'val',\n",
        "             'prg',\n",
        "             'tgd',\n",
        "             'age',\n",
        "             'race',\n",
        "             'dbty',\n",
        "             'aug'\n",
        "              ]].apply(pd.Series.value_counts)\n",
        "\n",
        "d_augmented.shape\n",
        "d_augmented.head(30)\n",
        "\n",
        "d_augmented.to_excel('d_augmented.xlsx')"
      ],
      "metadata": {
        "id": "0TwYX6CCgNEb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_augmented = pd.read_excel(\n",
        "                            'd_augmented.xlsx',\n",
        "                            index_col = [0],\n",
        "                            )\n",
        "\n",
        "d_augmented[[\n",
        "             'asp',\n",
        "             'dep',\n",
        "             'val',\n",
        "             'prg',\n",
        "             'tgd',\n",
        "             'age',\n",
        "             'race',\n",
        "             'dbty',\n",
        "             'aug'\n",
        "              ]].apply(pd.Series.value_counts)"
      ],
      "metadata": {
        "id": "NFEWozlei13s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Append $\\mathcal{V}$ corpus, derive $\\mathcal{D}$<sub>inference</sub>"
      ],
      "metadata": {
        "id": "HMuLvUHmksoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "archives_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/inputs/archives/'\n",
        "#archives_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/inputs/archives/TEST'\n",
        "d_p, d_c = read_and_append_jsonl_archives(archives_path)\n",
        "\n",
        "    ### SJS 9/11: impossible RAM overhead using full corpus; solution TKTK\n",
        "\n",
        "d_p.shape\n",
        "d_p.head(3)\n",
        "\n",
        "d_c.shape\n",
        "d_c.head(3)"
      ],
      "metadata": {
        "id": "kD5s5ruNvy95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################\n",
        "############################################################\n",
        "############################################################ PRELIM (START)\n",
        "\n",
        "%cd inputs/data"
      ],
      "metadata": {
        "id": "-0I_VKzyaLJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reset_selective -f d_"
      ],
      "metadata": {
        "id": "wmAM9SkhZ1ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d_adapt 9/12 - r/SW, r/trans, r/TwoX 2022 posts only\n",
        "\n",
        "    ### SJS 9/12: importing one by one to mitigate RAM overhead - long-term solution TKTK\n",
        "\n",
        "archives_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/inputs/archives/'\n",
        "list_p = []\n",
        "\n",
        "p_path = f'{archives_path}r_TwoXChromosomes_posts.jsonl'\n",
        "\n",
        "d_tx_p = pd.read_json(\n",
        "                      p_path,\n",
        "                      lines = True,\n",
        "                     )\n",
        "\n",
        "# inspect\n",
        "\n",
        "d_tx_p.info()\n",
        "d_tx_p.head(3)\n",
        "\n",
        "# save\n",
        "\n",
        "d_tx_p.to_csv(\n",
        "              'd_tx_p.csv',\n",
        "              encoding = 'utf-8',\n",
        "              index = False,\n",
        "              header = True,\n",
        "              )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C6RDKV3-w5UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_tx_p = pd.read_csv('d_tx_p.csv')\n",
        "\n",
        "d_tx_p = d_tx_p.drop_duplicates(subset = 'id')\n",
        "\n",
        "# Check for non-numeric values in 'created_utc' column\n",
        "#non_numeric = d_sw_p[pd.to_numeric(d_sw_p['created_utc'], errors='coerce').isnull()]\n",
        "\n",
        "# Investigate or handle non-numeric values before converting to datetime\n",
        "#if len(non_numeric) > 0:\n",
        "#  print(f\"Found {len(non_numeric)} non-numeric values in 'created_utc':\")\n",
        "#  print(non_numeric['created_utc'])\n",
        "  # Consider removing or correcting these values before proceeding\n",
        "\n",
        "#d_sw_p = d_sw_p.drop(45358)\n",
        "\n",
        "d_tx_p['date'] = pd.to_datetime(\n",
        "                             d_tx_p.created_utc,\n",
        "                             unit = 's',\n",
        "                             )\n",
        "\n",
        "d_tx_p.set_index(\n",
        "              'date',\n",
        "              drop = False,\n",
        "              inplace = True,\n",
        "              )\n",
        "\n",
        "d_tx_p = d_tx_p.loc[(d_tx_p['date'] >= '2022-01-01') & (d_tx_p['date'] < '2022-12-31')] ### 2022 for d_adapt\n",
        "\n",
        "d_tx_p = d_tx_p[~d_tx_p['selftext'].isin(['[deleted]', '[removed]'])]\n",
        "\n",
        "# housekeeping\n",
        "\n",
        "d_tx_p = d_tx_p[[\n",
        "           'author',\n",
        "           'created_utc',\n",
        "           'date',\n",
        "           'id',\n",
        "           'num_comments',\n",
        "           'selftext',\n",
        "           'subreddit',\n",
        "           'title',\n",
        "           ]].copy()\n",
        "\n",
        "d_tx_p.rename(\n",
        "           columns = {\n",
        "                      'author': 'p_au',\n",
        "                      'created_utc': 'p_utc',\n",
        "                      'date': 'p_date',\n",
        "                      'num_comments': 'n_cmnt',\n",
        "                      'selftext': 'text',\n",
        "                      'subreddit': 'p_sbrt',\n",
        "                      'title': 'p_titl',\n",
        "                      }, inplace = True,\n",
        "            )\n",
        "\n",
        "d_tx_p.shape\n",
        "d_tx_p.head(3)\n",
        "\n",
        "d_tx_p.to_csv(\n",
        "              'd_tx_p.csv',\n",
        "              encoding = 'utf-8',\n",
        "              index = False,\n",
        "              header = True,\n",
        "              )\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iZaR5j2maMjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_sw_p.shape\n",
        "d_sw_p.head(3)\n",
        "\n",
        "d_tr_p.shape\n",
        "d_tr_p.head(3)\n",
        "\n",
        "d_tx_p.shape\n",
        "d_tx_p.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gSciYfnEaMqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_adapt = pd.concat([\n",
        "                     d_sw_p,\n",
        "                     d_tr_p,\n",
        "                     d_tx_p,\n",
        "                     ])\n",
        "\n",
        "d_adapt = shuffle(d_adapt)\n",
        "\n",
        "d_adapt.reset_index(\n",
        "                    drop = True,\n",
        "                    inplace = True,\n",
        "                    )\n",
        "\n",
        "d_adapt.shape\n",
        "d_adapt.head(3)\n",
        "\n",
        "d_adapt.to_csv(\n",
        "              'd_adapt.csv',\n",
        "              encoding = 'utf-8',\n",
        "              index = False,\n",
        "              header = True,\n",
        "              )\n",
        "\n",
        "############################################################ PRELIM (END)\n",
        "############################################################\n",
        "############################################################"
      ],
      "metadata": {
        "id": "8SWYQ49Dfihe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean, condense: posts**"
      ],
      "metadata": {
        "id": "AXDTdWwM10Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d_p = posts\n",
        "\n",
        "d_p = d_p.drop_duplicates(subset = 'id')\n",
        "\n",
        "d_p['date'] = pd.to_datetime(\n",
        "                             d_p.created_utc,\n",
        "                             unit = 's',\n",
        "                             )\n",
        "\n",
        "d_p.set_index(\n",
        "              'date',\n",
        "              drop = False,\n",
        "              inplace = True,\n",
        "              )\n",
        "\n",
        "d_p = d_p.loc[(d_p['date'] >= '2020-12-02') & (d_p['date'] < '2024-06-24')] ### yyyy-mm-dd = Dec 2, 2020 - Jun 24, 2024\n",
        "\n",
        "d_p = d_p[~d_p['selftext'].isin(['[deleted]', '[removed]'])]\n",
        "\n",
        "# housekeeping\n",
        "\n",
        "d_p = d_p[[\n",
        "           'author',\n",
        "           'created_utc',\n",
        "           'date',\n",
        "           'id',\n",
        "           'num_comments',\n",
        "           'selftext',\n",
        "           'subreddit',\n",
        "           'title',\n",
        "           ]].copy()\n",
        "\n",
        "d_p.rename(\n",
        "           columns = {\n",
        "                      'author': 'p_au',\n",
        "                      'created_utc': 'p_utc',\n",
        "                      'date': 'p_date',\n",
        "                      'num_comments': 'n_cmnt',\n",
        "                      'selftext': 'text',\n",
        "                      'subreddit': 'p_sbrt',\n",
        "                      'title': 'p_titl',\n",
        "                      }, inplace = True,\n",
        "            )"
      ],
      "metadata": {
        "id": "JicbL1zaxl7G",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NER anonymize: posts**"
      ],
      "metadata": {
        "id": "JOsK4F2R7ZYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_p['text'] = d_p['text'].astype(str).apply(lambda i: ner_redact_post_texts(i))"
      ],
      "metadata": {
        "id": "zjSPJExg7YUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean, condense: comments**"
      ],
      "metadata": {
        "id": "A4vlHfGW19v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d_c = comments\n",
        "\n",
        "d_c = d_c.drop_duplicates(subset = 'id')\n",
        "\n",
        "d_c['date'] = pd.to_datetime(\n",
        "                             d_c.created_utc,\n",
        "                             unit = 's',\n",
        "                             )\n",
        "\n",
        "d_c.set_index(\n",
        "              'date',\n",
        "              drop = False,\n",
        "              inplace = True,\n",
        "              )\n",
        "\n",
        "d_c = d_c.loc[(d_c['date'] >= '2020-12-02') & (d_c['date'] < '2024-06-24')] ### yyyy-mm-dd = Dec 2, 2020 - Jun 24, 2024\n",
        "\n",
        "d_c = d_c[~d_c['body'].isin(['[deleted]', '[removed]'])]\n",
        "\n",
        "# housekeeping\n",
        "\n",
        "d_c = d_c[[\n",
        "           'author',\n",
        "           'body',\n",
        "           'date',\n",
        "           'link_id',\n",
        "           'subreddit',\n",
        "           ]].copy()\n",
        "\n",
        "d_c.rename(\n",
        "           columns = {\n",
        "                      'author': 'c_au',\n",
        "                      'body': 'c_text',\n",
        "                      'date': 'c_date',\n",
        "                      'link_id': 'id',\n",
        "                      'subreddit': 'c_sbrt',\n",
        "                      }, inplace = True,\n",
        "            )\n",
        "\n",
        "# delete comment-level 'id' prefix for merge\n",
        "\n",
        "d_c['id'] = d_c['id'].str.replace('t3_', ' ')"
      ],
      "metadata": {
        "id": "9PrCoKyf19PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"V-corpus posts/d_adapt\")\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "d_p.shape\n",
        "d_p.head(3)\n",
        "d_p.tail(3)\n",
        "\n",
        "print(\"V-corpus comments\")\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "d_c.shape\n",
        "d_c.head(3)\n",
        "d_c.tail(3)"
      ],
      "metadata": {
        "id": "4P4N-5Cc2S5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for post-labeling merge\n",
        "\n",
        "d_c.to_csv(\n",
        "           'd_comments.csv',\n",
        "           encoding = 'utf-8',\n",
        "           index = False,\n",
        "           header = True,\n",
        "           )"
      ],
      "metadata": {
        "id": "2vM8HGGDvyuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathcal{d}$<sub>adapt</sub>: domain adaptation set"
      ],
      "metadata": {
        "id": "2_uqv-aHsDGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_adapt = d_p.copy()\n",
        "\n",
        "d_adapt.to_csv(\n",
        "               'd_adapt_TEST.csv',\n",
        "               encoding = 'utf-8',\n",
        "               index = False,\n",
        "               header = True,\n",
        "               )"
      ],
      "metadata": {
        "id": "UeNXevpnD7AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathcal{D}$<sub>inference</sub>: prediction set"
      ],
      "metadata": {
        "id": "zLgzHX1Tts0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_inference = d_p.sample(\n",
        "                         n = 1000000, ### TKTK - maybe\n",
        "                         random_state = 56,\n",
        "                         )"
      ],
      "metadata": {
        "id": "2paLevfVtsLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPE: encoding, extraction**"
      ],
      "metadata": {
        "id": "Z7iSW5X3q5tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# extract, count GPEs\n",
        "\n",
        "def extract_gpe(text):\n",
        "    doc = nlp(text)\n",
        "    gpes = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
        "    return gpes, len(gpes)\n",
        "\n",
        "d_inference[[\n",
        "             'gpe',\n",
        "             'gpe_count',\n",
        "             ]] = d_inference['text'].apply(lambda i: pd.Series(extract_gpe(i)))\n",
        "\n",
        "total_gpe_count = d_inference['gpe_count'].sum()\n",
        "\n",
        "print(f\"Total number of GPEs recognized: {total_gpe_count}\")"
      ],
      "metadata": {
        "id": "M9oLft4QqvE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    ### SJS 8/17: validation w/ GTP-4o TKTK: is each GPE in the US?"
      ],
      "metadata": {
        "id": "vgUzETyeygmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPE: concordance**"
      ],
      "metadata": {
        "id": "rqhqDda4wXa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join single string for nltk entry\n",
        "\n",
        "all_texts = ' '.join(d_inference['text'].tolist())\n",
        "\n",
        "# tokenize for nltk\n",
        "\n",
        "tokens = nltk.word_tokenize(all_texts)\n",
        "nltk_text = Text(tokens)\n",
        "\n",
        "# concordance: GPEs in context\n",
        "\n",
        "for gpes in d_inference['gpe']:\n",
        "    for gpe in gpes:  # gpes = list of GPEs\n",
        "        print(f\"\\nConcordance for '{gpe}':\")\n",
        "        nltk_text.concordance(gpe)"
      ],
      "metadata": {
        "id": "SQ6rDGwvwWBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicit suicidality: encoding**"
      ],
      "metadata": {
        "id": "i07lgnrTrBny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex = r'\\bsuicid\\S*'\n",
        "\n",
        "d_inference['sui'] = d_inference['text'].str.contains(\n",
        "                                                      regex,\n",
        "                                                      regex = True,\n",
        "                                                      ) | (d_inference['p_sbrt'] == 'SuicideWatch')\n",
        "\n",
        "\n",
        "d_inference['sui'] = d_inference['sui'].astype(int)\n",
        "\n",
        "d_inference.head(3)"
      ],
      "metadata": {
        "id": "gwyFFLqyqvLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    ### SJS 8/17: validation w/ GTP-4o TKTK: is sui in reference to other people?\n",
        "\n",
        "#%pwd\n",
        "%cd ../inputs/data"
      ],
      "metadata": {
        "id": "ClXtlLxzyqci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_inference.to_csv(\n",
        "                   'd_inference_TEST.csv',\n",
        "                   encoding = 'utf-8',\n",
        "                   index = False,\n",
        "                   header = True,\n",
        "                   )"
      ],
      "metadata": {
        "id": "MqAUEz2rqvXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Train-Test\n",
        "Trains baseline BERT, RoBERTa, and DistilBERT, using rationale-augmented data $\\mathcal{d}$<sub>augmented</sub>, iterating over a.) strains, b.) explicit targeting, c.) implicit vulnerabilities. Evaluates using de-augmented data. Outputs model x target _$F$_<sub>1</sub> (macro) performance scores.\n",
        "***"
      ],
      "metadata": {
        "id": "rxzedr7a-kuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%pwd\n",
        "%cd ../inputs/data"
      ],
      "metadata": {
        "id": "8Vh-AsKSAFuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_augmented = pd.read_excel(\n",
        "                            'd_augmented.xlsx',\n",
        "                            index_col = [0],\n",
        "                            )\n",
        "\n",
        "d_augmented.info()\n",
        "d_augmented.head(3)"
      ],
      "metadata": {
        "id": "vVEyG5UoH-uN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Condense for model entry**"
      ],
      "metadata": {
        "id": "Dkd3V_SzAZQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets = [\n",
        "           'asp',\n",
        "#           'dep',\n",
        "#           'val',\n",
        "#           'prg',\n",
        "#           'tgd',\n",
        "#           'age',\n",
        "#           'race',\n",
        "#           'dbty',\n",
        "           ]\n",
        "\n",
        "d_augmented = d_augmented[\n",
        "                          ['text',\n",
        "                           'aug'] +\n",
        "                           targets\n",
        "                           ].copy()\n",
        "\n",
        "d_augmented[\n",
        "            ['aug'] +\n",
        "             targets\n",
        "             ].apply(pd.Series.value_counts)"
      ],
      "metadata": {
        "id": "HTNkgDwhAO_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute weights ($w$): inverse class ($c$) freq<br>\n",
        "$w_c = N / (2 * n_c)$**"
      ],
      "metadata": {
        "id": "ZCRDXGEICOL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = {}\n",
        "\n",
        "for t in targets:\n",
        "\n",
        "    value_counts = d_augmented[t].value_counts()\n",
        "\n",
        "    w_pos = round(len(d_augmented) / (2 * value_counts.get(1, 0)), 4)\n",
        "    w_neg = round(len(d_augmented) / (2 * value_counts.get(0, 0)), 4)\n",
        "\n",
        "    class_weights[t] = {\n",
        "                        'w_pos': w_pos if w_pos != float('inf') else 0,\n",
        "                        'w_neg': w_neg if w_neg != float('inf') else 0,\n",
        "                        }\n",
        "\n",
        "class_weights"
      ],
      "metadata": {
        "id": "loj3xA_kCNdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate benchmark models: $k$-fold cross validate"
      ],
      "metadata": {
        "id": "I8K7x-loGZNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/temp\n",
        "\n",
        "# define target-specific aug-stratified df\n",
        "\n",
        "target_datasets = iterative_stratified_train_test_split_with_rationales(\n",
        "                                                                        d_augmented,\n",
        "                                                                        targets,\n",
        "                                                                        random_state = 56,\n",
        "                                                                        test_size = 0.2,\n",
        "                                                                        )"
      ],
      "metadata": {
        "id": "IvjM9N1evElY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_datasets.keys())"
      ],
      "metadata": {
        "id": "G39CdbnVEHua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define target-specific df\n",
        "\n",
        "#target_datasets = {\n",
        "#                   'asp': (d_train_asp, d_test_asp),\n",
        "#                   'dep': (d_train_dep, d_test_dep),\n",
        "#                   'val': (d_train_val, d_test_val),\n",
        "#                   'prg': (d_train_prg, d_test_prg),\n",
        "#                   'tgd': (d_train_tgd, d_test_tgd),\n",
        "#                   'age': (d_train_age, d_test_age),\n",
        "#                   'race': (d_train_race, d_test_race),\n",
        "#                   'dbty': (d_train_dbty, d_test_dbty),\n",
        "\n",
        "#}\n",
        "\n",
        "# define targets + class weights\n",
        "\n",
        "targets_and_class_weights = {\n",
        "                             'asp': [\n",
        "                                     0.7657, ### w_neg\n",
        "                                     1.4410, ### w_pos\n",
        "                                     ],\n",
        "#                             'dep': [\n",
        "#                                     0.5989,\n",
        "#                                     3.0286,\n",
        "#                                     ],\n",
        "#                             'val': [\n",
        "#                                     0.6876,\n",
        "#                                     1.8327,\n",
        "#                                     ],\n",
        "#                             'prg': [\n",
        "#                                     0.6024,\n",
        "#                                     2.9414,\n",
        "#                                     ],\n",
        "#                            'tgd': [\n",
        "#                                     0.5974,\n",
        "#                                     3.0676,\n",
        "#                                     ],\n",
        "#                             'age': [\n",
        "#                                     0.6303,\n",
        "#                                     2.4188,\n",
        "#                                     ],\n",
        "#                             'race': [\n",
        "#                                      0.506,\n",
        "#                                      42.0441,\n",
        "#                                      ],\n",
        "#                             'dbty': [\n",
        "#                                      0.5064,\n",
        "#                                      39.7083,\n",
        "#                                      ],\n",
        "                              }"
      ],
      "metadata": {
        "id": "AR64py6weO2w",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT, RoBERTa, DistilBERT**"
      ],
      "metadata": {
        "id": "FVlcQJqfdG0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define models\n",
        "\n",
        "models = {\n",
        "          'bert': (\n",
        "                   BertForSequenceClassification,\n",
        "                   BertTokenizer,\n",
        "                  'bert-base-uncased',\n",
        "                   ),\n",
        "\n",
        "          'roberta': (\n",
        "                      RobertaForSequenceClassification,\n",
        "                      RobertaTokenizer,\n",
        "                      'roberta-base',\n",
        "                      ),\n",
        "\n",
        "#          'distilbert': (\n",
        "#                         DistilBertForSequenceClassification,\n",
        "#                         DistilBertTokenizer,\n",
        "#                         'distilbert-base-uncased',\n",
        "#                         ),\n",
        "        }\n",
        "\n",
        "save_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/models/'\n",
        "\n",
        "# define hyperparameter grid\n",
        "\n",
        "hyperparameter_grid = {\n",
        "                       'batch_size': 8,\n",
        "                       'gradient_accumulation_steps': 2,\n",
        "                       'learning_rate': 2e-5,\n",
        "                       'num_epochs': 2,\n",
        "                       'warmup_steps': 0,\n",
        "                       'weight_decay': 0.00,\n",
        "                       }\n",
        "\n",
        "# set cycle\n",
        "\n",
        "cycle = 'benchmark'"
      ],
      "metadata": {
        "id": "KC5Bv5XydDVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../../outputs/tables\n",
        "\n",
        "# 'benchmark' cycle: train-test loop\n",
        "\n",
        "train_eval_save_bl_models(\n",
        "                          target_datasets = target_datasets,\n",
        "                          targets_and_class_weights = targets_and_class_weights,\n",
        "                          models = models,\n",
        "                          save_path = save_path,\n",
        "                          cycle = cycle,\n",
        "                          hyperparameter_grid = hyperparameter_grid,\n",
        "                          )"
      ],
      "metadata": {
        "id": "-Og-lIzTygll",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Llama**"
      ],
      "metadata": {
        "id": "gJESi1LsBsSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "##############################################################################\n",
        "############################################################################## PRELIM\n",
        "\n",
        "#%pip install llama-stack"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LcPgCY9pg5EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token '<my_token>'"
      ],
      "metadata": {
        "id": "i6kiTerwg435"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################## PRELIM\n",
        "##############################################################################\n",
        "##############################################################################"
      ],
      "metadata": {
        "id": "RdJk6VB-8xV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benchmark: viz**"
      ],
      "metadata": {
        "id": "7piusGAtPZR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from matplotlib.font_manager import get_font_names\n",
        "#print(get_font_names())\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab/bar_policy_suicidality/temp"
      ],
      "metadata": {
        "id": "UDFAqUdVIMTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "d_v = pd.read_excel('d_benchmark_performance_SHAM.xlsx')\n",
        "d_v.round({'f1_macro': 4, 'mcc': 4, 'auprc': 4})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yQxVho3DKKJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../figures\n",
        "\n",
        "performance_barplot(\n",
        "                    d_v,\n",
        "                    'benchmark_performance',\n",
        "                    )"
      ],
      "metadata": {
        "id": "IgOHSMq4L96Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adapt"
      ],
      "metadata": {
        "id": "qkBdAMlV1gjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab/bar_policy_suicidality/inputs/data\n",
        "#%cd inputs/data"
      ],
      "metadata": {
        "id": "8ButJIYk6Otd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_adapt = pd.read_csv('d_adapt.csv')\n",
        "\n",
        "d_adapt.info()\n",
        "d_adapt.head(3)"
      ],
      "metadata": {
        "id": "pB2WZzlO6AHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_adapt = d_adapt[[\n",
        "                   'text',\n",
        "                   'p_sbrt',\n",
        "                   ]].copy()\n",
        "\n",
        "d_adapt.shape\n",
        "d_adapt.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0pa6qlHn4UjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../../outputs/models\n",
        "models_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/models/'"
      ],
      "metadata": {
        "id": "_hASR09T40YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Domain adaptation proxy task: subreddit clr**"
      ],
      "metadata": {
        "id": "Bs2bRT3LJyiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prep dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.df.iloc[index]['text']\n",
        "        label = self.df.iloc[index]['p_sbrt']\n",
        "\n",
        "        if pd.isna(text):\n",
        "            text = ' '  ### replaces NaN w/ empty string\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "                                              text,\n",
        "                                              add_special_tokens = True,\n",
        "                                              max_length = self.max_length,\n",
        "                                              return_token_type_ids = False,\n",
        "                                              padding = 'max_length',\n",
        "                                              truncation = True,\n",
        "                                              return_attention_mask = True,\n",
        "                                              return_tensors = 'pt'\n",
        "                                              )\n",
        "        return {\n",
        "                'input_ids': encoding['input_ids'].flatten(),\n",
        "                'attention_mask': encoding['attention_mask'].flatten(),\n",
        "                'label': torch.tensor(label, dtype = torch.long)\n",
        "                }\n",
        "\n",
        "# hyperparams\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "max_length = 512\n",
        "\n",
        "# define models\n",
        "\n",
        "models_to_train = {\n",
        "                   #'BERT': {\n",
        "                   #         'model_class': BertForSequenceClassification,\n",
        "                   #         'tokenizer_class': BertTokenizer,\n",
        "                   #         'pretrained_model_name': 'bert-base-uncased',\n",
        "                   #         },\n",
        "                  #'RoBERTa': {\n",
        "                  #            'model_class': RobertaForSequenceClassification,\n",
        "                  #            'tokenizer_class': RobertaTokenizer,\n",
        "                  #            'pretrained_model_name': 'roberta-base',\n",
        "                  #          },\n",
        "                  'DistilBERT': {\n",
        "                                 'model_class': DistilBertForSequenceClassification,\n",
        "                                 'tokenizer_class': DistilBertTokenizer,\n",
        "                                 'pretrained_model_name': 'distilbert-base-uncased'\n",
        "                                 }\n",
        "                  }\n",
        "\n",
        "# encode labels\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "d_adapt['p_sbrt'] = label_encoder.fit_transform(d_adapt['p_sbrt'])\n",
        "\n",
        "# iterate over models\n",
        "\n",
        "for model_name, model_info in models_to_train.items():\n",
        "    print(f'\\nTraining {model_name}...')\n",
        "    print(\"--------------------------------------------------------------------------------------\")\n",
        "\n",
        "    # initialize tokenizer, model\n",
        "\n",
        "    tokenizer = model_info['tokenizer_class'].from_pretrained(model_info['pretrained_model_name'])\n",
        "    model = model_info['model_class'].from_pretrained(\n",
        "                                                      model_info['pretrained_model_name'],\n",
        "                                                      num_labels = 3, ### update for true run - maps to tensor dimensions\n",
        "                                                      )\n",
        "\n",
        "    # prep dataset\n",
        "\n",
        "    dataset = CustomDataset(\n",
        "                            d_adapt,\n",
        "                            tokenizer,\n",
        "                            max_length = max_length,\n",
        "                            )\n",
        "    dataloader = DataLoader(\n",
        "                            dataset,\n",
        "                            batch_size = batch_size,\n",
        "                            shuffle = True,\n",
        "                            )\n",
        "\n",
        "    # config optimizer\n",
        "\n",
        "    optimizer = AdamW(\n",
        "                      model.parameters(), ### ensures all model params are trained\n",
        "                      lr = learning_rate,\n",
        "                      )\n",
        "\n",
        "    # training loop\n",
        "\n",
        "    model.train()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch+1}/{epochs}')\n",
        "        for batch in tqdm(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                            input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            labels = labels,\n",
        "                            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # save\n",
        "\n",
        "    save_path = f'{models_path}{model_name}_adapted'\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    print(f'{model_name} saved to {save_path}\\n')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uFGYYiImO0Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate domain-adapted models: $k$-fold cross validate"
      ],
      "metadata": {
        "id": "7xoLLpCnlkpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets_and_class_weights = {\n",
        "                             'asp': [\n",
        "                                     0.7657, ### w_neg\n",
        "                                     1.4410, ### w_pos\n",
        "                                     ],\n",
        "                             'dep': [\n",
        "                                     0.5989,\n",
        "                                     3.0286,\n",
        "                                     ],\n",
        "                             'val': [\n",
        "                                     0.6876,\n",
        "                                     1.8327,\n",
        "                                     ],\n",
        "                             'prg': [\n",
        "                                     0.6024,\n",
        "                                     2.9414,\n",
        "                                     ],\n",
        "                             'tgd': [\n",
        "                                     0.5974,\n",
        "                                     3.0676,\n",
        "                                     ],\n",
        "                             'age': [\n",
        "                                     0.6303,\n",
        "                                     2.4188,\n",
        "                                     ],\n",
        "                             'race': [\n",
        "                                      0.506,\n",
        "                                      42.0441,\n",
        "                                      ],\n",
        "                             'dbty': [\n",
        "                                      0.5064,\n",
        "                                      39.7083,\n",
        "                                      ],\n",
        "                              }\n",
        "\n",
        "models_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/models/'"
      ],
      "metadata": {
        "id": "7-mwIf8dxRJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_and_class_weights = targets_and_class_weights\n",
        "\n",
        "models = {\n",
        "          'bert': (\n",
        "                   BertForSequenceClassification.from_pretrained(f'{models_path}BERT_adapted'),\n",
        "                   BertTokenizer.from_pretrained(f'{models_path}BERT_adapted'),\n",
        "                   'bert-base-uncased',\n",
        "                   ),\n",
        "\n",
        "          'roberta': (\n",
        "                      RobertaForSequenceClassification.from_pretrained(f'{models_path}RoBERTa_adapted'),\n",
        "                      RobertaTokenizer.from_pretrained(f'{models_path}RoBERTa_adapted'),\n",
        "                      'roberta-base',\n",
        "                      ),\n",
        "\n",
        "          'distilbert': (\n",
        "                         DistilBertForSequenceClassification.from_pretrained(f'{models_path}DistilBERT_adapted'),\n",
        "                         DistilBertTokenizer.from_pretrained(f'{models_path}DistilBERT_adapted'),\n",
        "                         'distilbert-base-uncased',\n",
        "                         )\n",
        "          }\n",
        "\n",
        "save_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/models/'\n",
        "\n",
        "cycle = 'adapted'"
      ],
      "metadata": {
        "id": "q_6DAaZzO0Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "v0PiGvMKxdIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../../outputs/tables\n",
        "\n",
        "# 'adapted' cycle: train-test loop\n",
        "\n",
        "results = train_eval_save_bl_models(\n",
        "                                    d_augmented,\n",
        "                                    targets_and_class_weights,\n",
        "                                    models,\n",
        "                                    save_path,\n",
        "                                    cycle,\n",
        "                                    )"
      ],
      "metadata": {
        "id": "wzyN3sIMmrGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adapted: viz**"
      ],
      "metadata": {
        "id": "GV0HVwTiRmyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../outputs/tables"
      ],
      "metadata": {
        "id": "8Ye3la_pYYlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/tables"
      ],
      "metadata": {
        "id": "U7ilAKBkq2jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "d_v = pd.read_excel('d_adapted_performance.xlsx')\n",
        "d_v.round({'f1_macro': 4, 'mcc': 4, 'auprc': 4})"
      ],
      "metadata": {
        "id": "OvqxmMeFRb0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../figures\n",
        "\n",
        "performance_barplot(\n",
        "                    d_v,\n",
        "                    'adapted_performance',\n",
        "                    )"
      ],
      "metadata": {
        "id": "79hIkMb1Rb6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Tune\n",
        "Builds stratified train-test sets, searches hyperparam space to optimize highest-performing target x pretrained model configs.\n",
        "***"
      ],
      "metadata": {
        "id": "G614tXUn0VWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "2nH8FoS6d4sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../inputs/data\n",
        "\n",
        "d_augmented = pd.read_excel('d_augmented.xlsx')"
      ],
      "metadata": {
        "id": "ZPmkMPkboW7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = [\n",
        "           'asp',\n",
        "           'dep',\n",
        "#           'val',\n",
        "#           'prg',\n",
        "#           'tgd',\n",
        "#           'age',\n",
        "#           'race',\n",
        "#           'dbty',\n",
        "           ]\n",
        "\n",
        "d_augmented = d_augmented[\n",
        "                          ['text',\n",
        "                           'aug'] +\n",
        "                           targets\n",
        "                           ].copy()\n",
        "\n",
        "d_augmented.info()\n",
        "d_augmented.head(3)"
      ],
      "metadata": {
        "id": "OBVrIz_W8du0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../../temp"
      ],
      "metadata": {
        "id": "TkdI-9lG0Tfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target-parsed $\\mathcal{d}$<sub>train</sub>($y$): augmented | $\\mathcal{d}$<sub>test</sub>($y$): de-augmented**"
      ],
      "metadata": {
        "id": "FKmj9QKr2Uf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_datasets = iterative_stratified_train_test_split_with_rationales(\n",
        "                                                               d_augmented,\n",
        "                                                               targets,\n",
        "                                                               random_state = 56,\n",
        "                                                               test_size = 0.2,\n",
        "                                                               )"
      ],
      "metadata": {
        "id": "IjHgWQk-7pAZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_datasets.keys())"
      ],
      "metadata": {
        "id": "TQxFeCc9w9tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop 'aug' + extract target-wise train/test df\n",
        "\n",
        "#for target, (d_train, d_test) in target_datasets.items():\n",
        "\n",
        "        # Drop the 'aug' column from both train and test datasets\n",
        "#    if 'aug' in d_train.columns:\n",
        "#        d_train = d_train.drop(columns=['aug'])\n",
        "#    if 'aug' in d_test.columns:\n",
        "#        d_test = d_test.drop(columns=['aug'])\n",
        "\n",
        "        # Update the dictionary with the modified dataframes\n",
        "d_train_asp, d_test_asp = target_datasets['asp']\n",
        "d_train_dep, d_test_dep = target_datasets['dep']"
      ],
      "metadata": {
        "id": "GJLRYddN7pKP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_train_asp = d_train_asp.drop('aug', axis = 1)\n",
        "d_test_asp = d_test_asp.drop('aug', axis = 1)\n",
        "\n",
        "d_train_asp.head(3)\n",
        "d_test_asp.head(3)\n",
        "\n",
        "d_train_dep = d_train_dep.drop('aug', axis = 1)\n",
        "d_test_dep = d_test_dep.drop('aug', axis = 1)\n",
        "\n",
        "d_train_dep.head(3)\n",
        "d_test_dep.head(3)"
      ],
      "metadata": {
        "id": "ci-d-b6AnjCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../../outputs/tables\n",
        "\n",
        "# define hyperparameter grid\n",
        "\n",
        "hyperparameter_grid = {\n",
        "                       'batch_size': [8, 16],\n",
        "                       'gradient_accumulation_steps': [1, 2],\n",
        "                       'learning_rate': [2e-5, 3e-5],\n",
        "                       'num_epochs': [2, 3],\n",
        "                       'warmup_steps': [0, 500],\n",
        "                       'weight_decay': [0.0, 0.3],\n",
        "                       }\n",
        "\n",
        "models_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/models'\n",
        "\n",
        "    ### SJS 9/8: no longer using dedicated subdirectories; just do save_path = models_path when calling the Fx...\n",
        "\n",
        "# define tuning param sets\n",
        "\n",
        "params = [\n",
        "\n",
        "    # asp: Best F1 (macro) for asp: 0.2162 achieved by bert - benchmark\n",
        "\n",
        "          {\n",
        "           'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
        "           'model_class': BertForSequenceClassification,\n",
        "           'pretrained_model_name': 'bert-base-uncased',\n",
        "           'd_train': d_train_asp,\n",
        "           'd_test': d_test_asp,\n",
        "           'target': 'asp',\n",
        "           'class_weights': torch.tensor([\n",
        "                                          0.7657, ### w_neg\n",
        "                                          1.4410, ### w_pos\n",
        "                                          ], dtype = torch.float),\n",
        "           'save_path': models_path,\n",
        "           },\n",
        "\n",
        "    # dep: Best F1 (macro) for dep: 0.6857 achieved by bert - benchmark\n",
        "\n",
        "          {\n",
        "           'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
        "           'model_class': BertForSequenceClassification,\n",
        "           'pretrained_model_name': 'bert-base-uncased',\n",
        "           'd_train': d_train_dep,\n",
        "           'd_test': d_test_dep,\n",
        "           'target': 'dep',\n",
        "           'class_weights': torch.tensor([\n",
        "                                          0.5989,\n",
        "                                          3.0286,\n",
        "                                          ], dtype = torch.float),\n",
        "           'save_path': models_path,\n",
        "           },\n",
        "\n",
        "    # val: Best F1 (macro) for val: 0.6259 achieved by roberta - roberta\n",
        "\n",
        "#          {\n",
        "#           'tokenizer': RobertaTokenizer.from_pretrained('roberta-base'),\n",
        "#           'model_class': RobertaForSequenceClassification,\n",
        "#           'pretrained_model_name': 'roberta-base',\n",
        "#           'd_train': d_train_val,\n",
        "#           'd_test': d_test_val,\n",
        "#           'target': 'val',\n",
        "#           'class_weights': torch.tensor([\n",
        "#                                          0.6876,\n",
        "#                                          1.8327,\n",
        "#                                          ], dtype = torch.float),\n",
        "#           'save_path': models_path,\n",
        "#           },\n",
        "     # prg: Best F1 (macro) for prg: 0.6013 achieved by roberta\n",
        "\n",
        "#          {\n",
        "#           'tokenizer': RobertaTokenizer.from_pretrained('roberta-base'),\n",
        "#           'model_class': RobertaForSequenceClassification,\n",
        "#           'pretrained_model_name': 'roberta-base',\n",
        "#           'd_train': d_train_prg,\n",
        "#           'd_test': d_test_prg,\n",
        "#           'target': 'prg',\n",
        "#           'class_weights': torch.tensor([\n",
        "#                                          0.6024,\n",
        "#                                          2.9414,\n",
        "#                                          ], dtype = torch.float),\n",
        "#           'save_path': models_path,\n",
        "#           },\n",
        "\n",
        "    # tgd: Best F1 (macro) for tgd: 0.7414 achieved by bert\n",
        "\n",
        "#          {\n",
        "#           'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
        "#           'model_class': BertForSequenceClassification,\n",
        "#           'pretrained_model_name': 'bert-base-uncased',\n",
        "#           'd_train': d_train_tgd,\n",
        "#           'd_test': d_test_tgd,\n",
        "#           'target': 'tgd',\n",
        "#           'class_weights': torch.tensor([\n",
        "#                                          0.5974,\n",
        "#                                          3.0676,\n",
        "#                                          ], dtype = torch.float),\n",
        "#           'save_path': models_path,\n",
        "#           },\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "# tuning loop\n",
        "\n",
        "all_tuned_performance = pd.DataFrame()\n",
        "\n",
        "for p in params:\n",
        "    #print(f\"Inspecting parameter set: {p}\")  # inspect for dict format\n",
        "    d_test, d_tuned_performance = tune_and_optimize_model_hyperparams(\n",
        "                                                                      tokenizer = p['tokenizer'],\n",
        "                                                                      model_class = p['model_class'],\n",
        "                                                                      pretrained_model_name = p['pretrained_model_name'],\n",
        "                                                                      d_train = p['d_train'],\n",
        "                                                                      d_test = p['d_test'],\n",
        "                                                                      target = p['target'],\n",
        "                                                                      class_weights = p['class_weights'],\n",
        "                                                                      save_path = p['save_path'],\n",
        "                                                                      hyperparameter_grid = hyperparameter_grid,\n",
        "                                                                      )\n",
        "\n",
        "    all_tuned_performance = pd.concat([all_tuned_performance, d_tuned_performance], ignore_index = True)\n",
        "\n",
        "print(all_tuned_performance.head())"
      ],
      "metadata": {
        "id": "8AYBEaro0TrZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_tuned_performance.head())\n",
        "all_tuned_performance.to_excel('all_tuned_performance.xlsx')"
      ],
      "metadata": {
        "id": "pr3qgvh5BkT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ARMHR viz - concat\n",
        "\n",
        "%cd ../temp/\n",
        "\n",
        "temp_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/temp/'\n",
        "\n",
        "targets = [\n",
        "           'asp',\n",
        "           'dep',\n",
        "           'val',\n",
        "           'prg',\n",
        "           'tgd',\n",
        "           #'age',\n",
        "           #'race',\n",
        "           #'dbty',\n",
        "           ]\n",
        "\n",
        "d_list = []\n",
        "\n",
        "for target in targets:\n",
        "    file_path = os.path.join(temp_path, f'd_tuned_performance_{target}.xlsx')\n",
        "\n",
        "    d = pd.read_excel(\n",
        "                       file_path,\n",
        "                       index_col = [0],\n",
        "                       )\n",
        "\n",
        "    d['target'] = target\n",
        "    d_list.append(d)\n",
        "\n",
        "d_all_tuned_performance = pd.concat(\n",
        "                                    d_list,\n",
        "                                    ignore_index = True,\n",
        "                                    )\n",
        "\n",
        "d_all_tuned_performance.info()\n",
        "d_all_tuned_performance.head(3)\n",
        "\n",
        "d_all_tuned_performance.to_excel('d_all_tuned_performance.xlsx')"
      ],
      "metadata": {
        "id": "of0D977qfqAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Infer"
      ],
      "metadata": {
        "id": "xnP3hWY2bGlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SJS 9/18: for ARMHR\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab/bar_policy_suicidality/inputs/data\n",
        "#%cd inputs/data\n",
        "\n",
        "d_adapt = pd.read_csv('d_adapt.csv')\n",
        "\n",
        "d_adapt.info()\n",
        "d_adapt.head(3)"
      ],
      "metadata": {
        "id": "AAeXGG-n0TuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_inference = d_adapt.sample(\n",
        "                             n = 10000,\n",
        "                             random_state = 56,\n",
        "                             ).reset_index(drop = True)\n",
        "\n",
        "d_inference.info()\n",
        "d_inference.head(3)"
      ],
      "metadata": {
        "id": "6rJcflyfTt5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/models/'\n",
        "\n",
        "# define inference param sets\n",
        "\n",
        "params = [\n",
        "          {\n",
        "           'target': 'asp',\n",
        "           'model_class': BertForSequenceClassification,\n",
        "           'tokenizer_class': BertTokenizer,\n",
        "           'pretrained_model_name': 'bert-base-uncased',\n",
        "           'model_path': f'{models_path}asp_bert-base-uncased_best_tuned_model.bin',\n",
        "          },\n",
        "          {\n",
        "           'target': 'dep',\n",
        "           'model_class': BertForSequenceClassification,\n",
        "           'tokenizer_class': BertTokenizer,\n",
        "           'pretrained_model_name': 'bert-base-uncased',\n",
        "           'model_path': f'{models_path}dep_bert-base-uncased_best_tuned_model.bin',\n",
        "          },\n",
        "         {\n",
        "          'target': 'val',\n",
        "          'model_class': RobertaForSequenceClassification,\n",
        "          'tokenizer_class': RobertaTokenizer,\n",
        "          'pretrained_model_name': 'roberta-base',\n",
        "          'model_path': f'{models_path}val_roberta-base_best_tuned_model.bin',\n",
        "         },\n",
        "          {\n",
        "           'target': 'prg',\n",
        "          'model_class': RobertaForSequenceClassification,\n",
        "          'tokenizer_class': RobertaTokenizer,\n",
        "          'pretrained_model_name': 'roberta-base',\n",
        "           'model_path': f'{models_path}prg_roberta-base_best_tuned_model.bin',\n",
        "          },\n",
        "         {\n",
        "          'target': 'tgd',\n",
        "          'model_class': BertForSequenceClassification,\n",
        "          'tokenizer_class': BertTokenizer,\n",
        "          'pretrained_model_name': 'bert-base-uncased',\n",
        "          'model_path': f'{models_path}tgd_bert-base-uncased_best_tuned_model.bin',\n",
        "         },\n",
        "\n",
        "    ### SJS 9/8: full gamut TKTK\n",
        "\n",
        "]\n",
        "\n",
        "# coerce to str\n",
        "\n",
        "d_inference['text'] = d_inference['text'].astype(str)\n",
        "texts = d_inference['text'].tolist()\n",
        "\n",
        "# inference loop\n",
        "\n",
        "for p in params:\n",
        "    target = p['target']\n",
        "\n",
        "    # load tokenizers, models\n",
        "\n",
        "    tokenizer = p['tokenizer_class'].from_pretrained(p['pretrained_model_name'])\n",
        "    model = load_model(\n",
        "                       p['model_path'],\n",
        "                       p['model_class'],\n",
        "                       p['pretrained_model_name'],\n",
        "                       )\n",
        "\n",
        "    # infer predictions, probabilities\n",
        "\n",
        "    predictions, probabilities = predict(\n",
        "                                         model,\n",
        "                                         tokenizer,\n",
        "                                         texts,\n",
        "                                         )\n",
        "\n",
        "    d_inference[f'{target}_pred'] = predictions\n",
        "    d_inference[f'{target}_prob'] = probabilities\n",
        "\n",
        "    # inspect\n",
        "\n",
        "d_inference[[\n",
        "             'asp_pred',\n",
        "             'dep_pred',\n",
        "             'val_pred',\n",
        "             'prg_pred',\n",
        "             'tgd_pred',\n",
        "             ]].apply(pd.Series.value_counts)\n",
        "\n",
        "d_inference.head(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xjfl2BodDkEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_inference.to_csv('d_inference_pred_armhr.csv')"
      ],
      "metadata": {
        "id": "Ez_pjn75IHDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Explain"
      ],
      "metadata": {
        "id": "I5JVWw7spBaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "Y9zfVgzDrqeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd inputs/data\n",
        "\n",
        "### SJS 9/20: Fx TKTK to parsimoniously explain over targets - _or_ multilabel/ensemble...\n",
        "\n",
        "d_inference = pd.read_csv('d_inference_pred_armhr.csv')\n",
        "d_inference.drop(\n",
        "                 'Unnamed: 0',\n",
        "                 axis = 1,\n",
        "                 inplace = True,\n",
        "                 )\n",
        "\n",
        "d_inference.info()\n",
        "d_inference.head(3)"
      ],
      "metadata": {
        "id": "-wTlTOzAIJ95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$\\hat{s}$_<sub>1</sub>: asp clr"
      ],
      "metadata": {
        "id": "9utdKq-ZXBi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sort by confidence**"
      ],
      "metadata": {
        "id": "v1d5mfGXa8Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract pos proba\n",
        "\n",
        "d_inference['asp_prob'] = d_inference['asp_prob'].apply(lambda i: ast.literal_eval(i))\n",
        "d_inference['asp_pos'] = d_inference['asp_prob'].apply(lambda i: round(i[1], 4))\n",
        "\n",
        "#d_inference.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EwBNAel8ZKdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse by 'asp_pos' > 0.90\n",
        "\n",
        "pos_index = d_inference[d_inference['asp_pos'] > 0.90]\n",
        "pos_index"
      ],
      "metadata": {
        "id": "3yIr8R4ZdE_T",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIMETextExplainer**"
      ],
      "metadata": {
        "id": "7ylF39dps5De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "models_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/models/'\n",
        "\n",
        "# load pre-trained tokenizer, config, architecture from Hugging Face\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "                                                      'bert-base-uncased',\n",
        "                                                      num_labels = 2,\n",
        "                                                      )\n",
        "\n",
        "# load fine-tuned model weights\n",
        "\n",
        "asp_path = f'{models_path}asp_bert-base-uncased_best_tuned_model.bin'\n",
        "model.load_state_dict(torch.load(asp_path, map_location = torch.device('cuda')))\n",
        "\n",
        "# coerce eval mode\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# targets\n",
        "\n",
        "class_names = [\n",
        "               '0',\n",
        "               '1',\n",
        "               ]\n",
        "\n",
        "# tokenize Fx\n",
        "\n",
        "def predict_proba(texts):\n",
        "    inputs = tokenizer(\n",
        "                       texts,\n",
        "                       padding = True,\n",
        "                       truncation = True,\n",
        "                       return_tensors = 'pt',\n",
        "                       max_length = 512,\n",
        "                       )\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.softmax(\n",
        "                              outputs.logits,\n",
        "                              dim = 1,\n",
        "                              )\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "# initialize explainer\n",
        "\n",
        "### SJS 9/27: re 'bow' and 'char_level' params: https://lime-ml.readthedocs.io/en/latest/lime.html\n",
        "\n",
        "explainer = LimeTextExplainer(\n",
        "                              class_names = class_names,\n",
        "                              random_state = 56,\n",
        "                              #bow = False,\n",
        "                              #char_level = True,\n",
        "                              )\n",
        "\n",
        "# illustrative string - toy ex\n",
        "\n",
        "#text = '''\n",
        "#All my life I wished I could be a champion snowboarder, but my injuries as a child ruined that dream.\n",
        "#Because I am medicated and rely on a walker to navigate only short distances, my athletic career can\n",
        "#never return. I feel hopeless and stunted and passed by, and like a burden to my family.\n",
        "#'''\n",
        "\n",
        "# d_inference selection\n",
        "\n",
        "pos_instance = 51\n",
        "text = d_inference.loc[pos_instance, 'text']\n",
        "\n",
        "# explain\n",
        "\n",
        "exp = explainer.explain_instance(\n",
        "                                 text,\n",
        "                                 predict_proba,\n",
        "                                 num_features = 8,\n",
        "                                 num_samples = 1000,\n",
        "                                 distance_metric = 'cosine',\n",
        "                                 )\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R-tOOavfs4Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display explanation**"
      ],
      "metadata": {
        "id": "Se8Wvoq4a22Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_instance)\n",
        "exp.show_in_notebook(text = True)\n",
        "#exp.save_to_file('lime_explanation.html')"
      ],
      "metadata": {
        "id": "ZKPRDDpn_pNp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/outputs/figures\n",
        "exp.save_to_file('lime_explanation_asp_51.html')"
      ],
      "metadata": {
        "id": "4TIgZYSmbpR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$\\hat{s}$_<sub>2</sub>: dep clr"
      ],
      "metadata": {
        "id": "oWE0PfxkmoHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sort by confidence**"
      ],
      "metadata": {
        "id": "5dbTQMDlmwkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract pos proba\n",
        "\n",
        "#d_inference['dep_prob'] = d_inference['dep_prob'].apply(lambda i: ast.literal_eval(i))\n",
        "d_inference['dep_pos'] = d_inference['dep_prob'].apply(lambda i: round(i[1], 4))\n",
        "\n",
        "#d_inference.head()"
      ],
      "metadata": {
        "id": "1rIBUV5-f1LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse by 'asp_pos' > 0.90\n",
        "\n",
        "pos_index = d_inference[d_inference['dep_pos'] > 0.90]\n",
        "pos_index"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8AOKufJ_m2LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "models_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/models/'\n",
        "\n",
        "# load pre-trained tokenizer, config, architecture from Hugging Face\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "                                                      'bert-base-uncased',\n",
        "                                                      num_labels = 2,\n",
        "                                                      )\n",
        "\n",
        "# load fine-tuned model weights\n",
        "\n",
        "dep_path = f'{models_path}dep_bert-base-uncased_best_tuned_model.bin'\n",
        "model.load_state_dict(torch.load(dep_path, map_location = torch.device('cuda')))\n",
        "\n",
        "# coerce eval mode\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# targets\n",
        "\n",
        "class_names = [\n",
        "               '0',\n",
        "               '1',\n",
        "               ]\n",
        "\n",
        "# tokenize Fx\n",
        "\n",
        "def predict_proba(texts):\n",
        "    inputs = tokenizer(\n",
        "                       texts,\n",
        "                       padding = True,\n",
        "                       truncation = True,\n",
        "                       return_tensors = 'pt',\n",
        "                       max_length = 512,\n",
        "                       )\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.softmax(\n",
        "                              outputs.logits,\n",
        "                              dim = 1,\n",
        "                              )\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "# initialize explainer\n",
        "\n",
        "explainer = LimeTextExplainer(class_names = class_names)\n",
        "\n",
        "# d_inference selection\n",
        "\n",
        "pos_instance = 116\n",
        "text = d_inference.loc[pos_instance, 'text']\n",
        "\n",
        "# explain\n",
        "\n",
        "exp = explainer.explain_instance(\n",
        "                                 text,\n",
        "                                 predict_proba,\n",
        "                                 num_features = 8,\n",
        "                                 num_samples = 1000,\n",
        "                                 distance_metric = 'cosine',\n",
        "                                 )\n"
      ],
      "metadata": {
        "id": "3eJDsZq1m5ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_instance)\n",
        "exp.show_in_notebook(text = True)\n",
        "#exp.save_to_file('lime_explanation.html')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yuVxUk17m5ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/outputs/figures\n",
        "exp.save_to_file('lime_explanation_dep_116.html')"
      ],
      "metadata": {
        "id": "0qyTCCUBm5pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _$\\hat{s}$_<sub>3</sub>: val clr"
      ],
      "metadata": {
        "id": "Cpv2_RyqpvQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sort by confidence**"
      ],
      "metadata": {
        "id": "tK1JNUHKpwzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract pos proba\n",
        "\n",
        "#d_inference['val_prob'] = d_inference['val_prob'].apply(lambda i: ast.literal_eval(i))\n",
        "d_inference['val_pos'] = d_inference['val_prob'].apply(lambda i: round(i[1], 4))\n",
        "\n",
        "#d_inference.head()"
      ],
      "metadata": {
        "id": "2JPsnIyUoDpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse by 'asp_pos' > 0.90\n",
        "\n",
        "pos_index = d_inference[d_inference['val_pos'] > 0.90]\n",
        "pos_index"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vKBS3rZJpx22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "models_path = '/content/drive/MyDrive/Colab/bar_policy_suicidality/outputs/models/'\n",
        "\n",
        "# load pre-trained tokenizer, config, architecture from Hugging Face\n",
        "\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "                                                      'roberta-base',\n",
        "                                                      num_labels = 2,\n",
        "                                                      )\n",
        "# load fine-tuned model weights\n",
        "\n",
        "val_path = f'{models_path}val_roberta-base_best_tuned_model.bin'\n",
        "model.load_state_dict(torch.load(val_path, map_location = torch.device('cuda')))\n",
        "\n",
        "# coerce eval mode\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# targets\n",
        "\n",
        "class_names = [\n",
        "               '0',\n",
        "               '1',\n",
        "               ]\n",
        "\n",
        "# tokenize Fx\n",
        "\n",
        "def predict_proba(texts):\n",
        "    inputs = tokenizer(\n",
        "                       texts,\n",
        "                       padding = True,\n",
        "                       truncation = True,\n",
        "                       return_tensors = 'pt',\n",
        "                       max_length = 512,\n",
        "                       )\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.softmax(\n",
        "                              outputs.logits,\n",
        "                              dim = 1,\n",
        "                              )\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "# initialize explainer\n",
        "\n",
        "explainer = LimeTextExplainer(class_names = class_names)\n",
        "\n",
        "# d_inference selection\n",
        "\n",
        "pos_instance = 19\n",
        "text = d_inference.loc[pos_instance, 'text']\n",
        "\n",
        "# explain\n",
        "\n",
        "exp = explainer.explain_instance(\n",
        "                                 text,\n",
        "                                 predict_proba,\n",
        "                                 num_features = 8,\n",
        "                                 num_samples = 1000,\n",
        "                                 distance_metric = 'cosine',\n",
        "                                 )\n"
      ],
      "metadata": {
        "id": "bmk43Ktgpx7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_instance)\n",
        "exp.show_in_notebook(text = True)\n",
        "#exp.save_to_file('lime_explanation.html')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "X2RqAqYzpx_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/bar_policy_suicidality/outputs/figures\n",
        "exp.save_to_file('lime_explanation_val_19.html')"
      ],
      "metadata": {
        "id": "wy5IB8ghpyDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> End of aim_i_train_tune_predict.ipynb"
      ],
      "metadata": {
        "id": "-odwkzTUrD32"
      }
    }
  ]
}